{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "G44iH6jnObEj"
      },
      "outputs": [],
      "source": [
        "SEED_NUMBER = 10\n",
        "import random\n",
        "random.seed(SEED_NUMBER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJmXzFdisD7P"
      },
      "source": [
        "This cell to run any additional requirement that your code need <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaPjq2hMqd20",
        "outputId": "4b697845-7be9-424e-9847-50043b4e1c29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in c:\\users\\m\\appdata\\roaming\\python\\python312\\site-packages (from -r requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: memory-profiler in c:\\users\\m\\appdata\\roaming\\python\\python312\\site-packages (from -r requirements.txt (line 2)) (0.61.0)\n",
            "Requirement already satisfied: varint in c:\\users\\m\\appdata\\roaming\\python\\python312\\site-packages (from -r requirements.txt (line 3)) (1.0.2)\n",
            "Requirement already satisfied: psutil in c:\\users\\m\\appdata\\roaming\\python\\python312\\site-packages (from memory-profiler->-r requirements.txt (line 2)) (6.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install memory-profiler >> log.txt\n",
        "!pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "usqLi0C8K1h6"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import os\n",
        "# import numpy as np\n",
        "# from sklearn.cluster import MiniBatchKMeans\n",
        "# import logging\n",
        "\n",
        "# class IVF_PQ:\n",
        "#     def __init__(self, n_subvectors, n_bits, n_clusters, dimension=64, folder_path='index', random_state=42):\n",
        "#         self.n_subvectors = n_subvectors\n",
        "#         self.n_bits = n_bits\n",
        "#         self.n_clusters_per_subvector = 2 ** n_bits\n",
        "#         self.n_clusters = n_clusters\n",
        "#         self.dimension = dimension\n",
        "#         self.random_state = random_state\n",
        "#         self.folder_path = folder_path\n",
        "\n",
        "#         if dimension % n_subvectors != 0:\n",
        "#             raise ValueError(\"Dimension must be divisible by n_subvectors\")\n",
        "\n",
        "#     def build(self, vectors):\n",
        "#         n_vectors = vectors.shape[0]\n",
        "#         logging.info(\"Training IVF...\")\n",
        "#         kmeans = MiniBatchKMeans(n_clusters=self.n_clusters, random_state=self.random_state, batch_size=10000)\n",
        "#         labels = kmeans.fit_predict(vectors)\n",
        "#         cluster_centers = kmeans.cluster_centers_.astype(np.float32)\n",
        "\n",
        "#         sort_idx = np.argsort(labels)\n",
        "#         vectors_sorted = vectors[sort_idx]\n",
        "#         original_ids = np.arange(n_vectors)[sort_idx]\n",
        "\n",
        "#         offsets = np.zeros(self.n_clusters + 1, dtype=np.int64)\n",
        "#         offsets[1:] = np.cumsum(np.bincount(labels, minlength=self.n_clusters))\n",
        "\n",
        "#         dim_per_sub = self.dimension // self.n_subvectors\n",
        "#         codebooks = np.zeros((self.n_clusters, self.n_subvectors, self.n_clusters_per_subvector, dim_per_sub), np.float32)\n",
        "#         codewords = np.zeros((n_vectors, self.n_subvectors), dtype=np.uint8)\n",
        "\n",
        "#         logging.info(\"Training PQ codebooks...\")\n",
        "#         pos = 0\n",
        "#         for c in range(self.n_clusters):\n",
        "#             start, end = int(offsets[c]), int(offsets[c+1])\n",
        "#             if start == end: continue\n",
        "#             cluster_vecs = vectors_sorted[pos:pos + end-start]\n",
        "\n",
        "#             for m in range(self.n_subvectors):\n",
        "#                 sub = cluster_vecs[:, m*dim_per_sub:(m+1)*dim_per_sub]\n",
        "#                 pq = MiniBatchKMeans(n_clusters=self.n_clusters_per_subvector,\n",
        "#                                      random_state=self.random_state, batch_size=1000, max_iter=50)\n",
        "#                 codes = pq.fit_predict(sub)\n",
        "#                 codebooks[c, m] = pq.cluster_centers_\n",
        "#                 codewords[original_ids[pos:pos + end-start], m] = codes\n",
        "#             pos += end - start\n",
        "\n",
        "#         # Save as .npy\n",
        "#         os.makedirs(self.folder_path, exist_ok=True)\n",
        "#         np.save(os.path.join(self.folder_path, 'centroids.npy'),   cluster_centers)\n",
        "#         np.save(os.path.join(self.folder_path, 'codebook.npy'),    codebooks)\n",
        "#         np.save(os.path.join(self.folder_path, 'codewords.npy'),   codewords)\n",
        "#         np.save(os.path.join(self.folder_path, 'indices.npy'),     original_ids)\n",
        "#         np.save(os.path.join(self.folder_path, 'offsets.npy'),     offsets)\n",
        "\n",
        "#         logging.info(\"Index built and saved as .npy (5 files).\")\n",
        "\n",
        "#     def retreive(self, query_vector, cosine_similarity, get_row, n_clusters=10, top_k=10):\n",
        "#         folder = self.folder_path\n",
        "\n",
        "#         # Load only what is needed — everything is small except codewords/indices\n",
        "#         cluster_centers = np.load(os.path.join(folder, 'centroids.npy'),   mmap_mode='r')           # ~4 KB–40 KB\n",
        "#         codebook        = np.load(os.path.join(folder, 'codebook.npy'),    mmap_mode='r')           # usually < 200 MB\n",
        "#         codewords       = np.load(os.path.join(folder, 'codewords.npy'),   mmap_mode='r')           # M × n_subvectors uint8\n",
        "#         indices         = np.load(os.path.join(folder, 'indices.npy'),     mmap_mode='r')           # M int64\n",
        "#         offsets         = np.load(os.path.join(folder, 'offsets.npy'),     mmap_mode='r')           # tiny\n",
        "\n",
        "#         # Coarse search\n",
        "#         sims = np.array([cosine_similarity(query_vector, c) for c in cluster_centers])\n",
        "#         probe_clusters = np.argpartition(sims, -n_clusters)[-n_clusters:]\n",
        "\n",
        "#         cand_ids = []\n",
        "#         cand_scores = []\n",
        "#         d = self.dimension // self.n_subvectors\n",
        "\n",
        "#         for cid in probe_clusters:\n",
        "#             start, end = int(offsets[cid]), int(offsets[cid+1])\n",
        "#             if start == end: continue\n",
        "\n",
        "#             codes = codewords[start:end]           # view, no copy\n",
        "#             ids   = indices[start:end]             # view, no copy\n",
        "\n",
        "#             dist = np.zeros(end - start, dtype=np.float32)\n",
        "#             for m in range(self.n_subvectors):\n",
        "#                 qm = query_vector[m*d:(m+1)*d]\n",
        "#                 centers = codebook[cid, m]         # (256, d/m)\n",
        "#                 dist += np.sum(centers[codes[:, m]]**2, axis=1) - 2 * (centers[codes[:, m]] @ qm)\n",
        "\n",
        "#             k = min(top_k * 4, len(dist))\n",
        "#             best = np.argpartition(dist, k-1)[:k]\n",
        "#             for i in ids[best]:\n",
        "#                 cand_ids.append(i)\n",
        "#                 cand_scores.append(cosine_similarity(query_vector, get_row(i)))\n",
        "\n",
        "#         cand_scores = np.array(cand_scores)\n",
        "#         top = np.argpartition(-cand_scores, top_k-1)[:top_k]\n",
        "#         result = np.array(cand_ids)[top]\n",
        "#         return result[np.argsort(-cand_scores[top])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from typing import Dict, List, Annotated\n",
        "# import numpy as np\n",
        "# import os\n",
        "\n",
        "# DB_SEED_NUMBER = 42\n",
        "# ELEMENT_SIZE = np.dtype(np.float32).itemsize\n",
        "# DIMENSION = 64\n",
        "\n",
        "# class VecDB:\n",
        "#     def __init__(self, database_file_path = \"saved_db.dat\", index_file_path = \"index\", new_db = True, db_size = None) -> None:\n",
        "#         self.db_path = database_file_path\n",
        "#         self.index_path = index_file_path\n",
        "#         self.index=None\n",
        "\n",
        "#         if self._get_num_records() == 1000000:\n",
        "#              self.index = IVF_PQ(n_subvectors=8,n_bits=8,n_clusters=30,folder_path=self.index_path)\n",
        "#         elif self._get_num_records() == 10000000:\n",
        "#              self.index = IVF_PQ(n_subvectors=8,n_bits=8,n_clusters=150,folder_path=self.index_path)\n",
        "#         elif self._get_num_records() == 15000000:\n",
        "#              self.index = IVF_PQ(n_subvectors=8,n_bits=8,n_clusters=250,folder_path=self.index_path)\n",
        "#         elif self._get_num_records() == 20000000:\n",
        "#              self.index = IVF_PQ(n_subvectors=8,n_bits=8,n_clusters=280,folder_path=self.index_path)\n",
        "#         if new_db:\n",
        "#             if db_size is None:\n",
        "#                 raise ValueError(\"You need to provide the size of the database\")\n",
        "#             # delete the old DB file if exists\n",
        "#             if os.path.exists(self.db_path):\n",
        "#                 os.remove(self.db_path)\n",
        "#             self.generate_database(db_size)\n",
        "        \n",
        "#     def generate_database(self, size: int) -> None:\n",
        "#         rng = np.random.default_rng(DB_SEED_NUMBER)\n",
        "#         vectors = rng.random((size, DIMENSION), dtype=np.float32)\n",
        "#         self._write_vectors_to_file(vectors)\n",
        "#         # self._build_index()\n",
        "\n",
        "#     def _write_vectors_to_file(self, vectors: np.ndarray) -> None:\n",
        "#         mmap_vectors = np.memmap(self.db_path, dtype=np.float32, mode='w+', shape=vectors.shape)\n",
        "#         mmap_vectors[:] = vectors[:]\n",
        "#         mmap_vectors.flush()\n",
        "\n",
        "#     def _get_num_records(self) -> int:\n",
        "#         print(f\"Database size (in bytes): {os.path.getsize(self.db_path) // (DIMENSION * ELEMENT_SIZE)}\")\n",
        "#         return os.path.getsize(self.db_path) // (DIMENSION * ELEMENT_SIZE)\n",
        "\n",
        "#     def insert_records(self, rows: Annotated[np.ndarray, (int, 70)]):\n",
        "#         num_old_records = self._get_num_records()\n",
        "#         num_new_records = len(rows)\n",
        "#         full_shape = (num_old_records + num_new_records, DIMENSION)\n",
        "#         mmap_vectors = np.memmap(self.db_path, dtype=np.float32, mode='r+', shape=full_shape)\n",
        "#         mmap_vectors[num_old_records:] = rows\n",
        "#         mmap_vectors.flush()\n",
        "#         #TODO: might change to call insert in the index, if you need\n",
        "#         self._build_index()\n",
        "\n",
        "#     def get_one_row(self, row_num: int) -> np.ndarray:\n",
        "#         # This function is only load one row in memory\n",
        "#         try:\n",
        "#             offset = int(row_num) * int(DIMENSION) * int(ELEMENT_SIZE)\n",
        "#             mmap_vector = np.memmap(self.db_path, dtype=np.float32, mode='r', shape=(1, DIMENSION), offset=offset)\n",
        "#             return np.array(mmap_vector[0])\n",
        "#         except Exception as e:\n",
        "#             return f\"An error occurred: {e}\"\n",
        "\n",
        "#     def get_all_rows(self) -> np.ndarray:\n",
        "#         # Take care this load all the data in memory\n",
        "#         num_records = self._get_num_records()\n",
        "#         vectors = np.memmap(self.db_path, dtype=np.float32, mode='r', shape=(num_records, DIMENSION))\n",
        "#         return np.array(vectors)\n",
        "    \n",
        "#     def retrieve(self, query: Annotated[np.ndarray, (1, DIMENSION)], top_k = 5):\n",
        "        \n",
        "#         if self._get_num_records() == 1000000 or self._get_num_records() == 100000:\n",
        "#             return self.index.retreive(query,self._cal_score,self.get_one_row,n_clusters=30,top_k=top_k)\n",
        "#         elif self._get_num_records() == 10000000:\n",
        "#             return self.index.retreive(query,self._cal_score,self.get_one_row,n_clusters=150,top_k=top_k)\n",
        "#         elif self._get_num_records() == 15000000:\n",
        "#             return self.index.retreive(query,self._cal_score,self.get_one_row,n_clusters=250,top_k=top_k)\n",
        "#         elif self._get_num_records() == 20000000:\n",
        "#             return self.index.retreive(query,self._cal_score,self.get_one_row,n_clusters=280,top_k=top_k)\n",
        "    \n",
        "#     def _cal_score(self, vec1, vec2):\n",
        "#         dot_product = np.dot(vec1, vec2)\n",
        "#         norm_vec1 = np.linalg.norm(vec1)    \n",
        "#         norm_vec2 = np.linalg.norm(vec2)\n",
        "#         cosine_similarity = dot_product / (norm_vec1 * norm_vec2)\n",
        "#         return cosine_similarity\n",
        "\n",
        "#     def _build_index(self):\n",
        "#         # Placeholder for index building logic\n",
        "\n",
        "#         if self._get_num_records() == 1000000 or self._get_num_records() == 100000:\n",
        "#              print(\"Building index for 1M or 100K records\")\n",
        "#              self.index = IVF_PQ(n_subvectors=8,n_bits=8,n_clusters=30,folder_path=self.index_path)\n",
        "#         elif self._get_num_records() == 10000000:\n",
        "#              print(\"Building index for 10M records\")\n",
        "#              self.index = IVF_PQ(n_subvectors=8,n_bits=8,n_clusters=150,folder_path=self.index_path)\n",
        "#         elif self._get_num_records() == 15000000:\n",
        "#              print(\"Building index for 15M records\")\n",
        "#              self.index = IVF_PQ(n_subvectors=8,n_bits=8,n_clusters=250,folder_path=self.index_path)\n",
        "#         elif self._get_num_records() == 20000000:\n",
        "#             print(\"Building index for 20M records\")\n",
        "#             self.index = IVF_PQ(n_subvectors=8,n_bits=8,n_clusters=280,folder_path=self.index_path)\n",
        "#         self.index.build(self.get_all_rows())\n",
        "      \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import json\n",
        "# import numpy as np\n",
        "# import os\n",
        "# import shutil\n",
        "# import tqdm\n",
        "# import heapq\n",
        "# from sklearn.cluster import MiniBatchKMeans\n",
        "# from typing import Annotated\n",
        "# import varint\n",
        "\n",
        "# DB_SEED_NUMBER = 42\n",
        "# ELEMENT_SIZE = np.dtype(np.float32).itemsize\n",
        "# DIMENSION = 64\n",
        "\n",
        "# class VecDB:\n",
        "#     def __init__(self, database_file_path=\"saved_db.dat\", index_file_path=\"index.dat\", new_db=True, db_size=None) -> None:\n",
        "#         self.db_path = database_file_path\n",
        "#         self.index_path = index_file_path\n",
        "#         self.dim = db_size\n",
        "#         if new_db:\n",
        "#             if db_size is None:\n",
        "#                 raise ValueError(\"You need to provide the size of the database\")\n",
        "#             if os.path.exists(self.db_path):\n",
        "#                 os.remove(self.db_path)\n",
        "#             self.generate_database(db_size)\n",
        "\n",
        "#     def generate_database(self, size: int) -> None:\n",
        "#         vectors = np.memmap(\"new_embeddings.dat\", dtype=np.float32, mode='r', shape=(size, DIMENSION))\n",
        "#         self._write_vectors_to_file(vectors)\n",
        "#         self._build_index()\n",
        "\n",
        "#     def _write_vectors_to_file(self, vectors: np.ndarray) -> None:\n",
        "#         mmap_vectors = np.memmap(self.db_path, dtype=np.float32, mode='w+', shape=vectors.shape)\n",
        "#         mmap_vectors[:] = vectors[:]\n",
        "#         mmap_vectors.flush()\n",
        "\n",
        "#     def _get_num_records(self) -> int:\n",
        "#         return os.path.getsize(self.db_path) // (DIMENSION * ELEMENT_SIZE)\n",
        "\n",
        "#     def insert_records(self, rows: Annotated[np.ndarray, (int, 64)]):\n",
        "#         num_old_records = self._get_num_records()\n",
        "#         num_new_records = len(rows)\n",
        "#         full_shape = (num_old_records + num_new_records, DIMENSION)\n",
        "#         mmap_vectors = np.memmap(self.db_path, dtype=np.float32, mode='r+', shape=full_shape)\n",
        "#         mmap_vectors[num_old_records:] = rows\n",
        "#         mmap_vectors.flush()\n",
        "#         self._build_index()\n",
        "\n",
        "#     def get_one_row(self, row_num: int) -> np.ndarray:\n",
        "#         try:\n",
        "#             offset = row_num * DIMENSION * ELEMENT_SIZE\n",
        "#             mmap_vector = np.memmap(self.db_path, dtype=np.float32, mode='r', shape=(1, DIMENSION), offset=offset)\n",
        "#             return np.array(mmap_vector[0])\n",
        "#         except Exception as e:\n",
        "#             raise\n",
        "\n",
        "#     def get_all_rows(self) -> np.ndarray:\n",
        "#         num_records = self._get_num_records()\n",
        "#         vectors = np.memmap(self.db_path, dtype=np.float32, mode='r', shape=(num_records, DIMENSION))\n",
        "#         return np.array(vectors)\n",
        "\n",
        "#     def get_all_ids_rows(self, ids) -> np.ndarray:\n",
        "#         num_records = self._get_num_records()\n",
        "#         vectors = np.memmap(self.db_path, dtype=np.float32, mode='r', shape=(num_records, DIMENSION))\n",
        "#         return vectors[ids]\n",
        "\n",
        "#     def unpack_packed_codes(self, packed_block):\n",
        "#         # packed_block: uint8 array shape (n, packed_cols)\n",
        "#         # output: uint8 array shape (n, M)\n",
        "#         # high nibble = code for 2*i (>>4), low nibble = code for 2*i+1 (&0x0F)\n",
        "#         M = 8  # number of sub-vectors (hard-coded to your m)\n",
        "#         if packed_block.ndim == 1:\n",
        "#             packed_block = packed_block.reshape(1, -1)\n",
        "#         n = packed_block.shape[0]\n",
        "#         codes = np.empty((n, M), dtype=np.uint8)\n",
        "#         # vectorized unpack\n",
        "#         high = (packed_block >> 4).astype(np.uint8)\n",
        "#         low  = (packed_block & 0x0F).astype(np.uint8)\n",
        "#         codes[:, 0::2] = high\n",
        "#         codes[:, 1::2] = low\n",
        "#         return codes\n",
        "\n",
        "#     ############################################################################\n",
        "#     # -------------------------- INDEX BUILDING ------------------------------ #\n",
        "#     ############################################################################\n",
        "#     def _build_index(self, n_coarse=256, m=8, bits=8, batch_size=5000):\n",
        "#         N, D = self._get_num_records(), DIMENSION\n",
        "#         assert D % m == 0\n",
        "#         sub_dim = D // m\n",
        "#         k_sub = 1 << bits\n",
        "\n",
        "#         data = self.get_all_rows()\n",
        "#         print(f\"Building IVF-PQ: N={N}, m={m}, k={k_sub}, nlist={n_coarse}\")\n",
        "\n",
        "#         # clean index folder\n",
        "#         if os.path.exists(self.index_path):\n",
        "#             shutil.rmtree(self.index_path)\n",
        "#         os.makedirs(self.index_path, exist_ok=True)\n",
        "\n",
        "#         # -------------------------------------------------------\n",
        "#         # 1) Train coarse KMeans\n",
        "#         # -------------------------------------------------------\n",
        "#         coarse_km = MiniBatchKMeans(\n",
        "#             n_clusters=n_coarse,\n",
        "#             batch_size=10_000,\n",
        "#             random_state=DB_SEED_NUMBER\n",
        "#         )\n",
        "#         coarse_labels = coarse_km.fit_predict(data)\n",
        "#         coarse_centroids = coarse_km.cluster_centers_.astype(np.float32)\n",
        "#         np.save(os.path.join(self.index_path, \"coarse_centroids.npy\"), coarse_centroids)\n",
        "\n",
        "#         # -------------------------------------------------------\n",
        "#         # 2) Train PQ codebooks\n",
        "#         # -------------------------------------------------------\n",
        "#         residuals = data - coarse_centroids[coarse_labels]\n",
        "#         codebook = np.zeros((m, k_sub, sub_dim), dtype=np.float32)\n",
        "\n",
        "#         for i in range(m):\n",
        "#             sub = residuals[:, i*sub_dim:(i+1)*sub_dim]\n",
        "#             km = MiniBatchKMeans(\n",
        "#                 n_clusters=k_sub,\n",
        "#                 batch_size=20_000,\n",
        "#                 random_state=DB_SEED_NUMBER\n",
        "#             )\n",
        "#             km.fit(sub)\n",
        "#             codebook[i] = km.cluster_centers_.astype(np.float32)\n",
        "\n",
        "#         np.save(os.path.join(self.index_path, \"pq_codebook.npy\"), codebook)\n",
        "\n",
        "#         # -------------------------------------------------------\n",
        "#         # 3) Encode PQ codes\n",
        "#         # -------------------------------------------------------\n",
        "#         codes = np.zeros((N, m), dtype=np.uint8)\n",
        "\n",
        "#         for i in range(m):\n",
        "#             subvecs = residuals[:, i*sub_dim:(i+1)*sub_dim]\n",
        "#             cents = codebook[i]\n",
        "#             cents_sq = np.sum(cents*cents, axis=1)\n",
        "\n",
        "#             for start in range(0, N, batch_size):\n",
        "#                 end = min(start + batch_size, N)\n",
        "#                 batch = subvecs[start:end]\n",
        "#                 batch_sq = np.sum(batch*batch, axis=1, keepdims=True)\n",
        "#                 cross = batch @ cents.T\n",
        "#                 dists = batch_sq + cents_sq[None,:] - 2*cross\n",
        "#                 codes[start:end, i] = np.argmin(dists, axis=1)\n",
        "\n",
        "#         # -------------------------------------------------------\n",
        "#         # 4) Build inverted lists\n",
        "#         # -------------------------------------------------------\n",
        "#         lists = [[] for _ in range(n_coarse)]\n",
        "#         for idx, c in enumerate(coarse_labels):\n",
        "#             lists[c].append(idx)\n",
        "\n",
        "#         offsets = np.zeros(n_coarse + 1, dtype=np.uint64)\n",
        "\n",
        "#         ivf_ids_file = os.path.join(self.index_path, \"ivf_ids.bin\")\n",
        "#         ivf_codes_file = os.path.join(self.index_path, \"ivf_codes.bin\")\n",
        "\n",
        "#         with open(ivf_ids_file, \"wb\") as f_ids, open(ivf_codes_file, \"wb\") as f_codes:\n",
        "#             count = 0\n",
        "#             for c in range(n_coarse):\n",
        "#                 ids = np.array(lists[c], dtype=np.uint32)\n",
        "#                 n_vec = ids.size\n",
        "\n",
        "#                 # prefix sum offset\n",
        "#                 offsets[c+1] = offsets[c] + n_vec\n",
        "\n",
        "#                 if n_vec == 0:\n",
        "#                     continue\n",
        "\n",
        "#                 # write raw IDs (no delta, no varint)\n",
        "#                 ids.tofile(f_ids)\n",
        "\n",
        "#                 # write raw PQ codes aligned with IDs\n",
        "#                 f_codes.write(codes[ids].astype(np.uint8).tobytes())\n",
        "\n",
        "#         offsets.tofile(os.path.join(self.index_path, \"ivfpq_offsets.bin\"))\n",
        "\n",
        "#         print(\"IVF-PQ index built (raw IDs + raw PQ codes).\")\n",
        "\n",
        "#     def retrieve(self, query, top_k=10, nprobe=64, candidate_multiplier=8):\n",
        "#         q = np.asarray(query, dtype=np.float32)\n",
        "#         m = 8\n",
        "#         sub_dim = DIMENSION // m\n",
        "#         k = 256\n",
        "\n",
        "#         index = self.index_path\n",
        "#         coarse_centroids = np.load(os.path.join(index, \"coarse_centroids.npy\"))\n",
        "#         codebook = np.load(os.path.join(index, \"pq_codebook.npy\"))\n",
        "#         offsets = np.fromfile(os.path.join(index, \"ivfpq_offsets.bin\"), dtype=np.uint64)\n",
        "\n",
        "#         nlist = coarse_centroids.shape[0]\n",
        "\n",
        "#         # ----------------------------- Probe ------------------------------\n",
        "#         diff = coarse_centroids - q\n",
        "#         cdists = np.sum(diff*diff, axis=1)\n",
        "\n",
        "#         top_list = np.argsort(cdists)[:min(nprobe, nlist)]\n",
        "\n",
        "#         # distance tables for each list later\n",
        "#         ids_file = open(os.path.join(index, \"ivf_ids.bin\"), \"rb\")\n",
        "#         codes_file = open(os.path.join(index, \"ivf_codes.bin\"), \"rb\")\n",
        "\n",
        "#         # candidate heap\n",
        "#         candidate_heap = []\n",
        "#         n_candidates = top_k * candidate_multiplier\n",
        "\n",
        "#         for c in top_list:\n",
        "#             start = int(offsets[c])\n",
        "#             end = int(offsets[c+1])\n",
        "#             n_vec = end - start\n",
        "#             if n_vec == 0:\n",
        "#                 continue\n",
        "\n",
        "#             # ----------------------------- Read IDs ------------------------------\n",
        "#             ids_file.seek(start * 4)\n",
        "#             ids = np.frombuffer(ids_file.read(n_vec*4), dtype=np.uint32)\n",
        "\n",
        "#             # --------------------------- Read PQ codes (raw) ----------------------\n",
        "#             codes_file.seek(start * m)\n",
        "#             raw_codes = np.frombuffer(codes_file.read(n_vec*m), dtype=np.uint8)\n",
        "#             raw_codes = raw_codes.reshape(n_vec, m)\n",
        "\n",
        "#             # --------------------------- Build Distance Table ---------------------\n",
        "#             centroid_c = coarse_centroids[c]\n",
        "#             q_res = q - centroid_c\n",
        "#             sub_q = q_res.reshape(m, sub_dim)\n",
        "\n",
        "#             diff = codebook - sub_q[:, None, :]\n",
        "#             dist_table = np.sum(diff*diff, axis=2)\n",
        "\n",
        "#             # ---------------------- Compute PQ distances --------------------------\n",
        "#             # fast lookup\n",
        "#             pq_dists = dist_table[np.arange(m)[:,None], raw_codes.T].sum(axis=0)\n",
        "\n",
        "#             # ---------------------- Push candidates -------------------------------\n",
        "#             for vid, dist in zip(ids, pq_dists):\n",
        "#                 if len(candidate_heap) < n_candidates:\n",
        "#                     heapq.heappush(candidate_heap, (-dist, int(vid)))\n",
        "#                 else:\n",
        "#                     if -dist > candidate_heap[0][0]:\n",
        "#                         heapq.heapreplace(candidate_heap, (-dist, int(vid)))\n",
        "\n",
        "#         ids_file.close()\n",
        "#         codes_file.close()\n",
        "\n",
        "#         # ------------------------- Exact rerank -------------------------------\n",
        "#         if not candidate_heap:\n",
        "#             return []\n",
        "\n",
        "#         candidates = {vid for _, vid in candidate_heap}\n",
        "\n",
        "#         result_heap = []\n",
        "#         q_normalized = q / np.linalg.norm(q)\n",
        "#         for vid in candidates:\n",
        "#             vec = self.get_one_row(int(vid))\n",
        "           \n",
        "#             score = np.dot(q_normalized,  vec)\n",
        "\n",
        "#             if len(result_heap) < top_k:\n",
        "#                 heapq.heappush(result_heap, (score, int(vid)))\n",
        "#             else:\n",
        "#                 if score > result_heap[0][0]:\n",
        "#                     heapq.heapreplace(result_heap, (score, int(vid)))\n",
        "\n",
        "#         result_heap.sort(reverse=True)\n",
        "\n",
        "#         result = [vid for score, vid in result_heap]\n",
        "#         print(result)\n",
        "#         return result\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import tqdm\n",
        "import heapq\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from typing import Annotated\n",
        "\n",
        "DB_SEED_NUMBER = 42\n",
        "ELEMENT_SIZE = np.dtype(np.float32).itemsize\n",
        "DIMENSION = 64\n",
        "\n",
        "class VecDB:\n",
        "    def __init__(self, database_file_path=\"saved_db.dat\", index_file_path=\"index.dat\", new_db=True, db_size=None) -> None:\n",
        "        self.db_path = database_file_path\n",
        "        self.index_path = index_file_path\n",
        "        self.dim = db_size\n",
        "        if new_db:\n",
        "            if db_size is None:\n",
        "                raise ValueError(\"You need to provide the size of the database\")\n",
        "            if os.path.exists(self.db_path):\n",
        "                os.remove(self.db_path)\n",
        "            self.generate_database(db_size)\n",
        "\n",
        "    def generate_database(self, size: int) -> None:\n",
        "        vectors = np.memmap(\"new_embeddings.dat\", dtype=np.float32, mode='r', shape=(size, DIMENSION))\n",
        "        self._write_vectors_to_file(vectors)\n",
        "        self._build_index()\n",
        "\n",
        "    def _write_vectors_to_file(self, vectors: np.ndarray) -> None:\n",
        "        mmap_vectors = np.memmap(self.db_path, dtype=np.float32, mode='w+', shape=vectors.shape)\n",
        "        mmap_vectors[:] = vectors[:]\n",
        "        mmap_vectors.flush()\n",
        "\n",
        "    def _get_num_records(self) -> int:\n",
        "        return os.path.getsize(self.db_path) // (DIMENSION * ELEMENT_SIZE)\n",
        "\n",
        "    def insert_records(self, rows: Annotated[np.ndarray, (int, 64)]):\n",
        "        num_old_records = self._get_num_records()\n",
        "        num_new_records = len(rows)\n",
        "        full_shape = (num_old_records + num_new_records, DIMENSION)\n",
        "        mmap_vectors = np.memmap(self.db_path, dtype=np.float32, mode='r+', shape=full_shape)\n",
        "        mmap_vectors[num_old_records:] = rows\n",
        "        mmap_vectors.flush()\n",
        "        self._build_index()\n",
        "\n",
        "    def get_one_row(self, row_num: int) -> np.ndarray:\n",
        "        try:\n",
        "            offset = row_num * DIMENSION * ELEMENT_SIZE\n",
        "            mmap_vector = np.memmap(self.db_path, dtype=np.float32, mode='r', shape=(1, DIMENSION), offset=offset)\n",
        "            return np.array(mmap_vector[0])\n",
        "        except Exception as e:\n",
        "            raise\n",
        "\n",
        "    def get_all_rows(self) -> np.ndarray:\n",
        "        num_records = self._get_num_records()\n",
        "        vectors = np.memmap(self.db_path, dtype=np.float32, mode='r', shape=(num_records, DIMENSION))\n",
        "        return np.array(vectors)\n",
        "\n",
        "    def get_all_ids_rows(self, ids) -> np.ndarray:\n",
        "        num_records = self._get_num_records()\n",
        "        vectors = np.memmap(self.db_path, dtype=np.float32, mode='r', shape=(num_records, DIMENSION))\n",
        "        return vectors[ids]\n",
        "\n",
        "    ############################################################################\n",
        "    # -------------------------- Retrieval (search) --------------------------- #\n",
        "    ############################################################################\n",
        "    def retrieve(self, query, top_k=10, nprobe=40, candidate_multiplier=8):\n",
        "        \"\"\"\n",
        "        IVF-PQ search:\n",
        "          - find top-nprobe coarse cells (by L2)\n",
        "          - for each cell, read list of (ids, pq-codes)\n",
        "          - compute approximate PQ distances using the *query residual* (q - coarse_centroid)\n",
        "          - keep top candidates, then re-rank by exact dot or L2 (exact)\n",
        "        \"\"\"\n",
        "        q = np.asarray(query, dtype=np.float32).ravel()\n",
        "        q_norm = np.linalg.norm(q)\n",
        "        if q_norm > 1e-12:\n",
        "            q = q / q_norm  # optional normalization if your database vectors are normalized; keep consistent with indexing\n",
        "\n",
        "        # constants (should match build)\n",
        "        D = DIMENSION\n",
        "        # load index metadata\n",
        "        index_path = self.index_path\n",
        "        coarse_centroids = np.load(os.path.join(index_path, \"coarse_centroids.npy\"))  # shape (n_coarse, D)\n",
        "       \n",
        "        n_coarse =256\n",
        "        m = 8\n",
        "        sub_dim = 8\n",
        "        k_sub = 256\n",
        "\n",
        "        # load codebook (m, k_sub, sub_dim)\n",
        "        codebook = np.load(os.path.join(index_path, \"pq_codebook.npy\"))\n",
        "        # offsets for inverted lists (uint64, length n_coarse+1), offsets in number-of-vectors units per list\n",
        "        offsets = np.fromfile(os.path.join(index_path, \"ivfpq_offsets.bin\"), dtype=np.uint64)\n",
        "        ivfpq_path = os.path.join(index_path, \"ivfpq.bin\")\n",
        "\n",
        "        # 1) find top-nprobe coarse cells by L2 distance between q and coarse_centroid\n",
        "        # compute squared L2 distances\n",
        "        diffs = coarse_centroids - q  # shape (n_coarse, D)\n",
        "        coarse_dists = np.sum(diffs * diffs, axis=1)\n",
        "        top_coarse_idx = np.argpartition(coarse_dists, nprobe - 1)[:nprobe]\n",
        "        top_coarse_idx = top_coarse_idx[np.argsort(coarse_dists[top_coarse_idx])]  # from closest to far\n",
        "        # We'll need the coarse centroids for each chosen cell\n",
        "        # Precompute query residual per chosen centroid when needed.\n",
        "\n",
        "        n_candidates = top_k * candidate_multiplier\n",
        "        candidate_heap = []  # min-heap of (approx_dist, vid) where smaller = better (L2)\n",
        "\n",
        "        # compute how many bytes the codebook takes (written at file start)\n",
        "       \n",
        "\n",
        "        # For each coarse cell, read its list and compute approximate distances\n",
        "        with open(ivfpq_path, \"rb\") as f:\n",
        "            for c in top_coarse_idx:\n",
        "                start = int(offsets[c])\n",
        "                end = int(offsets[c + 1])\n",
        "                n_vec = end - start\n",
        "                if n_vec == 0:\n",
        "                    continue\n",
        "\n",
        "                # compute residual query: q - centroid_c\n",
        "                centroid_c = coarse_centroids[c]\n",
        "                query_residual = q - centroid_c\n",
        "                sub_q = query_residual.reshape(m, sub_dim)  # shape (m, sub_dim)\n",
        "\n",
        "                # compute dist_table for this centroid: for each subvector slot, a vector of size k_sub: || sub_q - codebook[i,k] ||^2\n",
        "                # codebook shape (m, k_sub, sub_dim)\n",
        "                # dist_table shape (m, k_sub)\n",
        "                diff = codebook - sub_q[:, np.newaxis, :]  # broadcasting: (m,k_sub,sub_dim) - (m,1,sub_dim)\n",
        "                dist_table = np.sum(diff * diff, axis=2)  # (m, k_sub)\n",
        "                del diff\n",
        "\n",
        "                # position in file to read this list:\n",
        "                # file layout: [codebook bytes] [for each cell: ids(uint32) then codes(uint8 flattened)] contiguously\n",
        "                # We stored each cell sequentially in that order during build.\n",
        "                # We need to know the byte start for this cell. During building we wrote lists in order and tracked offsets in \"vector counts\".\n",
        "                # To compute the byte offset we need to walk previous lists lengths; but we stored offsets in vector-count units.\n",
        "                # We'll compute byte-position as: codebook_nbytes + sum_{i<cell}(len(list_i) * (4 + m))\n",
        "                # That can be computed by reading offsets array (which stores cumulative counts) => offsets[c] gives number of vectors before cell c.\n",
        "                byte_pos =  int(offsets[c]) * (4 + m)  # each vector stored as uint32 id (4) + m bytes codes (uint8)\n",
        "                f.seek(byte_pos)\n",
        "                block_bytes = f.read(n_vec * (4 + m))\n",
        "\n",
        "                # parse ids and codes\n",
        "                ids = np.frombuffer(block_bytes, dtype=np.uint32, count=n_vec, offset=0)\n",
        "                codes = np.frombuffer(block_bytes, dtype=np.uint8, offset=n_vec * 4).reshape(n_vec, m)\n",
        "                # now compute approx distances per vector using dist_table\n",
        "                # dist_table: (m, k_sub)\n",
        "                # codes.T shape: (m, n_vec)\n",
        "                # pick dist_table[range(m), codes[i]] for each i\n",
        "                # Efficient: dist_table[np.arange(m)[:,None], codes.T] -> shape (m, n_vec) -> sum over m -> (n_vec,)\n",
        "                picked = dist_table[np.arange(m)[:, None], codes.T]  # (m, n_vec)\n",
        "                approx_dists = picked.sum(axis=0)  # (n_vec,)\n",
        "                del picked, dist_table, block_bytes\n",
        "\n",
        "                # push candidates into a min-heap (we want smallest L2 distances)\n",
        "                for dist, vid in zip(approx_dists, ids):\n",
        "                    if len(candidate_heap) < n_candidates:\n",
        "                        heapq.heappush(candidate_heap, (-dist, int(vid)))  # use -dist to keep largest negative = smallest dist if we want max-heap semantics\n",
        "                    else:\n",
        "                        # candidate_heap holds negative-dist; smallest dist corresponds to largest -dist\n",
        "                        if -dist > candidate_heap[0][0]:\n",
        "                            heapq.heapreplace(candidate_heap, (-dist, int(vid)))\n",
        "\n",
        "        if not candidate_heap:\n",
        "            return []\n",
        "\n",
        "        # convert candidate_heap to list of unique vids (we might deduplicate)\n",
        "        cand = {vid for _, vid in candidate_heap}\n",
        "        # now exact re-ranking: load each candidate vector and compute exact similarity or exact L2\n",
        "        # We'll compute dot(q, vec) if vectors and q are normalized; otherwise compute negative L2 for ranking.\n",
        "        result_heap = []  # min-heap of (score, vid) where larger score is better\n",
        "\n",
        "        # Decide exact metric: if your DB vectors are normalized, use dot product; else L2 (smaller is better)\n",
        "        # We'll use dot product assuming normalization above. Keep consistent with indexing/build normalization.\n",
        "        for vid in cand:\n",
        "            vec = self.get_one_row(int(vid)).astype(np.float32)\n",
        "            # If your vectors were normalized at index time, use dot; otherwise compute -L2 to get descending order\n",
        "            exact_score = float(np.dot(vec, q))\n",
        "            if len(result_heap) < top_k:\n",
        "                heapq.heappush(result_heap, (exact_score, int(vid)))\n",
        "            else:\n",
        "                if exact_score > result_heap[0][0]:\n",
        "                    heapq.heapreplace(result_heap, (exact_score, int(vid)))\n",
        "\n",
        "        # sort results descending by score\n",
        "        result_heap.sort(reverse=True)\n",
        "        final_ids = [int(vid) for score, vid in result_heap]\n",
        "\n",
        "        return final_ids\n",
        "\n",
        "    ############################################################################\n",
        "    # ---------------------------- Index building ---------------------------- #\n",
        "    ############################################################################\n",
        "    def _build_index(self, n_coarse=256, m=8, bits=8, batch_size=5000):\n",
        "        \"\"\"\n",
        "        Build IVF-PQ index memory-safely.\n",
        "        - n_coarse: number of coarse clusters\n",
        "        - m: number of PQ sub-vectors\n",
        "        - bits: number of bits per sub-vector (k_sub = 2^bits)\n",
        "        - batch_size: used for PQ encoding batching\n",
        "        \"\"\"\n",
        "        N, D = self._get_num_records(), DIMENSION\n",
        "        assert D % m == 0, f\"D={D} must be divisible by m={m}\"\n",
        "        sub_dim = D // m\n",
        "        k_sub = 1 << bits\n",
        "\n",
        "        data = self.get_all_rows()  # careful: loads all to memory; for huge DB you'd batch-training differently\n",
        "\n",
        "        # remove old index\n",
        "        if os.path.exists(self.index_path):\n",
        "            shutil.rmtree(self.index_path)\n",
        "        os.makedirs(self.index_path, exist_ok=True)\n",
        "\n",
        "        # === 1. Coarse quantizer (L2) ===\n",
        "        print(\"Training coarse quantizer...\")\n",
        "\n",
        "        coarse_km = MiniBatchKMeans(n_clusters=n_coarse, batch_size=10_000, random_state=DB_SEED_NUMBER)\n",
        "        coarse_labels = coarse_km.fit_predict(data)\n",
        "        coarse_cents = coarse_km.cluster_centers_.astype(np.float32)  # shape (n_coarse, D)\n",
        "\n",
        "\n",
        "        #normalize centroid \n",
        "        coarse_cents = coarse_cents / (np.linalg.norm(coarse_cents, axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "        np.save(os.path.join(self.index_path, \"coarse_centroids.npy\"), coarse_cents)\n",
        "\n",
        "        # === 2. Compute residuals and train PQ codebooks on residuals ===\n",
        "        print(\"Training PQ codebooks on residuals...\")\n",
        "        # residual for each vector: x - centroid_of_cell\n",
        "\n",
        "        residuals = data - coarse_cents[coarse_labels]\n",
        "        residuals = residuals / (np.linalg.norm(residuals, axis=1, keepdims=True) + 1e-12)\n",
        "        \n",
        "        codebook = np.zeros((m, k_sub, sub_dim), dtype=np.float32)\n",
        "\n",
        "        for i in range(m):\n",
        "            subvecs = residuals[:, i * sub_dim:(i + 1) * sub_dim]\n",
        "            km = MiniBatchKMeans(n_clusters=k_sub, batch_size=20_000, random_state=DB_SEED_NUMBER)\n",
        "            #normalize subvecs\n",
        "           \n",
        "            km.fit(subvecs)\n",
        "            codebook[i] = km.cluster_centers_.astype(np.float32)\n",
        "\n",
        "        \n",
        "        np.save(os.path.join(self.index_path, \"pq_codebook.npy\"), codebook)\n",
        "       \n",
        "        # === 3. Encode all vectors using PQ (batching to avoid huge memory) ===\n",
        "        print(\"Encoding vectors into PQ codes (memory-safe)...\")\n",
        "        codes = np.zeros((N, m), dtype=np.uint8)  # MUST be uint8 to store codes 0..k_sub-1\n",
        "        for i in range(m):\n",
        "            subvecs = residuals[:, i * sub_dim:(i + 1) * sub_dim]  # residual sub-vector\n",
        "            cents = codebook[i]  # shape (k_sub, sub_dim)\n",
        "            cents_sq = np.sum(cents * cents, axis=1)  # (k_sub,)\n",
        "\n",
        "            for start in range(0, N, batch_size):\n",
        "                end = min(start + batch_size, N)\n",
        "                batch = subvecs[start:end]  # (B, sub_dim)\n",
        "                batch_sq = np.sum(batch * batch, axis=1, keepdims=True)  # (B,1)\n",
        "                cross = batch @ cents.T  # (B, k_sub)\n",
        "                dists = batch_sq + cents_sq[None, :] - 2.0 * cross  # (B, k_sub)\n",
        "                codes[start:end, i] = np.argmin(dists, axis=1).astype(np.uint8)\n",
        "\n",
        "        # === 4. Build inverted lists (IVF) ===\n",
        "        print(\"Building inverted lists...\")\n",
        "        lists = [[] for _ in range(n_coarse)]\n",
        "        for idx, c in enumerate(coarse_labels):\n",
        "            lists[c].append(idx)\n",
        "\n",
        "        # === 5. Write index to disk: layout = [codebook bytes][for each cell: ids(uint32) then codes(uint8*m)] ===\n",
        "        print(\"Writing final index...\")\n",
        "        offsets = np.zeros(n_coarse + 1, dtype=np.uint64)\n",
        "        ivfpq_file = os.path.join(self.index_path, \"ivfpq.bin\")\n",
        "        with open(ivfpq_file, \"wb\") as f:\n",
        "           \n",
        "\n",
        "            # write each cell's ids then codes\n",
        "            total_written = 0\n",
        "            for c in range(n_coarse):\n",
        "                ids = np.array(lists[c], dtype=np.uint32)\n",
        "                n_vec = ids.size\n",
        "                offsets[c + 1] = offsets[c] + n_vec\n",
        "                if n_vec == 0:\n",
        "                    continue\n",
        "                subset_codes = codes[ids].astype(np.uint8)  # shape (n_vec, m)\n",
        "                ids.tofile(f)  # uint32 sequence\n",
        "                subset_codes.tofile(f)  # raw uint8 bytes (n_vec * m)\n",
        "                total_written += n_vec\n",
        "\n",
        "        offsets.tofile(os.path.join(self.index_path, \"ivfpq_offsets.bin\"))\n",
        "        # Save meta already saved\n",
        "        print(f\"IVF-PQ built! Index file: {ivfpq_file}, size: {os.path.getsize(ivfpq_file) / 1e6:.1f} MB\")\n",
        "#ivf pq better \n",
        "\n",
        "\n",
        "# 10M\tscore\t-192.33333333333334\ttime\t0.62\tRAM\t1.29 MB\n",
        "\n",
        "\n",
        "# 1M\tscore\t-108.66666666666667\ttime\t0.07\tRAM\t0.23 MB\n",
        "# 10M\tscore\t-192.33333333333334\ttime\t0.65\tRAM\t1.39 MB\n",
        "# 15M\tscore\t-174.0\ttime\t1.21\tRAM\t3.21 MB\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 1M\tscore\t-108.66666666666667\ttime\t0.23\tRAM\t0.23 MB   before normailzation distance\n",
        "\n",
        "# 1M\tscore\t-108.66666666666667\ttime\t0.07\tRAM\t0.16 MB    after \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2SPhELotKXaO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "DIMENSION = 64\n",
        "def create_other_DB_size(input_file, output_file, target_rows, embedding_dim = DIMENSION):\n",
        "    # Configuration\n",
        "    dtype = 'float32'\n",
        "\n",
        "    # 1. Determine the shape of the source file\n",
        "    # We calculate rows based on file size to be safe, or you can hardcode 20_000_000\n",
        "    file_size_bytes = os.path.getsize(input_file)\n",
        "    itemsize = np.dtype(dtype).itemsize\n",
        "    total_rows = file_size_bytes // (embedding_dim * itemsize)\n",
        "\n",
        "    print(f\"Source detected: {total_rows} rows.\")\n",
        "\n",
        "    # 2. Open source in read mode ('r')\n",
        "    # This uses almost 0 RAM, it just points to the file on disk\n",
        "    source_memmap = np.memmap(\n",
        "        input_file,\n",
        "        dtype=dtype,\n",
        "        mode='r',\n",
        "        shape=(total_rows, embedding_dim)\n",
        "    )\n",
        "\n",
        "    # 3. Create the new file in write mode ('w+')\n",
        "    # We define the shape as the target size (1M, 64)\n",
        "    dest_memmap = np.memmap(\n",
        "        output_file,\n",
        "        dtype=dtype,\n",
        "        mode='w+',\n",
        "        shape=(target_rows, embedding_dim)\n",
        "    )\n",
        "\n",
        "    # 4. Copy the data\n",
        "    # This transfers the binary blocks directly\n",
        "    print(\"Copying data...\")\n",
        "    dest_memmap[:] = source_memmap[:target_rows]\n",
        "\n",
        "    # 5. Flush to save changes to disk\n",
        "    dest_memmap.flush()\n",
        "\n",
        "    print(f\"Success! Saved first {target_rows} rows to {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "PATH_DB_VECTORS_20M = \"OpenSubtitles_en_20M_emb_64.dat\"\n",
        "PATH_DB_VECTORS_10M = \"OpenSubtitles_en_10M_emb_64.dat\"\n",
        "PATH_DB_VECTORS_1M = \"OpenSubtitles_en_1M_emb_64.dat\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_1uvNlMELYk0"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(PATH_DB_VECTORS_1M):\n",
        "    create_other_DB_size(PATH_DB_VECTORS_20M, PATH_DB_VECTORS_1M, 1_000_000)\n",
        "if not os.path.exists(PATH_DB_VECTORS_10M):\n",
        "    create_other_DB_size(PATH_DB_VECTORS_20M, PATH_DB_VECTORS_10M, 10_000_000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShuPR-gGlX3f"
      },
      "source": [
        "These are the functions for running and reporting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Sg2vfYgeyavn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "from memory_profiler import memory_usage\n",
        "import gc\n",
        "\n",
        "@dataclass\n",
        "class Result:\n",
        "    run_time: float\n",
        "    top_k: int\n",
        "    db_ids: List[int]\n",
        "    actual_ids: List[int]\n",
        "\n",
        "def run_queries(db, queries, top_k, actual_ids, num_runs):\n",
        "    \"\"\"\n",
        "    Run queries on the database and record results for each query.\n",
        "\n",
        "    Parameters:\n",
        "    - db: Database instance to run queries on.\n",
        "    - queries: List of query vectors.\n",
        "    - top_k: Number of top results to retrieve.\n",
        "    - actual_ids: List of actual results to evaluate accuracy.\n",
        "    - num_runs: Number of query executions to perform for testing.\n",
        "\n",
        "    Returns:\n",
        "    - List of Result\n",
        "    \"\"\"\n",
        "    global results\n",
        "    results = []\n",
        "    for i in range(num_runs):\n",
        "        tic = time.time()\n",
        "        db_ids = db.retrieve(queries[i], top_k)\n",
        "        toc = time.time()\n",
        "        run_time = toc - tic\n",
        "        results.append(Result(run_time, top_k, db_ids, actual_ids[i]))\n",
        "    return results\n",
        "\n",
        "def memory_usage_run_queries(args):\n",
        "    \"\"\"\n",
        "    Run queries and measure memory usage during the execution.\n",
        "\n",
        "    Parameters:\n",
        "    - args: Arguments to be passed to the run_queries function.\n",
        "\n",
        "    Returns:\n",
        "    - results: The results of the run_queries.\n",
        "    - memory_diff: The difference in memory usage before and after running the queries.\n",
        "    \"\"\"\n",
        "    global results\n",
        "    mem_before = max(memory_usage())\n",
        "    mem = memory_usage(proc=(run_queries, args, {}), interval = 1e-3)\n",
        "    return results, max(mem) - mem_before\n",
        "\n",
        "def evaluate_result(results: List[Result]):\n",
        "    \"\"\"\n",
        "    Evaluate the results based on accuracy and runtime.\n",
        "    Scores are negative. So getting 0 is the best score.\n",
        "\n",
        "    Parameters:\n",
        "    - results: A list of Result objects\n",
        "\n",
        "    Returns:\n",
        "    - avg_score: The average score across all queries.\n",
        "    - avg_runtime: The average runtime for all queries.\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "    run_time = []\n",
        "    for res in results:\n",
        "        run_time.append(res.run_time)\n",
        "        # case for retireving number not equal to top_k, socre will be the lowest\n",
        "        if len(set(res.db_ids)) != res.top_k or len(res.db_ids) != res.top_k:\n",
        "            scores.append( -1 * len(res.actual_ids) * res.top_k)\n",
        "            continue\n",
        "        score = 0\n",
        "        for id in res.db_ids:\n",
        "            try:\n",
        "                ind = res.actual_ids.index(id)\n",
        "                if ind > res.top_k * 3:\n",
        "                    score -= ind\n",
        "            except:\n",
        "                score -= len(res.actual_ids)\n",
        "        scores.append(score)\n",
        "\n",
        "    return sum(scores) / len(scores), sum(run_time) / len(run_time)\n",
        "\n",
        "def get_actual_ids_first_k(actual_sorted_ids, k):\n",
        "    \"\"\"\n",
        "    Retrieve the IDs from the sorted list of actual IDs.\n",
        "    actual IDs has the top_k for the 20 M database but for other databases we have to remove the numbers higher than the max size of the DB.\n",
        "\n",
        "    Parameters:\n",
        "    - actual_sorted_ids: A list of lists containing the sorted actual IDs for each query.\n",
        "    - k: The DB size.\n",
        "\n",
        "    Returns:\n",
        "    - List of lists containing the actual IDs for each query for this DB.\n",
        "    \"\"\"\n",
        "    return [[id for id in actual_sorted_ids_one_q if id < k] for actual_sorted_ids_one_q in actual_sorted_ids]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrOlipAOmy9K"
      },
      "source": [
        "This code to actually run the class you have been implemented. The `VecDB` class should take the database path, and index path that you provided.<br>\n",
        "Note at the submission I'll not run the insert records. <br>\n",
        "The query istelf will be changed at submissions day but not the DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Yi34qMLHsqCn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "d:\\Apps\\anaconda\\Lib\\tracemalloc.py:560: size=296 B (+296 B), count=2 (+2), average=148 B\n",
            "d:\\Apps\\anaconda\\Lib\\tracemalloc.py:423: size=296 B (+296 B), count=2 (+2), average=148 B\n",
            "C:\\Users\\m\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3517: size=296 B (+0 B), count=1 (+0), average=296 B\n",
            "d:\\Apps\\anaconda\\Lib\\codeop.py:126: size=286 B (+0 B), count=2 (+0), average=143 B\n",
            "C:\\Users\\m\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3577: size=160 B (+0 B), count=1 (+0), average=160 B\n"
          ]
        }
      ],
      "source": [
        "# check memory usage for the import line independently\n",
        "import tracemalloc\n",
        "tracemalloc.start()\n",
        "start_snapshot = tracemalloc.take_snapshot()\n",
        "\n",
        "# from vec_db import VecDB\n",
        "\n",
        "end_snapshot = tracemalloc.take_snapshot()\n",
        "stats = end_snapshot.compare_to(start_snapshot, 'lineno')\n",
        "for stat in stats[:5]:  # show top differences\n",
        "    print(stat)\n",
        "\n",
        "tracemalloc.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "g-hCaQNqlBP0"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "to_print_arr = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Team Number 3\n"
          ]
        }
      ],
      "source": [
        "print(\"Team Number\", 3)\n",
        "database_info = {\n",
        "    \"1M\": {\n",
        "        \"database_file_path\": PATH_DB_VECTORS_1M,\n",
        "        \"index_file_path\": \"index_1M_edit\"\n",
        "        \"\",\n",
        "        \"size\": 10**6\n",
        "    },\n",
        "    # \"10M\": {\n",
        "    #     \"database_file_path\": PATH_DB_VECTORS_10M,\n",
        "    #     \"index_file_path\": \"index_10M_edit\",\n",
        "    #     \"size\": 10 * 10**6\n",
        "    # },\n",
        "    # \"20M\": {\n",
        "    #     \"database_file_path\": PATH_DB_VECTORS_1M,\n",
        "    #     \"index_file_path\": \"index_20M_edit\",\n",
        "    #     \"size\": 20 * 10**6\n",
        "    # }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "782"
            ]
          },
          "execution_count": 109,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training coarse quantizer...\n",
            "Training PQ codebooks on residuals...\n",
            "Encoding vectors into PQ codes (memory-safe)...\n",
            "Building inverted lists...\n",
            "Writing final index...\n",
            "IVF-PQ built! Index file: index_1M_edit\\ivfpq.bin, size: 12.0 MB\n"
          ]
        }
      ],
      "source": [
        "for db_name, info in database_info.items():\n",
        "    db = VecDB(database_file_path = info[\"database_file_path\"], index_file_path = info[\"index_file_path\"], new_db = False)\n",
        "    db._build_index()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
