{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "G44iH6jnObEj"
      },
      "outputs": [],
      "source": [
        "QUERY_SEED_NUMBER = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "miaF8OeibbIL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "DB_SEED_NUMBER = 42\n",
        "ELEMENT_SIZE = np.dtype(np.float32).itemsize\n",
        "DIMENSION = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import tqdm\n",
        "import heapq\n",
        "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
        "from typing import Annotated\n",
        "import time\n",
        "\n",
        "DB_SEED_NUMBER = 42\n",
        "ELEMENT_SIZE = np.dtype(np.float32).itemsize\n",
        "DIMENSION = 64\n",
        "\n",
        "class VecDB:\n",
        "    def __init__(self, database_file_path = \"saved_db.dat\", index_file_path = \"index.dat\", new_db = True, db_size = None) -> None:\n",
        "        self.db_path = database_file_path\n",
        "        self.index_path = index_file_path\n",
        "        if new_db:\n",
        "            if db_size is None:\n",
        "                raise ValueError(\"You need to provide the size of the database\")\n",
        "            # delete the old DB file if exists\n",
        "            if os.path.exists(self.db_path):\n",
        "                os.remove(self.db_path)\n",
        "            self.generate_database(db_size)\n",
        "    \n",
        "    def generate_database(self, size: int) -> None:\n",
        "        # rng = np.random.default_rng(DB_SEED_NUMBER)\n",
        "        vectors = np.memmap(\"new_embeddings.dat\", dtype=np.float32, mode='r', shape=(size, DIMENSION))\n",
        "        self._write_vectors_to_file(vectors)\n",
        "        self._build_index()\n",
        "\n",
        "    def _write_vectors_to_file(self, vectors: np.ndarray) -> None:\n",
        "        mmap_vectors = np.memmap(self.db_path, dtype=np.float32, mode='w+', shape=vectors.shape)\n",
        "        mmap_vectors[:] = vectors[:]\n",
        "        mmap_vectors.flush()\n",
        "\n",
        "    def _get_num_records(self) -> int:\n",
        "        return os.path.getsize(self.db_path) // (DIMENSION * ELEMENT_SIZE)\n",
        "\n",
        "    def insert_records(self, rows: Annotated[np.ndarray, (int, 64)]):\n",
        "        num_old_records = self._get_num_records()\n",
        "        num_new_records = len(rows)\n",
        "        full_shape = (num_old_records + num_new_records, DIMENSION)\n",
        "        mmap_vectors = np.memmap(self.db_path, dtype=np.float32, mode='r+', shape=full_shape)\n",
        "        mmap_vectors[num_old_records:] = rows\n",
        "        mmap_vectors.flush()\n",
        "        #TODO: might change to call insert in the index, if you need\n",
        "        self._build_index()\n",
        "\n",
        "    def get_one_row(self, row_num: int) -> np.ndarray:\n",
        "        # This function is only load one row in memory\n",
        "        try:\n",
        "            offset = row_num * DIMENSION * ELEMENT_SIZE\n",
        "            mmap_vector = np.memmap(self.db_path, dtype=np.float32, mode='r', shape=(1, DIMENSION), offset=offset)\n",
        "            return np.array(mmap_vector[0])\n",
        "        except Exception as e:\n",
        "            return f\"An error occurred: {e}\"\n",
        "\n",
        "    def get_all_rows(self) -> np.ndarray:\n",
        "        # Take care this load all the data in memory\n",
        "        num_records = self._get_num_records()\n",
        "        vectors = np.memmap(self.db_path, dtype=np.float32, mode='r', shape=(num_records, DIMENSION))\n",
        "        return np.array(vectors)\n",
        "    \n",
        "    def get_all_ids_rows_optimized(self, ids):\n",
        "        ids = np.array(ids)\n",
        "        num_records = self._get_num_records()\n",
        "\n",
        "        sorted_idx = np.argsort(ids)\n",
        "        sorted_ids = ids[sorted_idx]\n",
        "\n",
        "        base = sorted_ids[0]\n",
        "        row_size_bytes = DIMENSION * np.dtype(np.float32).itemsize\n",
        "        offset = base * row_size_bytes\n",
        "\n",
        "        # memmap starting from the base\n",
        "        vectors = np.memmap(\n",
        "            self.db_path, dtype=np.float32, mode='r',\n",
        "            offset=offset,\n",
        "            shape=(num_records - base, DIMENSION)\n",
        "        )\n",
        "\n",
        "        local_ids = sorted_ids - base\n",
        "        \n",
        "        result = np.empty((len(ids), DIMENSION), dtype=np.float32)\n",
        "        result[sorted_idx] = vectors[local_ids]\n",
        "\n",
        "        del vectors\n",
        "        return result\n",
        "    \n",
        "    \n",
        "    \n",
        "    def _cal_score(self, vec1, vec2):\n",
        "        dot_product = np.dot(vec1, vec2)\n",
        "        norm_vec1 = np.linalg.norm(vec1)\n",
        "        norm_vec2 = np.linalg.norm(vec2)\n",
        "        cosine_similarity = dot_product / (norm_vec1 * norm_vec2)\n",
        "        return cosine_similarity\n",
        "\n",
        "####################################################################################\n",
        "####################################################################################\n",
        "####################################################################################\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    def retrieve(self, query, top_k=5, n_probe_level2=5, n_probe_level1=6, chunk_size=50):\n",
        "        self.no_centroids = 7000\n",
        "        self.no_level2_centroids = 80\n",
        "        self.index_path = f\"index_10M_{self.no_level2_centroids}_{self.no_centroids}_centroids\"\n",
        "\n",
        "        query = np.asarray(query, dtype=np.float32).squeeze()\n",
        "        query_norm = np.linalg.norm(query)\n",
        "        if query_norm == 0:\n",
        "            query_norm = 1.0\n",
        "        normalized_query = query / query_norm\n",
        "\n",
        "        centroids_level2_path = os.path.join(self.index_path, \"centroids_level2.npy\")\n",
        "        centroids_level1_path = os.path.join(self.index_path, \"centroids.npy\")\n",
        "        if not os.path.exists(centroids_level2_path) or not os.path.exists(centroids_level1_path):\n",
        "            return []\n",
        "\n",
        "        # Load headers\n",
        "        level2_header_arr = np.fromfile(\n",
        "            os.path.join(self.index_path, \"level2_header.bin\"), dtype=np.uint32\n",
        "        ).reshape(-1, 2)\n",
        "        index_header_arr = np.fromfile(\n",
        "            os.path.join(self.index_path, \"index_header.bin\"), dtype=np.uint32\n",
        "        ).reshape(-1, 2)\n",
        "        flat_index_path = os.path.join(self.index_path, \"all_indices.bin\")\n",
        "\n",
        "        # Load level-2 centroids\n",
        "        centroids_level2 = np.load(centroids_level2_path, mmap_mode=\"r\")\n",
        "        sims_level2 = centroids_level2.dot(normalized_query)\n",
        "        # pick top n_probe_level2 level2 centroids\n",
        "        nearest_level2 = np.argpartition(sims_level2, -n_probe_level2)[-n_probe_level2:]\n",
        "        del sims_level2, centroids_level2\n",
        "\n",
        "        centroids_level1 = np.load(centroids_level1_path, mmap_mode=\"r\")\n",
        "        top_heap = []\n",
        "\n",
        "        # Iterate over selected top-level clusters\n",
        "        for lvl2_idx in nearest_level2:\n",
        "            offset_lvl2, length_lvl2 = level2_header_arr[lvl2_idx]\n",
        "            if length_lvl2 == 0:\n",
        "                continue\n",
        "\n",
        "            # slice first-level centroids for this level2 cluster\n",
        "            level1_start = offset_lvl2\n",
        "            level1_end = offset_lvl2 + length_lvl2\n",
        "            sims_level1 = centroids_level1[level1_start:level1_end].dot(normalized_query)\n",
        "\n",
        "            # pick top n_probe_level1 first-level centroids\n",
        "            nearest_first_level = np.argpartition(sims_level1, -n_probe_level1)[-n_probe_level1:]\n",
        "            del sims_level1\n",
        "\n",
        "            for idx in nearest_first_level:\n",
        "                # map to global first-level index\n",
        "                c = level1_start + idx\n",
        "                offset, length = index_header_arr[c]\n",
        "                if length == 0:\n",
        "                    continue\n",
        "\n",
        "                # process vectors in chunks\n",
        "                for start in range(0, length, chunk_size):\n",
        "                    cur_len = min(chunk_size, length - start)\n",
        "                    ids_mm = np.memmap(\n",
        "                        flat_index_path,\n",
        "                        dtype=np.uint32,\n",
        "                        mode=\"r\",\n",
        "                        offset=offset + start * np.dtype(np.uint32).itemsize,\n",
        "                        shape=(cur_len,)\n",
        "                    )\n",
        "                    chunk_ids = ids_mm[:]\n",
        "                    del ids_mm\n",
        "\n",
        "                    vecs = self.get_all_ids_rows_optimized(chunk_ids)\n",
        "                    scores = vecs.dot(normalized_query)\n",
        "\n",
        "                    for score, id in zip(scores, chunk_ids):\n",
        "                        if len(top_heap) < top_k:\n",
        "                            heapq.heappush(top_heap, (score, id))\n",
        "                        else:\n",
        "                            heapq.heappushpop(top_heap, (score, id))\n",
        "\n",
        "                    del scores, chunk_ids, vecs\n",
        "\n",
        "        # extract top-k sorted\n",
        "        results = [idx for score, idx in heapq.nlargest(top_k, top_heap)]\n",
        "        del top_heap\n",
        "        return results\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    def _build_index(self):\n",
        "        self.no_centroids = 7000\n",
        "        self.no_level2_centroids = 80\n",
        "        self.index_path = f\"index_10M_{self.no_level2_centroids}_{self.no_centroids}_centroids\"\n",
        "\n",
        "        data = self.get_all_rows()\n",
        "\n",
        "        # 1-level clustering\n",
        "        kmeans = MiniBatchKMeans(\n",
        "            n_clusters=self.no_centroids,\n",
        "            init=\"k-means++\",\n",
        "            batch_size=10_000,\n",
        "            random_state=42\n",
        "        )\n",
        "        kmeans.fit(data)\n",
        "        labels = kmeans.labels_\n",
        "        centers = kmeans.cluster_centers_.astype(np.float32)\n",
        "        del data, kmeans\n",
        "\n",
        "        if os.path.exists(self.index_path):\n",
        "            shutil.rmtree(self.index_path)\n",
        "        os.makedirs(self.index_path, exist_ok=True)\n",
        "\n",
        "        cluster_infos = [(cid, np.where(labels == cid)[0].astype(np.uint32))\n",
        "                        for cid in range(self.no_centroids)]\n",
        "\n",
        "        # 2-level clustering\n",
        "        kmeans2 = MiniBatchKMeans(\n",
        "            n_clusters=self.no_level2_centroids,\n",
        "            init=\"k-means++\",\n",
        "            batch_size=1000,\n",
        "            random_state=42,\n",
        "        )\n",
        "        kmeans2.fit(centers)\n",
        "        centers2 = kmeans2.cluster_centers_.astype(np.float32)\n",
        "        labels2 = kmeans2.labels_\n",
        "        cluster_level2_infos = [(cid, np.where(labels2 == cid)[0].astype(np.uint32))\n",
        "                            for cid in range(self.no_level2_centroids)]\n",
        "\n",
        "        reordered_centers = []\n",
        "        reordered_cluster_infos = []\n",
        "        for _, inds in cluster_level2_infos:\n",
        "            for ind in inds:\n",
        "                reordered_centers.append(centers[ind])\n",
        "                reordered_cluster_infos.append(cluster_infos[ind])\n",
        "        centers = np.array(reordered_centers, dtype=np.float32)\n",
        "        cluster_infos = reordered_cluster_infos\n",
        "        del labels, labels2\n",
        "        del reordered_centers, reordered_cluster_infos\n",
        "\n",
        "        header = []\n",
        "        flat_path = os.path.join(self.index_path, \"all_indices.bin\")\n",
        "        with open(flat_path, \"wb\") as f:\n",
        "            offset = 0\n",
        "            for _, inds in cluster_infos:\n",
        "                length = inds.size\n",
        "                f.write(inds.tobytes())\n",
        "                header.append([offset, length])\n",
        "                offset += length * inds.dtype.itemsize\n",
        "        header_matrix = np.array(header, dtype=np.uint32)\n",
        "        header_matrix.tofile(os.path.join(self.index_path, \"index_header.bin\"))\n",
        "\n",
        "        # save level2 header (offset, length) for easy slicing later\n",
        "        level2_header = []\n",
        "        offset = 0\n",
        "        for _, inds in cluster_level2_infos:\n",
        "            length = len(inds)  \n",
        "            level2_header.append([offset, length])\n",
        "            offset += length\n",
        "        np.array(level2_header, dtype=np.uint32).tofile(os.path.join(self.index_path, \"level2_header.bin\"))\n",
        "\n",
        "        # normalize centers\n",
        "        centers /= (np.linalg.norm(centers, axis=1, keepdims=True) + 1e-12)\n",
        "        np.save(os.path.join(self.index_path, \"centroids.npy\"), centers)\n",
        "\n",
        "        centers2 /= (np.linalg.norm(centers2, axis=1, keepdims=True) + 1e-12)\n",
        "        np.save(os.path.join(self.index_path, \"centroids_level2.npy\"), centers2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Sg2vfYgeyavn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "# from vec_db import VecDB\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "from memory_profiler import memory_usage\n",
        "import gc\n",
        "\n",
        "@dataclass\n",
        "class Result:\n",
        "    run_time: float\n",
        "    top_k: int\n",
        "    db_ids: List[int]\n",
        "    actual_ids: List[int]\n",
        "\n",
        "def run_queries(db, queries, top_k, actual_ids, num_runs):\n",
        "    \"\"\"\n",
        "    Run queries on the database and record results for each query.\n",
        "\n",
        "    Parameters:\n",
        "    - db: Database instance to run queries on.\n",
        "    - queries: List of query vectors.\n",
        "    - top_k: Number of top results to retrieve.\n",
        "    - actual_ids: List of actual results to evaluate accuracy.\n",
        "    - num_runs: Number of query executions to perform for testing.\n",
        "\n",
        "    Returns:\n",
        "    - List of Result\n",
        "    \"\"\"\n",
        "    global results\n",
        "    results = []\n",
        "    for i in range(num_runs):\n",
        "        tic = time.time()\n",
        "        db_ids = db.retrieve(queries[i], top_k)\n",
        "        toc = time.time()\n",
        "        run_time = toc - tic\n",
        "        results.append(Result(run_time, top_k, db_ids, actual_ids[i]))\n",
        "    return results\n",
        "\n",
        "def memory_usage_run_queries(args):\n",
        "    \"\"\"\n",
        "    Run queries and measure memory usage during the execution.\n",
        "\n",
        "    Parameters:\n",
        "    - args: Arguments to be passed to the run_queries function.\n",
        "\n",
        "    Returns:\n",
        "    - results: The results of the run_queries.\n",
        "    - memory_diff: The difference in memory usage before and after running the queries.\n",
        "    \"\"\"\n",
        "    global results\n",
        "    mem_before = max(memory_usage())\n",
        "    mem = memory_usage(proc=(run_queries, args, {}), interval = 1e-3)\n",
        "    return results, max(mem) - mem_before\n",
        "\n",
        "def evaluate_result(results: List[Result]):\n",
        "    \"\"\"\n",
        "    Evaluate the results based on accuracy and runtime.\n",
        "    Scores are negative. So getting 0 is the best score.\n",
        "\n",
        "    Parameters:\n",
        "    - results: A list of Result objects\n",
        "\n",
        "    Returns:\n",
        "    - avg_score: The average score across all queries.\n",
        "    - avg_runtime: The average runtime for all queries.\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "    run_time = []\n",
        "    for res in results:\n",
        "        run_time.append(res.run_time)\n",
        "        # case for retireving number not equal to top_k, socre will be the lowest\n",
        "        if len(set(res.db_ids)) != res.top_k or len(res.db_ids) != res.top_k:\n",
        "            scores.append( -1 * len(res.actual_ids) * res.top_k)\n",
        "            continue\n",
        "        score = 0\n",
        "        for id in res.db_ids:\n",
        "            try:\n",
        "                ind = res.actual_ids.index(id)\n",
        "                if ind > res.top_k * 3:\n",
        "                    score -= ind\n",
        "            except:\n",
        "                score -= len(res.actual_ids)\n",
        "        scores.append(score)\n",
        "\n",
        "    return sum(scores) / len(scores), sum(run_time) / len(run_time)\n",
        "\n",
        "def get_actual_ids_first_k(actual_sorted_ids, k):\n",
        "    \"\"\"\n",
        "    Retrieve the IDs from the sorted list of actual IDs.\n",
        "    actual IDs has the top_k for the 20 M database but for other databases we have to remove the numbers higher than the max size of the DB.\n",
        "\n",
        "    Parameters:\n",
        "    - actual_sorted_ids: A list of lists containing the sorted actual IDs for each query.\n",
        "    - k: The DB size.\n",
        "\n",
        "    Returns:\n",
        "    - List of lists containing the actual IDs for each query for this DB.\n",
        "    \"\"\"\n",
        "    return [[id for id in actual_sorted_ids_one_q if id < k] for actual_sorted_ids_one_q in actual_sorted_ids]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3bQQzzWlce4"
      },
      "source": [
        "This code to generate all the files for databases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "zZPsvyMqX17g"
      },
      "outputs": [],
      "source": [
        "def _write_vectors_to_file(vectors: np.ndarray, db_path) -> None:\n",
        "    mmap_vectors = np.memmap(db_path, dtype=np.float32, mode='w+', shape=vectors.shape)\n",
        "    mmap_vectors[:] = vectors[:]\n",
        "    mmap_vectors.flush()\n",
        "\n",
        "# def generate_database(size: int) -> None:\n",
        "#     rng = np.random.default_rng(DB_SEED_NUMBER)\n",
        "#     vectors = rng.random((size, DIMENSION), dtype=np.float32)\n",
        "#     return vectors\n",
        "def generated_database():\n",
        "    # Load memmap\n",
        "    mmap_vectors = np.memmap(\"new_embeddings.dat\", dtype=np.float32, mode='r', shape=(20_000_000, 64))\n",
        "    return mmap_vectors\n",
        "\n",
        "\n",
        "vectors = generated_database()\n",
        "\n",
        "db_filename_size_20M = 'saved_db_20M.dat'\n",
        "if not os.path.exists(db_filename_size_20M): _write_vectors_to_file(vectors, db_filename_size_20M)\n",
        "db_filename_size_15M = 'saved_db_15M.dat'\n",
        "if not os.path.exists(db_filename_size_15M): _write_vectors_to_file(vectors[:15*10**6], db_filename_size_15M)\n",
        "db_filename_size_10M = 'saved_db_10M.dat'\n",
        "if not os.path.exists(db_filename_size_10M): _write_vectors_to_file(vectors[:10*10**6], db_filename_size_10M)\n",
        "db_filename_size_1M = 'saved_db_1M.dat'\n",
        "if not os.path.exists(db_filename_size_1M): _write_vectors_to_file(vectors[:1*10**6], db_filename_size_1M)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c83ybYSKK85G",
        "outputId": "91dd0f3a-be2c-4dac-a42f-0a1aabe492f4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Karim Mahmoud\\AppData\\Local\\Temp\\ipykernel_3528\\1873110300.py:8: RuntimeWarning: invalid value encountered in divide\n",
            "  actual_sorted_ids_20m_q1 = np.argsort(vectors.dot(query1.T).T / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(query1)), axis= 1).squeeze().tolist()[::-1][:needed_top_k]\n",
            "C:\\Users\\Karim Mahmoud\\AppData\\Local\\Temp\\ipykernel_3528\\1873110300.py:10: RuntimeWarning: invalid value encountered in divide\n",
            "  actual_sorted_ids_20m_q2 = np.argsort(vectors.dot(query2.T).T / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(query2)), axis= 1).squeeze().tolist()[::-1][:needed_top_k]\n",
            "C:\\Users\\Karim Mahmoud\\AppData\\Local\\Temp\\ipykernel_3528\\1873110300.py:12: RuntimeWarning: invalid value encountered in divide\n",
            "  actual_sorted_ids_20m_q3 = np.argsort(vectors.dot(query3.T).T / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(query3)), axis= 1).squeeze().tolist()[::-1][:needed_top_k]\n"
          ]
        }
      ],
      "source": [
        "needed_top_k = 10000\n",
        "rng = np.random.default_rng(QUERY_SEED_NUMBER)\n",
        "query1 = rng.random((1, 64), dtype=np.float32)\n",
        "query2 = rng.random((1, 64), dtype=np.float32)\n",
        "query3 = rng.random((1, 64), dtype=np.float32)\n",
        "query_dummy = rng.random((1, 64), dtype=np.float32)\n",
        "\n",
        "actual_sorted_ids_20m_q1 = np.argsort(vectors.dot(query1.T).T / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(query1)), axis= 1).squeeze().tolist()[::-1][:needed_top_k]\n",
        "gc.collect()\n",
        "actual_sorted_ids_20m_q2 = np.argsort(vectors.dot(query2.T).T / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(query2)), axis= 1).squeeze().tolist()[::-1][:needed_top_k]\n",
        "gc.collect()\n",
        "actual_sorted_ids_20m_q3 = np.argsort(vectors.dot(query3.T).T / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(query3)), axis= 1).squeeze().tolist()[::-1][:needed_top_k]\n",
        "gc.collect()\n",
        "\n",
        "queries = [query1, query2, query3]\n",
        "actual_sorted_ids_20m = [actual_sorted_ids_20m_q1, actual_sorted_ids_20m_q2, actual_sorted_ids_20m_q3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bW7eI-hIvIfb",
        "outputId": "6dc4b563-e084-4bcf-f6a4-d10148efc0fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# No more need to the actual vectors so delete it\n",
        "del vectors\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "g-hCaQNqlBP0"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "to_print_arr = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s-envHmfeBr"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qackwdJN8Tor",
        "outputId": "1a0ae8eb-dccf-431c-a3a3-b5be6d8a32f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Team Number 3\n"
          ]
        }
      ],
      "source": [
        "print(\"Team Number\", 3)\n",
        "database_info = {\n",
        "    # \"1M\": {\n",
        "    #     \"database_file_path\": db_filename_size_1M,\n",
        "    #     \"index_file_path\": \"index.dat\",\n",
        "    #     \"size\": 10**6\n",
        "    # },\n",
        "    \"10M\": {\n",
        "        \"database_file_path\": db_filename_size_10M,\n",
        "        \"index_file_path\": \"index_10M_80_7000_centroids\",\n",
        "        \"size\": 10 * 10**6\n",
        "    },\n",
        "    # \"15M\": {\n",
        "    #     \"database_file_path\": db_filename_size_15M,\n",
        "    #     \"index_file_path\": PATH_DB_15M,\n",
        "    #     \"size\": 15 * 10**6\n",
        "    # },\n",
        "    # \"20M\": {\n",
        "    #     \"database_file_path\": db_filename_size_20M,\n",
        "    #     \"index_file_path\": \"index.dat\",\n",
        "    #     \"size\": 20 * 10**6\n",
        "    # }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "EXM0xTij8IhU"
      },
      "outputs": [],
      "source": [
        "\n",
        "# for db_name, info in database_info.items():\n",
        "#     db = VecDB(database_file_path = info[\"database_file_path\"], index_file_path = info[\"index_file_path\"], new_db = False)\n",
        "#     db._build_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "h-UFbhBPlQtz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10M\tscore\t0.0\ttime\t0.28\tRAM\t0.31 MB\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for db_name, info in database_info.items():\n",
        "    db = VecDB(database_file_path = info[\"database_file_path\"], index_file_path = info[\"index_file_path\"], new_db = False)\n",
        "    actual_ids = get_actual_ids_first_k(actual_sorted_ids_20m, info[\"size\"])\n",
        "    # Make a dummy run query to make everything fresh and loaded (wrap up)\n",
        "    res = run_queries(db, query_dummy, 5, actual_ids, 1)\n",
        "    # actual runs to evaluate\n",
        "    res, mem = memory_usage_run_queries((db, queries, 5, actual_ids, 3))\n",
        "    eval = evaluate_result(res)\n",
        "    to_print = f\"{db_name}\\tscore\\t{eval[0]}\\ttime\\t{eval[1]:.2f}\\tRAM\\t{mem:.2f} MB\"\n",
        "    print(to_print)\n",
        "    to_print_arr.append(to_print)\n",
        "    del db\n",
        "    del actual_ids\n",
        "    del res\n",
        "    del mem\n",
        "    del eval\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def get_one_row(row_num: int) -> np.ndarray:\n",
        "    # This function is only load one row in memory\n",
        "    try:\n",
        "        offset = row_num * 64 * np.dtype(np.float32).itemsize\n",
        "        mmap_vector = np.memmap(\"saved_db_1M.dat\", dtype=np.float32, mode='r', shape=(1, 64), offset=offset)\n",
        "        return np.array(mmap_vector[0])\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {e}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_all_ids_rows( ids) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Load only the requested rows from the memmap, without loading all data in RAM.\n",
        "    Updated: Instead of loading all vectors into memory, we load only the requested batch.\n",
        "    \"\"\"\n",
        "    num_records = 1_000_000\n",
        "    vectors = np.memmap(\"saved_db_1M.dat\", dtype=np.float32, mode='r', shape=(num_records, 64))\n",
        "    \n",
        "    # Sort IDs to access memmap sequentially (faster disk read)\n",
        "    sorted_idx = np.argsort(ids)\n",
        "    sorted_ids = np.array(ids)[sorted_idx]\n",
        "    \n",
        "    # Load only selected rows\n",
        "    result = np.empty((len(ids), 64), dtype=np.float32)\n",
        "    result[sorted_idx] = vectors[sorted_ids]\n",
        "    \n",
        "    del vectors\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_all_ids_rows_optimized(ids):\n",
        "    ids = np.array(ids)\n",
        "    num_records = 1_000_000\n",
        "\n",
        "    sorted_idx = np.argsort(ids)\n",
        "    sorted_ids = ids[sorted_idx]\n",
        "\n",
        "    base = sorted_ids[0]\n",
        "    row_size_bytes = 64 * np.dtype(np.float32).itemsize\n",
        "    offset = base * row_size_bytes\n",
        "\n",
        "    # memmap starting from the base\n",
        "    vectors = np.memmap(\n",
        "        \"saved_db_1M.dat\", dtype=np.float32, mode='r',\n",
        "        offset=offset,\n",
        "        shape=(num_records - base, 64)\n",
        "    )\n",
        "\n",
        "    local_ids = sorted_ids - base\n",
        "    \n",
        "    result = np.empty((len(ids), 64), dtype=np.float32)\n",
        "    result[sorted_idx] = vectors[local_ids]\n",
        "\n",
        "    del vectors\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([633553, 609427, 600199, 612447, 639489, 642724, 610822, 649498,\n",
              "       604144, 636958, 643106, 638695, 606188, 601414, 618471, 629282,\n",
              "       615177, 634304, 612609, 612144, 606113, 615908, 600821, 615118,\n",
              "       613466, 626497, 642111, 630188, 637237, 633109, 636480, 624148,\n",
              "       605503, 603918, 638478, 621123, 649717, 642294, 647609, 611076,\n",
              "       641514, 622062, 609413, 638340, 630263, 641252, 614644, 626335,\n",
              "       628102, 617523], dtype=int32)"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "high_base = 600_000\n",
        "spread = 50000  # random range width\n",
        "ids = high_base + np.random.choice(spread, size=50, replace=False)\n",
        "ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time taken: 0.01510763168334961\n"
          ]
        }
      ],
      "source": [
        "time1 = time.time()\n",
        "vectors1 = get_all_ids_rows(ids)\n",
        "time2 = time.time()\n",
        "# print(vectors)\n",
        "print(\"Time taken:\", time2 - time1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time taken: 0.0012903213500976562\n"
          ]
        }
      ],
      "source": [
        "time1 = time.time()\n",
        "vectors2 = get_all_ids_rows_optimized(ids)\n",
        "time2 = time.time()\n",
        "# print(vectors)\n",
        "print(\"Time taken:\", time2 - time1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nice\n"
          ]
        }
      ],
      "source": [
        "if np.array_equal(np.array(vectors1[1]), np.array(vectors2[1])):\n",
        "    print(\"nice\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-6.28111288e-02 -1.47320539e-01  1.90533586e-02  4.43077274e-02\n",
            "  1.05074465e-01  2.83172131e-01 -1.63409859e-01  1.95712700e-01\n",
            "  1.07049912e-01 -1.23330027e-01 -3.00835390e-02  2.45135650e-01\n",
            " -5.26915416e-02 -5.52256368e-02  9.29732770e-02 -6.97657317e-02\n",
            " -6.27087284e-05 -1.32255822e-01 -2.69026589e-02  7.89163932e-02\n",
            " -1.89308122e-01  6.02391511e-02  3.74443419e-02 -1.43766150e-01\n",
            " -2.76008368e-01 -6.13625012e-02  5.22740744e-02  1.01051793e-01\n",
            " -5.17447963e-02  3.67304794e-02 -2.33252682e-02  1.31471306e-01\n",
            " -1.30366221e-01  6.15089536e-02 -1.63679924e-02 -1.46132097e-01\n",
            " -8.24083313e-02  1.16798908e-01 -1.00988574e-01  1.16019160e-01\n",
            " -6.51810691e-02  4.59860981e-04  1.06093474e-01  1.89122364e-01\n",
            " -2.01225087e-01  8.32308456e-02 -4.10196595e-02 -1.48688197e-01\n",
            "  2.44087949e-02 -2.67753322e-02  9.42895487e-02 -1.49306238e-01\n",
            " -1.71684042e-01 -3.24866235e-01  3.02920695e-02 -1.64438151e-02\n",
            " -1.86713353e-01 -1.15885682e-01  1.82194505e-02  9.29413810e-02\n",
            "  9.96821746e-02  1.78462937e-01 -1.56787243e-02 -1.76343977e-01]\n"
          ]
        }
      ],
      "source": [
        "print(vectors[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-6.28111288e-02 -1.47320539e-01  1.90533586e-02  4.43077274e-02\n",
            "  1.05074465e-01  2.83172131e-01 -1.63409859e-01  1.95712700e-01\n",
            "  1.07049912e-01 -1.23330027e-01 -3.00835390e-02  2.45135650e-01\n",
            " -5.26915416e-02 -5.52256368e-02  9.29732770e-02 -6.97657317e-02\n",
            " -6.27087284e-05 -1.32255822e-01 -2.69026589e-02  7.89163932e-02\n",
            " -1.89308122e-01  6.02391511e-02  3.74443419e-02 -1.43766150e-01\n",
            " -2.76008368e-01 -6.13625012e-02  5.22740744e-02  1.01051793e-01\n",
            " -5.17447963e-02  3.67304794e-02 -2.33252682e-02  1.31471306e-01\n",
            " -1.30366221e-01  6.15089536e-02 -1.63679924e-02 -1.46132097e-01\n",
            " -8.24083313e-02  1.16798908e-01 -1.00988574e-01  1.16019160e-01\n",
            " -6.51810691e-02  4.59860981e-04  1.06093474e-01  1.89122364e-01\n",
            " -2.01225087e-01  8.32308456e-02 -4.10196595e-02 -1.48688197e-01\n",
            "  2.44087949e-02 -2.67753322e-02  9.42895487e-02 -1.49306238e-01\n",
            " -1.71684042e-01 -3.24866235e-01  3.02920695e-02 -1.64438151e-02\n",
            " -1.86713353e-01 -1.15885682e-01  1.82194505e-02  9.29413810e-02\n",
            "  9.96821746e-02  1.78462937e-01 -1.56787243e-02 -1.76343977e-01]\n"
          ]
        }
      ],
      "source": [
        "vector_tmp=get_one_row(ids[0])\n",
        "print(vector_tmp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "header_arr = np.fromfile(\"./index_10M_80_7000_centroids/level2_header.bin\", dtype=np.uint32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "160"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(header_arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "[WinError 8] Not enough memory resources are available to process this command",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m ids_mm = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmemmap\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./index_10M_80_7000_centroids/all_indices.bin\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43muint32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m                \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m100_000_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda\\envs\\genai-env\\Lib\\site-packages\\numpy\\_core\\memmap.py:279\u001b[39m, in \u001b[36mmemmap.__new__\u001b[39m\u001b[34m(subtype, filename, dtype, mode, offset, shape, order)\u001b[39m\n\u001b[32m    277\u001b[39m \u001b[38;5;28mbytes\u001b[39m -= start\n\u001b[32m    278\u001b[39m array_offset = offset - start\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m mm = \u001b[43mmmap\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfileno\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccess\u001b[49m\u001b[43m=\u001b[49m\u001b[43macc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[38;5;28mself\u001b[39m = ndarray.\u001b[34m__new__\u001b[39m(subtype, shape, dtype=descr, buffer=mm,\n\u001b[32m    282\u001b[39m                        offset=array_offset, order=order)\n\u001b[32m    283\u001b[39m \u001b[38;5;28mself\u001b[39m._mmap = mm\n",
            "\u001b[31mOSError\u001b[39m: [WinError 8] Not enough memory resources are available to process this command"
          ]
        }
      ],
      "source": [
        "ids_mm = np.memmap(\"./index_10M_80_7000_centroids/all_indices.bin\", dtype=np.uint32, mode=\"r\",\n",
        "                offset=0,\n",
        "                shape=(10_000_000,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10000000"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(ids_mm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "genai-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
