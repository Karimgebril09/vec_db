{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "G44iH6jnObEj"
      },
      "outputs": [],
      "source": [
        "QUERY_SEED_NUMBER = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "miaF8OeibbIL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "DB_SEED_NUMBER = 42\n",
        "ELEMENT_SIZE = np.dtype(np.float32).itemsize\n",
        "DIMENSION = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import tqdm\n",
        "import heapq\n",
        "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
        "from typing import Annotated\n",
        "import time\n",
        "\n",
        "DB_SEED_NUMBER = 42\n",
        "ELEMENT_SIZE = np.dtype(np.float32).itemsize\n",
        "DIMENSION = 64\n",
        "\n",
        "class VecDB:\n",
        "    def __init__(self, database_file_path = \"saved_db.dat\", index_file_path = \"index.dat\", new_db = True, db_size = None) -> None:\n",
        "        self.db_path = database_file_path\n",
        "        self.index_path = index_file_path\n",
        "        if new_db:\n",
        "            if db_size is None:\n",
        "                raise ValueError(\"You need to provide the size of the database\")\n",
        "            # delete the old DB file if exists\n",
        "            if os.path.exists(self.db_path):\n",
        "                os.remove(self.db_path)\n",
        "            self.generate_database(db_size)\n",
        "    \n",
        "    def generate_database(self, size: int) -> None:\n",
        "        # rng = np.random.default_rng(DB_SEED_NUMBER)\n",
        "        vectors = np.memmap(\"new_embeddings.dat\", dtype=np.float32, mode='r', shape=(size, DIMENSION))\n",
        "        self._write_vectors_to_file(vectors)\n",
        "        self._build_index()\n",
        "\n",
        "    def _write_vectors_to_file(self, vectors: np.ndarray) -> None:\n",
        "        mmap_vectors = np.memmap(self.db_path, dtype=np.float32, mode='w+', shape=vectors.shape)\n",
        "        mmap_vectors[:] = vectors[:]\n",
        "        mmap_vectors.flush()\n",
        "\n",
        "    def _get_num_records(self) -> int:\n",
        "        return os.path.getsize(self.db_path) // (DIMENSION * ELEMENT_SIZE)\n",
        "\n",
        "    def insert_records(self, rows: Annotated[np.ndarray, (int, 64)]):\n",
        "        num_old_records = self._get_num_records()\n",
        "        num_new_records = len(rows)\n",
        "        full_shape = (num_old_records + num_new_records, DIMENSION)\n",
        "        mmap_vectors = np.memmap(self.db_path, dtype=np.float32, mode='r+', shape=full_shape)\n",
        "        mmap_vectors[num_old_records:] = rows\n",
        "        mmap_vectors.flush()\n",
        "        #TODO: might change to call insert in the index, if you need\n",
        "        self._build_index()\n",
        "\n",
        "    def get_one_row(self, row_num: int) -> np.ndarray:\n",
        "        # This function is only load one row in memory\n",
        "        try:\n",
        "            offset = row_num * DIMENSION * ELEMENT_SIZE\n",
        "            mmap_vector = np.memmap(self.db_path, dtype=np.float32, mode='r', shape=(1, DIMENSION), offset=offset)\n",
        "            return np.array(mmap_vector[0])\n",
        "        except Exception as e:\n",
        "            return f\"An error occurred: {e}\"\n",
        "\n",
        "    def get_all_rows(self) -> np.ndarray:\n",
        "        # Take care this load all the data in memory\n",
        "        num_records = self._get_num_records()\n",
        "        vectors = np.memmap(self.db_path, dtype=np.float32, mode='r', shape=(num_records, DIMENSION))\n",
        "        return np.array(vectors)\n",
        "    \n",
        "    def get_all_ids_rows_optimized(self, ids):\n",
        "        ids = np.array(ids)\n",
        "        num_records = self._get_num_records()\n",
        "\n",
        "        sorted_idx = np.argsort(ids)\n",
        "        sorted_ids = ids[sorted_idx]\n",
        "\n",
        "        base = sorted_ids[0]\n",
        "        row_size_bytes = DIMENSION * np.dtype(np.float32).itemsize\n",
        "        offset = base * row_size_bytes\n",
        "\n",
        "        # memmap starting from the base\n",
        "        vectors = np.memmap(\n",
        "            self.db_path, dtype=np.float32, mode='r',\n",
        "            offset=offset,\n",
        "            shape=(num_records - base, DIMENSION)\n",
        "        )\n",
        "\n",
        "        local_ids = sorted_ids - base\n",
        "        \n",
        "        result = np.empty((len(ids), DIMENSION), dtype=np.float32)\n",
        "        result[sorted_idx] = vectors[local_ids]\n",
        "\n",
        "        del vectors\n",
        "        return result\n",
        "    \n",
        "    \n",
        "    \n",
        "    def _cal_score(self, vec1, vec2):\n",
        "        dot_product = np.dot(vec1, vec2)\n",
        "        norm_vec1 = np.linalg.norm(vec1)\n",
        "        norm_vec2 = np.linalg.norm(vec2)\n",
        "        cosine_similarity = dot_product / (norm_vec1 * norm_vec2)\n",
        "        return cosine_similarity\n",
        "\n",
        "####################################################################################\n",
        "####################################################################################\n",
        "####################################################################################\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    def retrieve(self, query, top_k=5, n_probe=None, chunk_size=50):\n",
        "        self.no_centroids = 6000\n",
        "        self.index_path = f\"index_10M_{self.no_centroids}_centroids\"\n",
        "        query = np.asarray(query, dtype=np.float32).squeeze()\n",
        "\n",
        "        query_norm = np.linalg.norm(query)\n",
        "        if query_norm == 0:\n",
        "            query_norm = 1.0\n",
        "        normalized_query = query / query_norm\n",
        "\n",
        "        centers_path = os.path.join(self.index_path, \"centroids.npy\")\n",
        "        if not os.path.exists(centers_path):\n",
        "            del query, normalized_query\n",
        "            return []\n",
        "\n",
        "        # Load centroids with memory-mapping to save RAM\n",
        "        centroids = np.load(centers_path, mmap_mode=\"r\")\n",
        "\n",
        "        if n_probe is None:\n",
        "            num_records = self._get_num_records()\n",
        "            n_probe = 10 if num_records <= 15_000_000 else 8\n",
        "\n",
        "        # batch_size = 50\n",
        "        # min_heap = []\n",
        "\n",
        "        # for start in range(0, n_centroids, batch_size):\n",
        "        #     end = min(start + batch_size, n_centroids)\n",
        "        #     batch = centroids[start:end]  # only this batch is loaded in RAM\n",
        "        #     sims = batch.dot(normalized_query)\n",
        "\n",
        "        #     for i, score in enumerate(sims):\n",
        "        #         centroid_index = start + i\n",
        "        #         if len(min_heap) < n_probe:\n",
        "        #             heapq.heappush(min_heap, (score, centroid_index))\n",
        "        #         elif score > min_heap[0][0]:\n",
        "        #             heapq.heappushpop(min_heap, (score, centroid_index))\n",
        "            \n",
        "        #     del batch, sims\n",
        "\n",
        "\n",
        "        # nearest_centroids = [centroid_index for score, centroid_index in heapq.nlargest(n_probe, min_heap)]\n",
        "        # del centroids\n",
        "        sims = centroids.dot(normalized_query)\n",
        "        nearest_centroids = np.argpartition(sims, -n_probe)[-n_probe:]\n",
        "        del sims\n",
        "        del centroids\n",
        "\n",
        "\n",
        "        header_arr = np.fromfile(os.path.join(self.index_path, \"index_header.bin\"), dtype=np.uint32)\n",
        "        header_arr = header_arr.reshape(-1, 2)   # shape: (num_centroids, 2)\n",
        "\n",
        "\n",
        "        # Prepare top-k heap\n",
        "        top_heap = []\n",
        "\n",
        "        flat_index_path = os.path.join(self.index_path, \"all_indices.bin\")\n",
        "        # For each centroid to probe:\n",
        "        for c in nearest_centroids:\n",
        "            offset  = header_arr[c, 0]   # first column\n",
        "            length  = header_arr[c, 1]   # second column\n",
        "            if length == 0:\n",
        "                continue\n",
        "\n",
        "            # Process each chunk by remapping the memmap for that chunk\n",
        "            for start in range(0, length, chunk_size):\n",
        "                cur_len = min(chunk_size, length - start)\n",
        "                # Remap only this chunk\n",
        "                ids_mm = np.memmap(flat_index_path, dtype=np.uint32, mode=\"r\",\n",
        "                                offset=offset + start * np.dtype(np.uint32).itemsize,\n",
        "                                shape=(cur_len,))\n",
        "\n",
        "                chunk_ids = ids_mm[:]  \n",
        "                del ids_mm  # free memmap\n",
        "\n",
        "                \n",
        "                vecs = self.get_all_ids_rows_optimized(chunk_ids)\n",
        "                scores = vecs.dot(normalized_query)\n",
        "                for score, id in zip(scores, chunk_ids):\n",
        "                    if len(top_heap) < top_k:\n",
        "                        heapq.heappush(top_heap, (score, id))\n",
        "                    else:\n",
        "                        heapq.heappushpop(top_heap, (score, id))\n",
        "\n",
        "                del scores, chunk_ids\n",
        "\n",
        "        # Extract and return top-k IDs sorted by score\n",
        "        results = [idx for score, idx in heapq.nlargest(top_k, top_heap)]\n",
        "        del top_heap\n",
        "        # print(f\"Time for centroid similarity calculation: {end_time - start_time} seconds\")\n",
        "    \n",
        "        # print(f\"Time for memory cleanup: {end2 - start2} seconds\")\n",
        "        # print(f\"Time for processing chunks: {end3 - start3} seconds\")\n",
        "        return results\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    def _build_index(self):\n",
        "       \n",
        "        # sqrt(N) rule\n",
        "        self.no_centroids = 7000\n",
        "        self.index_path = f\"index_10M_{self.no_centroids}_centroids\"\n",
        "        # data is a reference to the memmap object, not the data in RAM\n",
        "        data = self.get_all_rows()\n",
        "\n",
        "        kmeans = MiniBatchKMeans(\n",
        "            n_clusters=self.no_centroids,\n",
        "            init=\"k-means++\",   # supported and default\n",
        "            batch_size=10_000,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        kmeans.fit(data)\n",
        "\n",
        "        # labels and centers are new arrays created in RAM\n",
        "        labels = kmeans.labels_\n",
        "        centers = kmeans.cluster_centers_.astype(np.float32)\n",
        "\n",
        "        # Added deletion of the memmap reference and the kmeans object\n",
        "        del data\n",
        "        del kmeans\n",
        "       \n",
        "         \n",
        "\n",
        "        if os.path.exists(self.index_path):\n",
        "            shutil.rmtree(self.index_path)  # remove old index if any\n",
        "        os.makedirs(self.index_path, exist_ok=True)\n",
        "               \n",
        "        if not os.path.isdir(self.index_path):\n",
        "            os.makedirs(self.index_path, exist_ok=True)\n",
        "        # 1. Build up a list of (cluster_id, indices_array)\n",
        "        cluster_infos = []\n",
        "        for cid in range(self.no_centroids):\n",
        "            indices = np.where(labels == cid)[0].astype(np.uint32)\n",
        "            cluster_infos.append((cid, indices))\n",
        "\n",
        "      \n",
        "        header = []\n",
        "\n",
        "        flat_path = os.path.join(self.index_path, \"all_indices.bin\")\n",
        "        with open(flat_path, \"wb\") as f:\n",
        "            offset = 0\n",
        "            for cid, inds in cluster_infos:\n",
        "                length = inds.size\n",
        "                f.write(inds.tobytes())\n",
        "                header.append([offset, length])\n",
        "                offset += length * inds.dtype.itemsize\n",
        "\n",
        "        # Convert to a matrix (2 columns: offset, length)\n",
        "        header_matrix = np.array(header, dtype=np.uint32)\n",
        "\n",
        "        # Save matrix as binary file\n",
        "        header_bin_path = os.path.join(self.index_path, \"index_header.bin\")\n",
        "        header_matrix.tofile(header_bin_path)\n",
        "\n",
        "        norms = np.linalg.norm(centers, axis=1, keepdims=True)\n",
        "        centers = centers / (norms + 1e-12)\n",
        "        # 4. Save centers as before\n",
        "        np.save(os.path.join(self.index_path, \"centroids.npy\"), centers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "Sg2vfYgeyavn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "# from vec_db import VecDB\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "from memory_profiler import memory_usage\n",
        "import gc\n",
        "\n",
        "@dataclass\n",
        "class Result:\n",
        "    run_time: float\n",
        "    top_k: int\n",
        "    db_ids: List[int]\n",
        "    actual_ids: List[int]\n",
        "\n",
        "def run_queries(db, queries, top_k, actual_ids, num_runs):\n",
        "    \"\"\"\n",
        "    Run queries on the database and record results for each query.\n",
        "\n",
        "    Parameters:\n",
        "    - db: Database instance to run queries on.\n",
        "    - queries: List of query vectors.\n",
        "    - top_k: Number of top results to retrieve.\n",
        "    - actual_ids: List of actual results to evaluate accuracy.\n",
        "    - num_runs: Number of query executions to perform for testing.\n",
        "\n",
        "    Returns:\n",
        "    - List of Result\n",
        "    \"\"\"\n",
        "    global results\n",
        "    results = []\n",
        "    for i in range(num_runs):\n",
        "        tic = time.time()\n",
        "        db_ids = db.retrieve(queries[i], top_k)\n",
        "        toc = time.time()\n",
        "        run_time = toc - tic\n",
        "        results.append(Result(run_time, top_k, db_ids, actual_ids[i]))\n",
        "    return results\n",
        "\n",
        "def memory_usage_run_queries(args):\n",
        "    \"\"\"\n",
        "    Run queries and measure memory usage during the execution.\n",
        "\n",
        "    Parameters:\n",
        "    - args: Arguments to be passed to the run_queries function.\n",
        "\n",
        "    Returns:\n",
        "    - results: The results of the run_queries.\n",
        "    - memory_diff: The difference in memory usage before and after running the queries.\n",
        "    \"\"\"\n",
        "    global results\n",
        "    mem_before = max(memory_usage())\n",
        "    mem = memory_usage(proc=(run_queries, args, {}), interval = 1e-3)\n",
        "    return results, max(mem) - mem_before\n",
        "\n",
        "def evaluate_result(results: List[Result]):\n",
        "    \"\"\"\n",
        "    Evaluate the results based on accuracy and runtime.\n",
        "    Scores are negative. So getting 0 is the best score.\n",
        "\n",
        "    Parameters:\n",
        "    - results: A list of Result objects\n",
        "\n",
        "    Returns:\n",
        "    - avg_score: The average score across all queries.\n",
        "    - avg_runtime: The average runtime for all queries.\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "    run_time = []\n",
        "    for res in results:\n",
        "        run_time.append(res.run_time)\n",
        "        # case for retireving number not equal to top_k, socre will be the lowest\n",
        "        if len(set(res.db_ids)) != res.top_k or len(res.db_ids) != res.top_k:\n",
        "            scores.append( -1 * len(res.actual_ids) * res.top_k)\n",
        "            continue\n",
        "        score = 0\n",
        "        for id in res.db_ids:\n",
        "            try:\n",
        "                ind = res.actual_ids.index(id)\n",
        "                if ind > res.top_k * 3:\n",
        "                    score -= ind\n",
        "            except:\n",
        "                score -= len(res.actual_ids)\n",
        "        scores.append(score)\n",
        "\n",
        "    return sum(scores) / len(scores), sum(run_time) / len(run_time)\n",
        "\n",
        "def get_actual_ids_first_k(actual_sorted_ids, k):\n",
        "    \"\"\"\n",
        "    Retrieve the IDs from the sorted list of actual IDs.\n",
        "    actual IDs has the top_k for the 20 M database but for other databases we have to remove the numbers higher than the max size of the DB.\n",
        "\n",
        "    Parameters:\n",
        "    - actual_sorted_ids: A list of lists containing the sorted actual IDs for each query.\n",
        "    - k: The DB size.\n",
        "\n",
        "    Returns:\n",
        "    - List of lists containing the actual IDs for each query for this DB.\n",
        "    \"\"\"\n",
        "    return [[id for id in actual_sorted_ids_one_q if id < k] for actual_sorted_ids_one_q in actual_sorted_ids]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3bQQzzWlce4"
      },
      "source": [
        "This code to generate all the files for databases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "zZPsvyMqX17g"
      },
      "outputs": [],
      "source": [
        "def _write_vectors_to_file(vectors: np.ndarray, db_path) -> None:\n",
        "    mmap_vectors = np.memmap(db_path, dtype=np.float32, mode='w+', shape=vectors.shape)\n",
        "    mmap_vectors[:] = vectors[:]\n",
        "    mmap_vectors.flush()\n",
        "\n",
        "# def generate_database(size: int) -> None:\n",
        "#     rng = np.random.default_rng(DB_SEED_NUMBER)\n",
        "#     vectors = rng.random((size, DIMENSION), dtype=np.float32)\n",
        "#     return vectors\n",
        "def generated_database():\n",
        "    # Load memmap\n",
        "    mmap_vectors = np.memmap(\"new_embeddings.dat\", dtype=np.float32, mode='r', shape=(20_000_000, 64))\n",
        "    return mmap_vectors\n",
        "\n",
        "\n",
        "vectors = generated_database()\n",
        "\n",
        "db_filename_size_20M = 'saved_db_20M.dat'\n",
        "if not os.path.exists(db_filename_size_20M): _write_vectors_to_file(vectors, db_filename_size_20M)\n",
        "db_filename_size_15M = 'saved_db_15M.dat'\n",
        "if not os.path.exists(db_filename_size_15M): _write_vectors_to_file(vectors[:15*10**6], db_filename_size_15M)\n",
        "db_filename_size_10M = 'saved_db_10M.dat'\n",
        "if not os.path.exists(db_filename_size_10M): _write_vectors_to_file(vectors[:10*10**6], db_filename_size_10M)\n",
        "db_filename_size_1M = 'saved_db_1M.dat'\n",
        "if not os.path.exists(db_filename_size_1M): _write_vectors_to_file(vectors[:1*10**6], db_filename_size_1M)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c83ybYSKK85G",
        "outputId": "91dd0f3a-be2c-4dac-a42f-0a1aabe492f4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Karim Mahmoud\\AppData\\Local\\Temp\\ipykernel_20184\\1873110300.py:8: RuntimeWarning: invalid value encountered in divide\n",
            "  actual_sorted_ids_20m_q1 = np.argsort(vectors.dot(query1.T).T / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(query1)), axis= 1).squeeze().tolist()[::-1][:needed_top_k]\n",
            "C:\\Users\\Karim Mahmoud\\AppData\\Local\\Temp\\ipykernel_20184\\1873110300.py:10: RuntimeWarning: invalid value encountered in divide\n",
            "  actual_sorted_ids_20m_q2 = np.argsort(vectors.dot(query2.T).T / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(query2)), axis= 1).squeeze().tolist()[::-1][:needed_top_k]\n",
            "C:\\Users\\Karim Mahmoud\\AppData\\Local\\Temp\\ipykernel_20184\\1873110300.py:12: RuntimeWarning: invalid value encountered in divide\n",
            "  actual_sorted_ids_20m_q3 = np.argsort(vectors.dot(query3.T).T / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(query3)), axis= 1).squeeze().tolist()[::-1][:needed_top_k]\n"
          ]
        }
      ],
      "source": [
        "needed_top_k = 10000\n",
        "rng = np.random.default_rng(QUERY_SEED_NUMBER)\n",
        "query1 = rng.random((1, 64), dtype=np.float32)\n",
        "query2 = rng.random((1, 64), dtype=np.float32)\n",
        "query3 = rng.random((1, 64), dtype=np.float32)\n",
        "query_dummy = rng.random((1, 64), dtype=np.float32)\n",
        "\n",
        "actual_sorted_ids_20m_q1 = np.argsort(vectors.dot(query1.T).T / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(query1)), axis= 1).squeeze().tolist()[::-1][:needed_top_k]\n",
        "gc.collect()\n",
        "actual_sorted_ids_20m_q2 = np.argsort(vectors.dot(query2.T).T / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(query2)), axis= 1).squeeze().tolist()[::-1][:needed_top_k]\n",
        "gc.collect()\n",
        "actual_sorted_ids_20m_q3 = np.argsort(vectors.dot(query3.T).T / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(query3)), axis= 1).squeeze().tolist()[::-1][:needed_top_k]\n",
        "gc.collect()\n",
        "\n",
        "queries = [query1, query2, query3]\n",
        "actual_sorted_ids_20m = [actual_sorted_ids_20m_q1, actual_sorted_ids_20m_q2, actual_sorted_ids_20m_q3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bW7eI-hIvIfb",
        "outputId": "6dc4b563-e084-4bcf-f6a4-d10148efc0fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# No more need to the actual vectors so delete it\n",
        "del vectors\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "g-hCaQNqlBP0"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "to_print_arr = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s-envHmfeBr"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qackwdJN8Tor",
        "outputId": "1a0ae8eb-dccf-431c-a3a3-b5be6d8a32f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Team Number 3\n"
          ]
        }
      ],
      "source": [
        "print(\"Team Number\", 3)\n",
        "database_info = {\n",
        "    # \"1M\": {\n",
        "    #     \"database_file_path\": db_filename_size_1M,\n",
        "    #     \"index_file_path\": \"index.dat\",\n",
        "    #     \"size\": 10**6\n",
        "    # },\n",
        "    \"10M\": {\n",
        "        \"database_file_path\": db_filename_size_10M,\n",
        "        \"index_file_path\": \"index_10M_6000_centroids\",\n",
        "        \"size\": 10 * 10**6\n",
        "    },\n",
        "    # \"15M\": {\n",
        "    #     \"database_file_path\": db_filename_size_15M,\n",
        "    #     \"index_file_path\": PATH_DB_15M,\n",
        "    #     \"size\": 15 * 10**6\n",
        "    # },\n",
        "    # \"20M\": {\n",
        "    #     \"database_file_path\": db_filename_size_20M,\n",
        "    #     \"index_file_path\": \"index.dat\",\n",
        "    #     \"size\": 20 * 10**6\n",
        "    # }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "EXM0xTij8IhU"
      },
      "outputs": [],
      "source": [
        "\n",
        "# for db_name, info in database_info.items():\n",
        "#     db = VecDB(database_file_path = info[\"database_file_path\"], index_file_path = info[\"index_file_path\"], new_db = False)\n",
        "#     db._build_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "h-UFbhBPlQtz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10M\tscore\t0.0\ttime\t0.53\tRAM\t0.17 MB\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for db_name, info in database_info.items():\n",
        "    db = VecDB(database_file_path = info[\"database_file_path\"], index_file_path = info[\"index_file_path\"], new_db = False)\n",
        "    actual_ids = get_actual_ids_first_k(actual_sorted_ids_20m, info[\"size\"])\n",
        "    # Make a dummy run query to make everything fresh and loaded (wrap up)\n",
        "    res = run_queries(db, query_dummy, 5, actual_ids, 1)\n",
        "    # actual runs to evaluate\n",
        "    res, mem = memory_usage_run_queries((db, queries, 5, actual_ids, 3))\n",
        "    eval = evaluate_result(res)\n",
        "    to_print = f\"{db_name}\\tscore\\t{eval[0]}\\ttime\\t{eval[1]:.2f}\\tRAM\\t{mem:.2f} MB\"\n",
        "    print(to_print)\n",
        "    to_print_arr.append(to_print)\n",
        "    del db\n",
        "    del actual_ids\n",
        "    del res\n",
        "    del mem\n",
        "    del eval\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def get_one_row(row_num: int) -> np.ndarray:\n",
        "    # This function is only load one row in memory\n",
        "    try:\n",
        "        offset = row_num * 64 * np.dtype(np.float32).itemsize\n",
        "        mmap_vector = np.memmap(\"saved_db_1M.dat\", dtype=np.float32, mode='r', shape=(1, 64), offset=offset)\n",
        "        return np.array(mmap_vector[0])\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {e}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_all_ids_rows( ids) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Load only the requested rows from the memmap, without loading all data in RAM.\n",
        "    Updated: Instead of loading all vectors into memory, we load only the requested batch.\n",
        "    \"\"\"\n",
        "    num_records = 1_000_000\n",
        "    vectors = np.memmap(\"saved_db_1M.dat\", dtype=np.float32, mode='r', shape=(num_records, 64))\n",
        "    \n",
        "    # Sort IDs to access memmap sequentially (faster disk read)\n",
        "    sorted_idx = np.argsort(ids)\n",
        "    sorted_ids = np.array(ids)[sorted_idx]\n",
        "    \n",
        "    # Load only selected rows\n",
        "    result = np.empty((len(ids), 64), dtype=np.float32)\n",
        "    result[sorted_idx] = vectors[sorted_ids]\n",
        "    \n",
        "    del vectors\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_all_ids_rows_optimized(ids):\n",
        "    ids = np.array(ids)\n",
        "    num_records = 1_000_000\n",
        "\n",
        "    sorted_idx = np.argsort(ids)\n",
        "    sorted_ids = ids[sorted_idx]\n",
        "\n",
        "    base = sorted_ids[0]\n",
        "    row_size_bytes = 64 * np.dtype(np.float32).itemsize\n",
        "    offset = base * row_size_bytes\n",
        "\n",
        "    # memmap starting from the base\n",
        "    vectors = np.memmap(\n",
        "        \"saved_db_1M.dat\", dtype=np.float32, mode='r',\n",
        "        offset=offset,\n",
        "        shape=(num_records - base, 64)\n",
        "    )\n",
        "\n",
        "    local_ids = sorted_ids - base\n",
        "    \n",
        "    result = np.empty((len(ids), 64), dtype=np.float32)\n",
        "    result[sorted_idx] = vectors[local_ids]\n",
        "\n",
        "    del vectors\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([633553, 609427, 600199, 612447, 639489, 642724, 610822, 649498,\n",
              "       604144, 636958, 643106, 638695, 606188, 601414, 618471, 629282,\n",
              "       615177, 634304, 612609, 612144, 606113, 615908, 600821, 615118,\n",
              "       613466, 626497, 642111, 630188, 637237, 633109, 636480, 624148,\n",
              "       605503, 603918, 638478, 621123, 649717, 642294, 647609, 611076,\n",
              "       641514, 622062, 609413, 638340, 630263, 641252, 614644, 626335,\n",
              "       628102, 617523], dtype=int32)"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "high_base = 600_000\n",
        "spread = 50000  # random range width\n",
        "ids = high_base + np.random.choice(spread, size=50, replace=False)\n",
        "ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time taken: 0.01510763168334961\n"
          ]
        }
      ],
      "source": [
        "time1 = time.time()\n",
        "vectors1 = get_all_ids_rows(ids)\n",
        "time2 = time.time()\n",
        "# print(vectors)\n",
        "print(\"Time taken:\", time2 - time1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time taken: 0.0012903213500976562\n"
          ]
        }
      ],
      "source": [
        "time1 = time.time()\n",
        "vectors2 = get_all_ids_rows_optimized(ids)\n",
        "time2 = time.time()\n",
        "# print(vectors)\n",
        "print(\"Time taken:\", time2 - time1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nice\n"
          ]
        }
      ],
      "source": [
        "if np.array_equal(np.array(vectors1[1]), np.array(vectors2[1])):\n",
        "    print(\"nice\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-6.28111288e-02 -1.47320539e-01  1.90533586e-02  4.43077274e-02\n",
            "  1.05074465e-01  2.83172131e-01 -1.63409859e-01  1.95712700e-01\n",
            "  1.07049912e-01 -1.23330027e-01 -3.00835390e-02  2.45135650e-01\n",
            " -5.26915416e-02 -5.52256368e-02  9.29732770e-02 -6.97657317e-02\n",
            " -6.27087284e-05 -1.32255822e-01 -2.69026589e-02  7.89163932e-02\n",
            " -1.89308122e-01  6.02391511e-02  3.74443419e-02 -1.43766150e-01\n",
            " -2.76008368e-01 -6.13625012e-02  5.22740744e-02  1.01051793e-01\n",
            " -5.17447963e-02  3.67304794e-02 -2.33252682e-02  1.31471306e-01\n",
            " -1.30366221e-01  6.15089536e-02 -1.63679924e-02 -1.46132097e-01\n",
            " -8.24083313e-02  1.16798908e-01 -1.00988574e-01  1.16019160e-01\n",
            " -6.51810691e-02  4.59860981e-04  1.06093474e-01  1.89122364e-01\n",
            " -2.01225087e-01  8.32308456e-02 -4.10196595e-02 -1.48688197e-01\n",
            "  2.44087949e-02 -2.67753322e-02  9.42895487e-02 -1.49306238e-01\n",
            " -1.71684042e-01 -3.24866235e-01  3.02920695e-02 -1.64438151e-02\n",
            " -1.86713353e-01 -1.15885682e-01  1.82194505e-02  9.29413810e-02\n",
            "  9.96821746e-02  1.78462937e-01 -1.56787243e-02 -1.76343977e-01]\n"
          ]
        }
      ],
      "source": [
        "print(vectors[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-6.28111288e-02 -1.47320539e-01  1.90533586e-02  4.43077274e-02\n",
            "  1.05074465e-01  2.83172131e-01 -1.63409859e-01  1.95712700e-01\n",
            "  1.07049912e-01 -1.23330027e-01 -3.00835390e-02  2.45135650e-01\n",
            " -5.26915416e-02 -5.52256368e-02  9.29732770e-02 -6.97657317e-02\n",
            " -6.27087284e-05 -1.32255822e-01 -2.69026589e-02  7.89163932e-02\n",
            " -1.89308122e-01  6.02391511e-02  3.74443419e-02 -1.43766150e-01\n",
            " -2.76008368e-01 -6.13625012e-02  5.22740744e-02  1.01051793e-01\n",
            " -5.17447963e-02  3.67304794e-02 -2.33252682e-02  1.31471306e-01\n",
            " -1.30366221e-01  6.15089536e-02 -1.63679924e-02 -1.46132097e-01\n",
            " -8.24083313e-02  1.16798908e-01 -1.00988574e-01  1.16019160e-01\n",
            " -6.51810691e-02  4.59860981e-04  1.06093474e-01  1.89122364e-01\n",
            " -2.01225087e-01  8.32308456e-02 -4.10196595e-02 -1.48688197e-01\n",
            "  2.44087949e-02 -2.67753322e-02  9.42895487e-02 -1.49306238e-01\n",
            " -1.71684042e-01 -3.24866235e-01  3.02920695e-02 -1.64438151e-02\n",
            " -1.86713353e-01 -1.15885682e-01  1.82194505e-02  9.29413810e-02\n",
            "  9.96821746e-02  1.78462937e-01 -1.56787243e-02 -1.76343977e-01]\n"
          ]
        }
      ],
      "source": [
        "vector_tmp=get_one_row(ids[0])\n",
        "print(vector_tmp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "genai-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
