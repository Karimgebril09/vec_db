{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "G44iH6jnObEj"
      },
      "outputs": [],
      "source": [
        "QUERY_SEED_NUMBER = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "miaF8OeibbIL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "DB_SEED_NUMBER = 42\n",
        "ELEMENT_SIZE = np.dtype(np.float32).itemsize\n",
        "DIMENSION = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import tqdm\n",
        "import heapq\n",
        "from sklearn.cluster import KMeans\n",
        "from typing import Annotated\n",
        "\n",
        "\n",
        "DB_SEED_NUMBER = 42\n",
        "ELEMENT_SIZE = np.dtype(np.float32).itemsize\n",
        "DIMENSION = 64\n",
        "\n",
        "class VecDB:\n",
        "    def __init__(self, database_file_path = \"saved_db.dat\", index_file_path = \"index.dat\", new_db = True, db_size = None) -> None:\n",
        "        self.db_path = database_file_path\n",
        "        self.index_path = index_file_path\n",
        "        if new_db:\n",
        "            if db_size is None:\n",
        "                raise ValueError(\"You need to provide the size of the database\")\n",
        "            # delete the old DB file if exists\n",
        "            if os.path.exists(self.db_path):\n",
        "                os.remove(self.db_path)\n",
        "            self.generate_database(db_size)\n",
        "    \n",
        "    def generate_database(self, size: int) -> None:\n",
        "        rng = np.random.default_rng(DB_SEED_NUMBER)\n",
        "        vectors = rng.random((size, DIMENSION), dtype=np.float32)\n",
        "        self._write_vectors_to_file(vectors)\n",
        "        self._build_index()\n",
        "\n",
        "    def _write_vectors_to_file(self, vectors: np.ndarray) -> None:\n",
        "        mmap_vectors = np.memmap(self.db_path, dtype=np.float32, mode='w+', shape=vectors.shape)\n",
        "        mmap_vectors[:] = vectors[:]\n",
        "        mmap_vectors.flush()\n",
        "\n",
        "    def _get_num_records(self) -> int:\n",
        "        return os.path.getsize(self.db_path) // (DIMENSION * ELEMENT_SIZE)\n",
        "\n",
        "    def insert_records(self, rows: Annotated[np.ndarray, (int, 64)]):\n",
        "        num_old_records = self._get_num_records()\n",
        "        num_new_records = len(rows)\n",
        "        full_shape = (num_old_records + num_new_records, DIMENSION)\n",
        "        mmap_vectors = np.memmap(self.db_path, dtype=np.float32, mode='r+', shape=full_shape)\n",
        "        mmap_vectors[num_old_records:] = rows\n",
        "        mmap_vectors.flush()\n",
        "        #TODO: might change to call insert in the index, if you need\n",
        "        self._build_index()\n",
        "\n",
        "    def get_one_row(self, row_num: int) -> np.ndarray:\n",
        "        # This function is only load one row in memory\n",
        "        try:\n",
        "            offset = row_num * DIMENSION * ELEMENT_SIZE\n",
        "            mmap_vector = np.memmap(self.db_path, dtype=np.float32, mode='r', shape=(1, DIMENSION), offset=offset)\n",
        "            return np.array(mmap_vector[0])\n",
        "        except Exception as e:\n",
        "            return f\"An error occurred: {e}\"\n",
        "\n",
        "    def get_all_rows(self) -> np.ndarray:\n",
        "        # Take care this load all the data in memory\n",
        "        num_records = self._get_num_records()\n",
        "        vectors = np.memmap(self.db_path, dtype=np.float32, mode='r', shape=(num_records, DIMENSION))\n",
        "        return np.array(vectors)\n",
        "    \n",
        "    \n",
        "    \n",
        "    def _cal_score(self, vec1, vec2):\n",
        "        dot_product = np.dot(vec1, vec2)\n",
        "        norm_vec1 = np.linalg.norm(vec1)\n",
        "        norm_vec2 = np.linalg.norm(vec2)\n",
        "        cosine_similarity = dot_product / (norm_vec1 * norm_vec2)\n",
        "        return cosine_similarity\n",
        "\n",
        "\n",
        "\n",
        "    def retrieve(self, query, top_k=5, n_probe=None, chunk_size=10000):\n",
        "        query = np.asarray(query, dtype=np.float32).squeeze()\n",
        "\n",
        "        query_norm = np.linalg.norm(query)\n",
        "        if query_norm == 0:\n",
        "            query_norm = 1.0\n",
        "        normalized_query = query / query_norm\n",
        "\n",
        "        centers_path = os.path.join(self.index_path, \"centers.npy\")\n",
        "        if not os.path.exists(centers_path):\n",
        "            del query, normalized_query\n",
        "            return []\n",
        "\n",
        "        # Load centroids with memory-mapping to save RAM\n",
        "        centroids = np.load(centers_path, mmap_mode=\"r\")\n",
        "        n_centroids = centroids.shape[0]\n",
        "\n",
        "        # Determine number of centroids to probe\n",
        "        if n_probe is None:\n",
        "            n_probe = max(1, min(n_centroids, int(np.sqrt(self._get_num_records()))))\n",
        "            num_records = self._get_num_records()\n",
        "            if num_records <= 10 * 10**6:\n",
        "                n_probe = 12\n",
        "            elif num_records == 15 * 10**6:\n",
        "                n_probe = 10\n",
        "\n",
        "        # Compute similarity (cosine) to all centroids\n",
        "        sims = centroids.dot(normalized_query)\n",
        "        nearest_centroids = np.argpartition(sims, -n_probe)[-n_probe:]\n",
        "\n",
        "        # Clean up memory\n",
        "        del centroids, sims\n",
        "\n",
        "        # Load header that tells us where in the flat index file each cluster's indices are\n",
        "        header_path = os.path.join(self.index_path, \"index_header.json\")\n",
        "        with open(header_path, \"r\") as hf:\n",
        "            header = json.load(hf)\n",
        "\n",
        "        # Turn header into a dict for faster lookup: cid -> (offset, length)\n",
        "        header_dict = {item[\"cid\"]: (item[\"offset\"], item[\"length\"]) for item in header}\n",
        "\n",
        "        # Prepare top-k heap\n",
        "        top_heap = []\n",
        "\n",
        "        flat_index_path = os.path.join(self.index_path, \"all_indices.bin\")\n",
        "\n",
        "        # For each centroid to probe:\n",
        "        for c in nearest_centroids:\n",
        "            offset, length = header_dict[c]\n",
        "            if length == 0:\n",
        "                continue\n",
        "\n",
        "            # Process each chunk by remapping the memmap for that chunk\n",
        "            for start in range(0, length, chunk_size):\n",
        "                cur_len = min(chunk_size, length - start)\n",
        "                # Remap only this chunk\n",
        "                ids_mm = np.memmap(flat_index_path, dtype=np.uint32, mode=\"r\",\n",
        "                                offset=offset + start * np.dtype(np.uint32).itemsize,\n",
        "                                shape=(cur_len,))\n",
        "\n",
        "                chunk_ids = ids_mm[:]  # copy if you need to\n",
        "                del ids_mm  # free memmap\n",
        "\n",
        "                for id in chunk_ids:\n",
        "                    vec = self.get_one_row(id)\n",
        "                    norm = np.linalg.norm(vec)\n",
        "                    score = vec.dot(normalized_query) / (norm if norm != 0 else 1.0)\n",
        "                    if len(top_heap) < top_k:\n",
        "                        heapq.heappush(top_heap, (score, id))\n",
        "                    else:\n",
        "                        heapq.heappushpop(top_heap, (score, id))\n",
        "\n",
        "                    del vec, score\n",
        "                \n",
        "\n",
        "\n",
        "\n",
        "                del chunk_ids\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Extract and return top-k IDs sorted by score\n",
        "        results = [idx for score, idx in heapq.nlargest(top_k, top_heap)]\n",
        "        del top_heap\n",
        "        return results\n",
        "    def _build_index(self):\n",
        "       \n",
        "        # sqrt(N) rule\n",
        "        self.no_centroids = 250\n",
        "        # data is a reference to the memmap object, not the data in RAM\n",
        "        data = self.get_all_rows()\n",
        "\n",
        "        kmeans = KMeans(\n",
        "            n_clusters=self.no_centroids,\n",
        "            init='k-means++',   # <-- Use k-means++ initialization\n",
        "            random_state=42 \n",
        "        )\n",
        "\n",
        "        kmeans.fit(data)\n",
        "\n",
        "        # labels and centers are new arrays created in RAM\n",
        "        labels = kmeans.labels_\n",
        "        centers = kmeans.cluster_centers_.astype(np.float32)\n",
        "\n",
        "        # Added deletion of the memmap reference and the kmeans object\n",
        "        del data\n",
        "        del kmeans\n",
        "\n",
        "\n",
        "               \n",
        "        if not os.path.isdir(self.index_path):\n",
        "            os.makedirs(self.index_path, exist_ok=True)\n",
        "        # 1. Build up a list of (cluster_id, indices_array)\n",
        "        cluster_infos = []\n",
        "        for cid in range(self.no_centroids):\n",
        "            indices = np.where(labels == cid)[0].astype(np.uint32)\n",
        "            cluster_infos.append((cid, indices))\n",
        "\n",
        "\n",
        "        # 2. Write all indices into one big flat file\n",
        "        flat_path = os.path.join(self.index_path, \"all_indices.bin\")\n",
        "        with open(flat_path, \"wb\") as f:\n",
        "            offset = 0\n",
        "            header = []  # list of (cid, offset, length)\n",
        "            for cid, inds in cluster_infos:\n",
        "                length = inds.size\n",
        "                f.write(inds.tobytes())  # or .tofile but with offset tracking\n",
        "                header.append((cid, offset, length))\n",
        "                offset += length * inds.dtype.itemsize\n",
        "\n",
        "        # 3. Write header metadata\n",
        "        header_path = os.path.join(self.index_path, \"index_header.json\")\n",
        "\n",
        "        with open(header_path, \"w\") as hf:\n",
        "            json.dump([{\"cid\": cid, \"offset\": off, \"length\": length} for cid, off, length in header], hf)\n",
        "\n",
        "\n",
        "        norms = np.linalg.norm(centers, axis=1, keepdims=True)\n",
        "        centers = centers / (norms + 1e-12)\n",
        "        # 4. Save centers as before\n",
        "        np.save(os.path.join(self.index_path, \"centers.npy\"), centers)\n",
        "\n",
        "            \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "Sg2vfYgeyavn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "# from vec_db import VecDB\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "from memory_profiler import memory_usage\n",
        "import gc\n",
        "\n",
        "@dataclass\n",
        "class Result:\n",
        "    run_time: float\n",
        "    top_k: int\n",
        "    db_ids: List[int]\n",
        "    actual_ids: List[int]\n",
        "\n",
        "def run_queries(db, queries, top_k, actual_ids, num_runs):\n",
        "    \"\"\"\n",
        "    Run queries on the database and record results for each query.\n",
        "\n",
        "    Parameters:\n",
        "    - db: Database instance to run queries on.\n",
        "    - queries: List of query vectors.\n",
        "    - top_k: Number of top results to retrieve.\n",
        "    - actual_ids: List of actual results to evaluate accuracy.\n",
        "    - num_runs: Number of query executions to perform for testing.\n",
        "\n",
        "    Returns:\n",
        "    - List of Result\n",
        "    \"\"\"\n",
        "    global results\n",
        "    results = []\n",
        "    for i in range(num_runs):\n",
        "        tic = time.time()\n",
        "        db_ids = db.retrieve(queries[i], top_k)\n",
        "        toc = time.time()\n",
        "        run_time = toc - tic\n",
        "        results.append(Result(run_time, top_k, db_ids, actual_ids[i]))\n",
        "    return results\n",
        "\n",
        "def memory_usage_run_queries(args):\n",
        "    \"\"\"\n",
        "    Run queries and measure memory usage during the execution.\n",
        "\n",
        "    Parameters:\n",
        "    - args: Arguments to be passed to the run_queries function.\n",
        "\n",
        "    Returns:\n",
        "    - results: The results of the run_queries.\n",
        "    - memory_diff: The difference in memory usage before and after running the queries.\n",
        "    \"\"\"\n",
        "    global results\n",
        "    mem_before = max(memory_usage())\n",
        "    mem = memory_usage(proc=(run_queries, args, {}), interval = 1e-3)\n",
        "    return results, max(mem) - mem_before\n",
        "\n",
        "def evaluate_result(results: List[Result]):\n",
        "    \"\"\"\n",
        "    Evaluate the results based on accuracy and runtime.\n",
        "    Scores are negative. So getting 0 is the best score.\n",
        "\n",
        "    Parameters:\n",
        "    - results: A list of Result objects\n",
        "\n",
        "    Returns:\n",
        "    - avg_score: The average score across all queries.\n",
        "    - avg_runtime: The average runtime for all queries.\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "    run_time = []\n",
        "    for res in results:\n",
        "        run_time.append(res.run_time)\n",
        "        # case for retireving number not equal to top_k, socre will be the lowest\n",
        "        if len(set(res.db_ids)) != res.top_k or len(res.db_ids) != res.top_k:\n",
        "            scores.append( -1 * len(res.actual_ids) * res.top_k)\n",
        "            continue\n",
        "        score = 0\n",
        "        for id in res.db_ids:\n",
        "            try:\n",
        "                ind = res.actual_ids.index(id)\n",
        "                if ind > res.top_k * 3:\n",
        "                    score -= ind\n",
        "            except:\n",
        "                score -= len(res.actual_ids)\n",
        "        scores.append(score)\n",
        "\n",
        "    return sum(scores) / len(scores), sum(run_time) / len(run_time)\n",
        "\n",
        "def get_actual_ids_first_k(actual_sorted_ids, k):\n",
        "    \"\"\"\n",
        "    Retrieve the IDs from the sorted list of actual IDs.\n",
        "    actual IDs has the top_k for the 20 M database but for other databases we have to remove the numbers higher than the max size of the DB.\n",
        "\n",
        "    Parameters:\n",
        "    - actual_sorted_ids: A list of lists containing the sorted actual IDs for each query.\n",
        "    - k: The DB size.\n",
        "\n",
        "    Returns:\n",
        "    - List of lists containing the actual IDs for each query for this DB.\n",
        "    \"\"\"\n",
        "    return [[id for id in actual_sorted_ids_one_q if id < k] for actual_sorted_ids_one_q in actual_sorted_ids]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3bQQzzWlce4"
      },
      "source": [
        "This code to generate all the files for databases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "zZPsvyMqX17g"
      },
      "outputs": [],
      "source": [
        "def _write_vectors_to_file(vectors: np.ndarray, db_path) -> None:\n",
        "    mmap_vectors = np.memmap(db_path, dtype=np.float32, mode='w+', shape=vectors.shape)\n",
        "    mmap_vectors[:] = vectors[:]\n",
        "    mmap_vectors.flush()\n",
        "\n",
        "# def generate_database(size: int) -> None:\n",
        "#     rng = np.random.default_rng(DB_SEED_NUMBER)\n",
        "#     vectors = rng.random((size, DIMENSION), dtype=np.float32)\n",
        "#     return vectors\n",
        "def generated_database():\n",
        "    # Load memmap\n",
        "    mmap_vectors = np.memmap(\"data_20M.dat\", dtype=np.float32, mode='r', shape=(20_000_000, 64))\n",
        "    return mmap_vectors\n",
        "\n",
        "\n",
        "vectors = generated_database()\n",
        "\n",
        "db_filename_size_20M = 'saved_db_20M.dat'\n",
        "if not os.path.exists(db_filename_size_20M): _write_vectors_to_file(vectors, db_filename_size_20M)\n",
        "db_filename_size_15M = 'saved_db_15M.dat'\n",
        "if not os.path.exists(db_filename_size_15M): _write_vectors_to_file(vectors[:15*10**6], db_filename_size_15M)\n",
        "db_filename_size_10M = 'saved_db_10M.dat'\n",
        "if not os.path.exists(db_filename_size_10M): _write_vectors_to_file(vectors[:10*10**6], db_filename_size_10M)\n",
        "db_filename_size_1M = 'saved_db_1M.dat'\n",
        "if not os.path.exists(db_filename_size_1M): _write_vectors_to_file(vectors[:1*10**6], db_filename_size_1M)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c83ybYSKK85G",
        "outputId": "91dd0f3a-be2c-4dac-a42f-0a1aabe492f4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\m\\AppData\\Local\\Temp\\ipykernel_2944\\1873110300.py:8: RuntimeWarning: invalid value encountered in divide\n",
            "  actual_sorted_ids_20m_q1 = np.argsort(vectors.dot(query1.T).T / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(query1)), axis= 1).squeeze().tolist()[::-1][:needed_top_k]\n",
            "C:\\Users\\m\\AppData\\Local\\Temp\\ipykernel_2944\\1873110300.py:10: RuntimeWarning: invalid value encountered in divide\n",
            "  actual_sorted_ids_20m_q2 = np.argsort(vectors.dot(query2.T).T / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(query2)), axis= 1).squeeze().tolist()[::-1][:needed_top_k]\n",
            "C:\\Users\\m\\AppData\\Local\\Temp\\ipykernel_2944\\1873110300.py:12: RuntimeWarning: invalid value encountered in divide\n",
            "  actual_sorted_ids_20m_q3 = np.argsort(vectors.dot(query3.T).T / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(query3)), axis= 1).squeeze().tolist()[::-1][:needed_top_k]\n"
          ]
        }
      ],
      "source": [
        "needed_top_k = 10000\n",
        "rng = np.random.default_rng(QUERY_SEED_NUMBER)\n",
        "query1 = rng.random((1, 64), dtype=np.float32)\n",
        "query2 = rng.random((1, 64), dtype=np.float32)\n",
        "query3 = rng.random((1, 64), dtype=np.float32)\n",
        "query_dummy = rng.random((1, 64), dtype=np.float32)\n",
        "\n",
        "actual_sorted_ids_20m_q1 = np.argsort(vectors.dot(query1.T).T / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(query1)), axis= 1).squeeze().tolist()[::-1][:needed_top_k]\n",
        "gc.collect()\n",
        "actual_sorted_ids_20m_q2 = np.argsort(vectors.dot(query2.T).T / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(query2)), axis= 1).squeeze().tolist()[::-1][:needed_top_k]\n",
        "gc.collect()\n",
        "actual_sorted_ids_20m_q3 = np.argsort(vectors.dot(query3.T).T / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(query3)), axis= 1).squeeze().tolist()[::-1][:needed_top_k]\n",
        "gc.collect()\n",
        "\n",
        "queries = [query1, query2, query3]\n",
        "actual_sorted_ids_20m = [actual_sorted_ids_20m_q1, actual_sorted_ids_20m_q2, actual_sorted_ids_20m_q3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bW7eI-hIvIfb",
        "outputId": "6dc4b563-e084-4bcf-f6a4-d10148efc0fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# No more need to the actual vectors so delete it\n",
        "del vectors\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "g-hCaQNqlBP0"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "to_print_arr = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s-envHmfeBr"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qackwdJN8Tor",
        "outputId": "1a0ae8eb-dccf-431c-a3a3-b5be6d8a32f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Team Number 3\n"
          ]
        }
      ],
      "source": [
        "print(\"Team Number\", 3)\n",
        "database_info = {\n",
        "    \"1M\": {\n",
        "        \"database_file_path\": db_filename_size_1M,\n",
        "        \"index_file_path\": \"PATH_DB_1M\",\n",
        "        \"size\": 10**6\n",
        "    },\n",
        "    # \"10M\": {\n",
        "    #     \"database_file_path\": db_filename_size_10M,\n",
        "    #     \"index_file_path\": \"PATH_DB_10M\",\n",
        "    #     \"size\": 10 * 10**6\n",
        "    # },\n",
        "    # \"15M\": {\n",
        "    #     \"database_file_path\": db_filename_size_15M,\n",
        "    #     \"index_file_path\": PATH_DB_15M,\n",
        "    #     \"size\": 15 * 10**6\n",
        "    # },\n",
        "    # \"20M\": {\n",
        "    #     \"database_file_path\": db_filename_size_20M,\n",
        "    #     \"index_file_path\": PATH_DB_20M,\n",
        "    #     \"size\": 20 * 10**6\n",
        "    # }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "EXM0xTij8IhU"
      },
      "outputs": [],
      "source": [
        "\n",
        "for db_name, info in database_info.items():\n",
        "    db = VecDB(database_file_path = info[\"database_file_path\"], index_file_path = info[\"index_file_path\"], new_db = False)\n",
        "    db._build_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "h-UFbhBPlQtz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1M\tscore\t0.0\ttime\t3.61\tRAM\t1.62 MB\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for db_name, info in database_info.items():\n",
        "    db = VecDB(database_file_path = info[\"database_file_path\"], index_file_path = info[\"index_file_path\"], new_db = False)\n",
        "    actual_ids = get_actual_ids_first_k(actual_sorted_ids_20m, info[\"size\"])\n",
        "    # Make a dummy run query to make everything fresh and loaded (wrap up)\n",
        "    res = run_queries(db, query_dummy, 5, actual_ids, 1)\n",
        "    # actual runs to evaluate\n",
        "    res, mem = memory_usage_run_queries((db, queries, 5, actual_ids, 3))\n",
        "    eval = evaluate_result(res)\n",
        "    to_print = f\"{db_name}\\tscore\\t{eval[0]}\\ttime\\t{eval[1]:.2f}\\tRAM\\t{mem:.2f} MB\"\n",
        "    print(to_print)\n",
        "    to_print_arr.append(to_print)\n",
        "    del db\n",
        "    del actual_ids\n",
        "    del res\n",
        "    del mem\n",
        "    del eval\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jt1_7ihfB37Z"
      },
      "outputs": [],
      "source": [
        "print(\"Team Number\", 3)\n",
        "print(\"\\n\".join(to_print_arr))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
