{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj9-3U--Krvc"
      },
      "source": [
        "# ADB Phase 2 Project Evaluation Notebook\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hV2Nc_f8Mbqh"
      },
      "source": [
        "**Purpose**: This notebook evaluates the performance of a semantic search project by analyzing databases of various sizes.\n",
        "\n",
        "### Evaluation Focus:\n",
        "- **Database Sizes**:\n",
        "  - 1 Million Records\n",
        "  - 10 Million Records\n",
        "  - 15 Million Records\n",
        "  - 20 Million Records\n",
        "\n",
        "For each database size, this notebook will:\n",
        "- Generate random vectors for the database.\n",
        "- Use the `VecDB` class (implemented by students) to retrieve queries\n",
        "- Evaluate and report retrieval time, accuracy, and RAM usage.\n",
        "\n",
        "### Project Constraints:\n",
        "Refer to the project document for details on RAM, Disk, Time, and Score constraints.\n",
        "\n",
        "### Notebook Structure:\n",
        "1. **Part 1 - Modifiable Cells**:\n",
        "   - Includes cells that teams are allowed to modify, specifically for these variables only:\n",
        "     - GitHub repository link (including PAT token).\n",
        "     - Google Drive IDs for indexes files.\n",
        "     - Paths for loading existing indexes.\n",
        "\n",
        "2. **Part 2 - Non-Modifiable Cells**:\n",
        "   - Contains essential setup and evaluation code that must not be modified.\n",
        "   - Students should only modify inputs in Part 1 to ensure smooth execution of the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4EV_xB6Kw17"
      },
      "source": [
        "## Part 1 - Modifiable Cells"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AODP-iztLtBV"
      },
      "source": [
        "Each team must provide a unique GitHub repository link that includes a PAT token. This link will allow the notebook to download the necessary code for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3WwwMZKsdBu",
        "outputId": "26a6d636-48c0-4ad1-b761-0fdcc4161b5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "f:\\School\\4th-year\\Semester-1-F25\\ADB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\hankerz\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
          ]
        }
      ],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keELA25WsgZi",
        "outputId": "0640fe96-240b-4878-f565-485e1139337e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Volume in drive F is New Volume\n",
            " Volume Serial Number is D605-7CF8\n",
            "\n",
            " Directory of f:\\School\\4th-year\\Semester-1-F25\\ADB\n",
            "\n",
            "11/27/2025  08:51 PM    <DIR>          .\n",
            "10/24/2025  09:39 PM    <DIR>          ..\n",
            "10/24/2025  09:40 PM    <DIR>          semantic-search-index\n",
            "11/28/2025  07:53 PM    <DIR>          vec_db\n",
            "               0 File(s)              0 bytes\n",
            "               4 Dir(s)  91,653,292,032 bytes free\n"
          ]
        }
      ],
      "source": [
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNzc-QrjsiAh"
      },
      "outputs": [],
      "source": [
        "%rm -rf vec_db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCR6Z8ABxE3w",
        "outputId": "fb9a27b0-1926-4ee1-9517-41f483365bfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'vec_db'...\n",
            "remote: Enumerating objects: 97, done.\u001b[K\n",
            "remote: Counting objects: 100% (97/97), done.\u001b[K\n",
            "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
            "remote: Total 97 (delta 52), reused 67 (delta 25), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (97/97), 40.17 KiB | 5.02 MiB/s, done.\n",
            "Resolving deltas: 100% (52/52), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone -b IVF_opt --single-branch https://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@github.com/Karimgebril09/vec_db.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyDa2kFksx6z",
        "outputId": "8ce74120-eb94-4ef7-c329-83e6409f59f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "import json\n",
            "import numpy as np\n",
            "import os\n",
            "import shutil\n",
            "import tqdm\n",
            "import heapq\n",
            "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
            "from typing import Annotated\n",
            "import time\n",
            "\n",
            "DB_SEED_NUMBER = 42\n",
            "ELEMENT_SIZE = np.dtype(np.float32).itemsize\n",
            "DIMENSION = 64\n",
            "\n",
            "class VecDB:\n",
            "    def __init__(self, database_file_path = \"saved_db.dat\", index_file_path = \"index.dat\", new_db = True, db_size = None) -> None:\n",
            "        self.db_path = database_file_path\n",
            "        self.index_path = index_file_path\n",
            "        if new_db:\n",
            "            if db_size is None:\n",
            "                raise ValueError(\"You need to provide the size of the database\")\n",
            "            # delete the old DB file if exists\n",
            "            if os.path.exists(self.db_path):\n",
            "                os.remove(self.db_path)\n",
            "            self.generate_database(db_size)\n",
            "    \n",
            "    def generate_database(self, size: int) -> None:\n",
            "        # rng = np.random.default_rng(DB_SEED_NUMBER)\n",
            "        vectors = np.memmap(\"new_embeddings.dat\", dtype=np.float32, mode='r', shape=(size, DIMENSION))\n",
            "        self._write_vectors_to_file(vectors)\n",
            "        self._build_index()\n",
            "\n",
            "    def _write_vectors_to_file(self, vectors: np.ndarray) -> None:\n",
            "        mmap_vectors = np.memmap(self.db_path, dtype=np.float32, mode='w+', shape=vectors.shape)\n",
            "        mmap_vectors[:] = vectors[:]\n",
            "        mmap_vectors.flush()\n",
            "\n",
            "    def _get_num_records(self) -> int:\n",
            "        return os.path.getsize(self.db_path) // (DIMENSION * ELEMENT_SIZE)\n",
            "\n",
            "    def insert_records(self, rows: Annotated[np.ndarray, (int, 64)]):\n",
            "        num_old_records = self._get_num_records()\n",
            "        num_new_records = len(rows)\n",
            "        full_shape = (num_old_records + num_new_records, DIMENSION)\n",
            "        mmap_vectors = np.memmap(self.db_path, dtype=np.float32, mode='r+', shape=full_shape)\n",
            "        mmap_vectors[num_old_records:] = rows\n",
            "        mmap_vectors.flush()\n",
            "        #TODO: might change to call insert in the index, if you need\n",
            "        self._build_index()\n",
            "\n",
            "    def get_one_row(self, row_num: int) -> np.ndarray:\n",
            "        # This function is only load one row in memory\n",
            "        try:\n",
            "            offset = row_num * DIMENSION * ELEMENT_SIZE\n",
            "            mmap_vector = np.memmap(self.db_path, dtype=np.float32, mode='r', shape=(1, DIMENSION), offset=offset)\n",
            "            return np.array(mmap_vector[0])\n",
            "        except Exception as e:\n",
            "            return f\"An error occurred: {e}\"\n",
            "\n",
            "    def get_all_rows(self) -> np.ndarray:\n",
            "        # Take care this load all the data in memory\n",
            "        num_records = self._get_num_records()\n",
            "        vectors = np.memmap(self.db_path, dtype=np.float32, mode='r', shape=(num_records, DIMENSION))\n",
            "        return np.array(vectors)\n",
            "    \n",
            "    def get_all_ids_rows_optimized(self, ids):\n",
            "        \"\"\"\n",
            "        Load rows corresponding to a list of IDs from disk efficiently.\n",
            "        Reads a single contiguous block from start_id to end_id, then selects only required IDs.\n",
            "\n",
            "        ids: list or np.array of integers (IDs within a window)\n",
            "        returns: np.array of shape (len(ids), DIMENSION)\n",
            "        \"\"\"\n",
            "        ids = np.array(ids, dtype=np.uint32)\n",
            "        ids.sort()  # ensure sorted\n",
            "\n",
            "        start_id = ids[0]\n",
            "        end_id = ids[-1]\n",
            "\n",
            "        # Read contiguous block from disk (from start_id to end_id)\n",
            "        offset = start_id * DIMENSION * 4   # float32 = 4 bytes\n",
            "        shape = (end_id - start_id + 1, DIMENSION)\n",
            "\n",
            "        block = np.memmap(self.db_path, dtype=np.float32, mode='r', offset=offset, shape=shape)\n",
            "        block_array = block[:]  \n",
            "        del block  \n",
            "\n",
            "        # Compute relative indices within the block\n",
            "        relative_indices = ids - start_id\n",
            "\n",
            "        # Select only the rows we care about\n",
            "        rows = block_array[relative_indices, :]\n",
            "        del block_array\n",
            "\n",
            "        return rows\n",
            "        \n",
            "    \n",
            "\n",
            "    def group_ids_by_window_fast(self, all_ids, window):\n",
            "        all_ids = np.asarray(all_ids)\n",
            "        n = all_ids.size\n",
            "        if n == 0:\n",
            "            return []\n",
            "\n",
            "        # Compute per-element group max = all_ids[i] + window\n",
            "        limits = all_ids + window\n",
            "\n",
            "        # For each i, ends[i] = first index where id > limits[i]\n",
            "        ends = np.searchsorted(all_ids, limits + 1, side=\"left\")\n",
            "\n",
            "        # We find group starts: a new group starts where i == previous group's end\n",
            "        starts = [0]\n",
            "        for i in range(1, n):\n",
            "            if ends[i-1] == i:\n",
            "                starts.append(i)\n",
            "\n",
            "        starts = np.array(starts, dtype=np.int64)\n",
            "        group_ends = ends[starts]\n",
            "        groups = [all_ids[s:e] for s, e in zip(starts, group_ends)]\n",
            "        return groups\n",
            "\n",
            "    \n",
            "    \n",
            "    def _cal_score(self, vec1, vec2):\n",
            "        dot_product = np.dot(vec1, vec2)\n",
            "        norm_vec1 = np.linalg.norm(vec1)\n",
            "        norm_vec2 = np.linalg.norm(vec2)\n",
            "        cosine_similarity = dot_product / (norm_vec1 * norm_vec2)\n",
            "        return cosine_similarity\n",
            "\n",
            "####################################################################################\n",
            "####################################################################################\n",
            "####################################################################################\n",
            "\n",
            "\n",
            "    \n",
            "\n",
            "\n",
            "    def retrieve(self, query, top_k=5, n_probe=None, chunk_size=50):\n",
            "        self.no_centroids = 7000\n",
            "        self.index_path = f\"index_10M_{self.no_centroids}_centroids\"\n",
            "        query = np.asarray(query, dtype=np.float32).squeeze()\n",
            "\n",
            "        query_norm = np.linalg.norm(query)\n",
            "        if query_norm == 0:\n",
            "            query_norm = 1.0\n",
            "        normalized_query = query / query_norm\n",
            "\n",
            "        centers_path = os.path.join(self.index_path, \"centroids.npy\")\n",
            "        if not os.path.exists(centers_path):\n",
            "            del query, normalized_query\n",
            "            return []\n",
            "\n",
            "        centroids = np.load(centers_path, mmap_mode=\"r\")\n",
            "        num_centroids, dim = centroids.shape\n",
            "\n",
            "        # Auto n_probe choice\n",
            "        if n_probe is None:\n",
            "            n_probe = 10 if num_centroids <= 15_000_000 else 8\n",
            "\n",
            "        heap = []\n",
            "\n",
            "        # Loop over batches\n",
            "        for start in range(0, num_centroids, 2000):\n",
            "            end = min(start + 2000, num_centroids)\n",
            "\n",
            "            batch = centroids[start:end]\n",
            "\n",
            "            sims = batch.dot(normalized_query)\n",
            "\n",
            "            # Insert into heap\n",
            "            for i, s in enumerate(sims):\n",
            "                cid = start + i\n",
            "                if len(heap) < n_probe:\n",
            "                    heapq.heappush(heap, (s, cid))\n",
            "                else:\n",
            "                    heapq.heappushpop(heap, (s, cid))\n",
            "\n",
            "            del sims\n",
            "\n",
            "        del centroids\n",
            "\n",
            "          # Extract top n_probe IDs sorted descending similarity\n",
            "        top = heapq.nlargest(n_probe, heap)\n",
            "        nearest_centroids = np.array([cid for (_, cid) in top], dtype=np.int32)\n",
            "        del heap, top\n",
            "\n",
            "\n",
            "        header_arr = np.fromfile(os.path.join(self.index_path, \"index_header.bin\"), dtype=np.uint32)\n",
            "        header_arr = header_arr.reshape(-1, 2)   # shape: (num_centroids, 2)\n",
            "\n",
            "\n",
            "        all_ids = []\n",
            "        flat_index_path = os.path.join(self.index_path, \"all_indices.bin\")\n",
            "        for c in nearest_centroids:\n",
            "            offset  = header_arr[c, 0]   # first column\n",
            "            length  = header_arr[c, 1]   # second column\n",
            "            if length == 0:\n",
            "                continue\n",
            "            ids_mm = np.memmap(flat_index_path, dtype=np.uint32, mode=\"r\",\n",
            "                            offset=offset, shape=(length,))\n",
            "            db_ids = ids_mm[:]\n",
            "            del ids_mm  \n",
            "            all_ids.extend(db_ids)\n",
            "\n",
            "\n",
            "        all_ids.sort()\n",
            "\n",
            "        grouped_ids = self.group_ids_by_window_fast(all_ids, window=1500)\n",
            "        del all_ids \n",
            "        top_heap = []\n",
            "\n",
            "        for group in grouped_ids:\n",
            "            vecs = self.get_all_ids_rows_optimized(group)\n",
            "            scores = vecs.dot(normalized_query)\n",
            "            for score, id in zip(scores, group):\n",
            "                if len(top_heap) < top_k:\n",
            "                    heapq.heappush(top_heap, (score, id))\n",
            "                else:\n",
            "                    heapq.heappushpop(top_heap, (score, id))\n",
            "\n",
            "            del scores, vecs\n",
            "        del grouped_ids\n",
            "\n",
            "        results = [idx for score, idx in heapq.nlargest(top_k, top_heap)]\n",
            "        del top_heap\n",
            "        return results\n",
            "\n",
            "\n",
            "    \n",
            "\n",
            "    def _build_index(self):\n",
            "       \n",
            "        # sqrt(N) rule\n",
            "        self.no_centroids = 7000\n",
            "        self.index_path = f\"index_10M_{self.no_centroids}_centroids\"\n",
            "        # data is a reference to the memmap object, not the data in RAM\n",
            "        data = self.get_all_rows()\n",
            "\n",
            "        kmeans = MiniBatchKMeans(\n",
            "            n_clusters=self.no_centroids,\n",
            "            init=\"k-means++\",   # supported and default\n",
            "            batch_size=10_000,\n",
            "            random_state=42\n",
            "        )\n",
            "\n",
            "        kmeans.fit(data)\n",
            "\n",
            "        # labels and centers are new arrays created in RAM\n",
            "        labels = kmeans.labels_\n",
            "        centers = kmeans.cluster_centers_.astype(np.float32)\n",
            "\n",
            "        # Added deletion of the memmap reference and the kmeans object\n",
            "        del data\n",
            "        del kmeans\n",
            "       \n",
            "         \n",
            "\n",
            "        if os.path.exists(self.index_path):\n",
            "            shutil.rmtree(self.index_path)  # remove old index if any\n",
            "        os.makedirs(self.index_path, exist_ok=True)\n",
            "               \n",
            "        if not os.path.isdir(self.index_path):\n",
            "            os.makedirs(self.index_path, exist_ok=True)\n",
            "        # 1. Build up a list of (cluster_id, indices_array)\n",
            "        cluster_infos = []\n",
            "        for cid in range(self.no_centroids):\n",
            "            indices = np.where(labels == cid)[0].astype(np.uint32)\n",
            "            cluster_infos.append((cid, indices))\n",
            "\n",
            "      \n",
            "        header = []\n",
            "\n",
            "        flat_path = os.path.join(self.index_path, \"all_indices.bin\")\n",
            "        with open(flat_path, \"wb\") as f:\n",
            "            offset = 0\n",
            "            for cid, inds in cluster_infos:\n",
            "                length = inds.size\n",
            "                f.write(inds.tobytes())\n",
            "                header.append([offset, length])\n",
            "                offset += length * inds.dtype.itemsize\n",
            "\n",
            "        # Convert to a matrix (2 columns: offset, length)\n",
            "        header_matrix = np.array(header, dtype=np.uint32)\n",
            "\n",
            "        # Save matrix as binary file\n",
            "        header_bin_path = os.path.join(self.index_path, \"index_header.bin\")\n",
            "        header_matrix.tofile(header_bin_path)\n",
            "\n",
            "        norms = np.linalg.norm(centers, axis=1, keepdims=True)\n",
            "        centers = centers / (norms + 1e-12)\n",
            "        # 4. Save centers as before\n",
            "        np.save(os.path.join(self.index_path, \"centroids.npy\"), centers)"
          ]
        }
      ],
      "source": [
        "%cat vec_db/vec_db.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7QSIX510KMF"
      },
      "source": [
        "# Database Path Instructions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsUXWYom6xRv"
      },
      "source": [
        "Teams need to specify paths for each database (1M, 10M, 15M, 20M records) as follows:\n",
        "\n",
        "1. Zip each database directory/file after generation.\n",
        "2. Upload the zip file to Google Drive.\n",
        "3. Share the file with \"Anyone with the link.\"\n",
        "4. Extract the file ID from the link (e.g., for `https://drive.google.com/file/d/1j1gAU3kvdRqcOoKI5K5FgMMUZpOQANah/view`, the ID is `1j1gAU3kvdRqcOoKI5K5FgMMUZpOQANah`).\n",
        "5. Assign each ID to the appropriate variable in Part 1.\n",
        "6. Provide the local PATH for each database to be passed to the initializer for automatic loading of the database and index (to be submitted during the project final phase).\n",
        "\n",
        "**Note**: The code will download and unzip these files automatically. Once extracted, the local path for each database should be specified to enable the notebook to load databases and indexes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kK46_ZVe5L3u"
      },
      "outputs": [],
      "source": [
        "## compressed file of index\n",
        "# GDRIVE_ID_DB_1M = \"1CA9ISh45NkKqQvAB0wF7G_cKXaniYOrg\"\n",
        "GDRIVE_ID_DB_10M = \"1rWw1Gda_Kpx021MqAX9td-vFow_wYKPj\"\n",
        "# GDRIVE_ID_DB_15M = \"1sPIgNxIuNDUUBnTvAuPMLryd1mqmgwV3\"\n",
        "# GDRIVE_ID_DB_20M = \"1j1gAU3kvdRqcOoKI5K5FgMMUZpOQANah\"\n",
        "## path to index\n",
        "# PATH_DB_1M = \"index.dat/\"\n",
        "PATH_DB_10M = \"index_10M_7000_centroids_f16\"\n",
        "# PATH_DB_15M = \"saved_db_15m.csv\"\n",
        "# PATH_DB_20M = \"saved_db_20m.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LGLg01fsujm"
      },
      "source": [
        "**Query Seed Number**:\n",
        "This number will be adjusted during discussions by the instructor.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "G44iH6jnObEj"
      },
      "outputs": [],
      "source": [
        "QUERY_SEED_NUMBER = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWaZ-ByWOIcK"
      },
      "source": [
        "**Final Submission Checklist**:\n",
        "Ensure the following items are included in your final submission:\n",
        "- `TEAM_NUMBER`\n",
        "- GitHub clone link (with PAT token)\n",
        "- Google Drive IDs for each database:\n",
        "  - `GDRIVE_ID_DB_1M`, `GDRIVE_ID_DB_10M`, `GDRIVE_ID_DB_15M`, `GDRIVE_ID_DB_20M`\n",
        "- Paths for each database:\n",
        "  - `PATH_DB_1M`, `PATH_DB_10M`, `PATH_DB_15M`, `PATH_DB_20M`\n",
        "- Project document detailing the work and findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzFTOecwu8wj"
      },
      "source": [
        "## Part 2: Do Not Modify Beyond This Point\n",
        "### Note:\n",
        "This section contains setup and evaluation code that should not be edited by students. Only the instructor may modify this section in case of a major bug.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "67NUn3KWXA6u"
      },
      "outputs": [],
      "source": [
        "# %load_ext autoreload\n",
        "# %autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dq5kOZBVDeN",
        "outputId": "55a420d3-f79c-4bf4-d193-7efb6c1d5d40"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'ls' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dqujj7tYTA1l",
        "outputId": "ffd58e91-bd3a-46d4-8f33-1b305bb68679"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "f:\\School\\4th-year\\Semester-1-F25\\ADB\\vec_db\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\hankerz\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
          ]
        }
      ],
      "source": [
        "%cd vec_db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJmXzFdisD7P"
      },
      "source": [
        "This cell to run any additional requirement that your code need <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaPjq2hMqd20",
        "outputId": "4e24355d-7dfc-4e18-a10d-e1ed490695fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: memory-profiler in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (0.61.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from memory-profiler->-r requirements.txt (line 2)) (5.9.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install memory-profiler >> log.txt\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lG0DALR498__"
      },
      "source": [
        "This cell to download the zip files and unzip them here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSv2z0PVp6HA",
        "outputId": "5bd079a7-2c2e-443b-a3ba-2e78ccc1cebc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1rWw1Gda_Kpx021MqAX9td-vFow_wYKPj\n",
            "From (redirected): https://drive.google.com/uc?id=1rWw1Gda_Kpx021MqAX9td-vFow_wYKPj&confirm=t&uuid=a31c706b-ad4b-4c33-ad00-7ba8c713c338\n",
            "To: /content/vec_db/saved_db_10m.zip\n",
            "100% 26.8M/26.8M [00:00<00:00, 58.8MB/s]\n",
            "Archive:  saved_db_10m.zip\n",
            "   creating: index_10M_7000_centroids_f16/\n",
            "  inflating: index_10M_7000_centroids_f16/all_indices.bin  \n",
            "  inflating: index_10M_7000_centroids_f16/centroids.npy  \n",
            "  inflating: index_10M_7000_centroids_f16/index_header.bin  \n"
          ]
        }
      ],
      "source": [
        "# !gdown $GDRIVE_ID_DB_1M -O saved_db_1m.zip\n",
        "!gdown $GDRIVE_ID_DB_10M -O saved_db_10m.zip\n",
        "# !gdown $GDRIVE_ID_DB_15M -O saved_db_15m.zip\n",
        "# !gdown $GDRIVE_ID_DB_20M -O saved_db_20m.zip\n",
        "# !unzip saved_db_1m.zip\n",
        "!unzip saved_db_10m.zip\n",
        "# !unzip saved_db_15m.zip\n",
        "# !unzip saved_db_20m.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShuPR-gGlX3f"
      },
      "source": [
        "These are the functions for running and reporting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "miaF8OeibbIL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "DB_SEED_NUMBER = 42\n",
        "ELEMENT_SIZE = np.dtype(np.float32).itemsize\n",
        "DIMENSION = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Sg2vfYgeyavn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from vec_db import VecDB\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "from memory_profiler import memory_usage\n",
        "import gc\n",
        "\n",
        "@dataclass\n",
        "class Result:\n",
        "    run_time: float\n",
        "    top_k: int\n",
        "    db_ids: List[int]\n",
        "    actual_ids: List[int]\n",
        "\n",
        "def run_queries(db, queries, top_k, actual_ids, num_runs):\n",
        "    \"\"\"\n",
        "    Run queries on the database and record results for each query.\n",
        "\n",
        "    Parameters:\n",
        "    - db: Database instance to run queries on.\n",
        "    - queries: List of query vectors.\n",
        "    - top_k: Number of top results to retrieve.\n",
        "    - actual_ids: List of actual results to evaluate accuracy.\n",
        "    - num_runs: Number of query executions to perform for testing.\n",
        "\n",
        "    Returns:\n",
        "    - List of Result\n",
        "    \"\"\"\n",
        "    global results\n",
        "    results = []\n",
        "    for i in range(num_runs):\n",
        "        tic = time.time()\n",
        "        db_ids = db.retrieve(queries[i], top_k)\n",
        "        toc = time.time()\n",
        "        run_time = toc - tic\n",
        "        results.append(Result(run_time, top_k, db_ids, actual_ids[i]))\n",
        "    return results\n",
        "\n",
        "def memory_usage_run_queries(args):\n",
        "    \"\"\"\n",
        "    Run queries and measure memory usage during the execution.\n",
        "\n",
        "    Parameters:\n",
        "    - args: Arguments to be passed to the run_queries function.\n",
        "\n",
        "    Returns:\n",
        "    - results: The results of the run_queries.\n",
        "    - memory_diff: The difference in memory usage before and after running the queries.\n",
        "    \"\"\"\n",
        "    global results\n",
        "    mem_before = max(memory_usage())\n",
        "    mem = memory_usage(proc=(run_queries, args, {}), interval = 1e-3)\n",
        "    return results, max(mem) - mem_before\n",
        "\n",
        "def evaluate_result(results: List[Result]):\n",
        "    \"\"\"\n",
        "    Evaluate the results based on accuracy and runtime.\n",
        "    Scores are negative. So getting 0 is the best score.\n",
        "\n",
        "    Parameters:\n",
        "    - results: A list of Result objects\n",
        "\n",
        "    Returns:\n",
        "    - avg_score: The average score across all queries.\n",
        "    - avg_runtime: The average runtime for all queries.\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "    run_time = []\n",
        "    for res in results:\n",
        "        run_time.append(res.run_time)\n",
        "        # case for retireving number not equal to top_k, socre will be the lowest\n",
        "        if len(set(res.db_ids)) != res.top_k or len(res.db_ids) != res.top_k:\n",
        "            scores.append( -1 * len(res.actual_ids) * res.top_k)\n",
        "            continue\n",
        "        score = 0\n",
        "        for id in res.db_ids:\n",
        "            try:\n",
        "                ind = res.actual_ids.index(id)\n",
        "                if ind > res.top_k * 3:\n",
        "                    score -= ind\n",
        "            except:\n",
        "                score -= len(res.actual_ids)\n",
        "        scores.append(score)\n",
        "\n",
        "    return sum(scores) / len(scores), sum(run_time) / len(run_time)\n",
        "\n",
        "def get_actual_ids_first_k(actual_sorted_ids, k):\n",
        "    \"\"\"\n",
        "    Retrieve the IDs from the sorted list of actual IDs.\n",
        "    actual IDs has the top_k for the 20 M database but for other databases we have to remove the numbers higher than the max size of the DB.\n",
        "\n",
        "    Parameters:\n",
        "    - actual_sorted_ids: A list of lists containing the sorted actual IDs for each query.\n",
        "    - k: The DB size.\n",
        "\n",
        "    Returns:\n",
        "    - List of lists containing the actual IDs for each query for this DB.\n",
        "    \"\"\"\n",
        "    return [[id for id in actual_sorted_ids_one_q if id < k] for actual_sorted_ids_one_q in actual_sorted_ids]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3bQQzzWlce4"
      },
      "source": [
        "This code to generate all the files for databases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KW94f2tmYCx2",
        "outputId": "28374e96-9815-45a8-a320-bb707945ab32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1a7KL0BmPeW8SsckllNTtCX42L1gS_8U0\n",
            "From (redirected): https://drive.google.com/uc?id=1a7KL0BmPeW8SsckllNTtCX42L1gS_8U0&confirm=t&uuid=af69dcf8-d32f-4b6a-9c1d-87c4b7ff3521\n",
            "To: /content/vec_db/data_20M.dat\n",
            "100% 5.12G/5.12G [00:46<00:00, 109MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1a7KL0BmPeW8SsckllNTtCX42L1gS_8U0 -O data_20M.dat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zZPsvyMqX17g"
      },
      "outputs": [],
      "source": [
        "def _write_vectors_to_file(vectors: np.ndarray, db_path) -> None:\n",
        "    mmap_vectors = np.memmap(db_path, dtype=np.float32, mode='w+', shape=vectors.shape)\n",
        "    mmap_vectors[:] = vectors[:]\n",
        "    mmap_vectors.flush()\n",
        "\n",
        "# def generate_database(size: int) -> None:\n",
        "#     rng = np.random.default_rng(DB_SEED_NUMBER)\n",
        "#     vectors = rng.random((size, DIMENSION), dtype=np.float32)\n",
        "#     return vectors\n",
        "def generated_database():\n",
        "    # Load memmap\n",
        "    mmap_vectors = np.memmap(\"data_20M.dat\", dtype=np.float32, mode='r', shape=(20_000_000, 64))\n",
        "    return mmap_vectors\n",
        "\n",
        "\n",
        "vectors = generated_database()\n",
        "\n",
        "db_filename_size_20M = 'saved_db_20M.dat'\n",
        "if not os.path.exists(db_filename_size_20M): _write_vectors_to_file(vectors, db_filename_size_20M)\n",
        "db_filename_size_15M = 'saved_db_15M.dat'\n",
        "if not os.path.exists(db_filename_size_15M): _write_vectors_to_file(vectors[:15*10**6], db_filename_size_15M)\n",
        "db_filename_size_10M = 'saved_db_10M.dat'\n",
        "if not os.path.exists(db_filename_size_10M): _write_vectors_to_file(vectors[:10*10**6], db_filename_size_10M)\n",
        "db_filename_size_1M = 'saved_db_1M.dat'\n",
        "if not os.path.exists(db_filename_size_1M): _write_vectors_to_file(vectors[:1*10**6], db_filename_size_1M)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4GDcog3dMZY"
      },
      "source": [
        "Code to generate the queries that will be used to evaluate the questions.\n",
        "\n",
        "Note: QUERY_SEED_NUMBER will be changed at submission day"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RydeHXEEZX6o",
        "outputId": "335e6f4a-4782-4eee-e259-6b5c4cd79bae"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'vectors' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mvectors\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n",
            "\u001b[1;31mNameError\u001b[0m: name 'vectors' is not defined"
          ]
        }
      ],
      "source": [
        "len(vectors[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c83ybYSKK85G",
        "outputId": "02ad4d0d-c145-43f2-e73a-5d635957c004"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1873110300.py:8: RuntimeWarning: invalid value encountered in divide\n",
            "  actual_sorted_ids_20m_q1 = np.argsort(vectors.dot(query1.T).T / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(query1)), axis= 1).squeeze().tolist()[::-1][:needed_top_k]\n",
            "/tmp/ipython-input-1873110300.py:10: RuntimeWarning: invalid value encountered in divide\n",
            "  actual_sorted_ids_20m_q2 = np.argsort(vectors.dot(query2.T).T / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(query2)), axis= 1).squeeze().tolist()[::-1][:needed_top_k]\n",
            "/tmp/ipython-input-1873110300.py:12: RuntimeWarning: invalid value encountered in divide\n",
            "  actual_sorted_ids_20m_q3 = np.argsort(vectors.dot(query3.T).T / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(query3)), axis= 1).squeeze().tolist()[::-1][:needed_top_k]\n"
          ]
        }
      ],
      "source": [
        "needed_top_k = 10000\n",
        "rng = np.random.default_rng(QUERY_SEED_NUMBER)\n",
        "query1 = rng.random((1, 64), dtype=np.float32)\n",
        "query2 = rng.random((1, 64), dtype=np.float32)\n",
        "query3 = rng.random((1, 64), dtype=np.float32)\n",
        "query_dummy = rng.random((1, 64), dtype=np.float32)\n",
        "\n",
        "actual_sorted_ids_20m_q1 = np.argsort(vectors.dot(query1.T).T / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(query1)), axis= 1).squeeze().tolist()[::-1][:needed_top_k]\n",
        "gc.collect()\n",
        "actual_sorted_ids_20m_q2 = np.argsort(vectors.dot(query2.T).T / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(query2)), axis= 1).squeeze().tolist()[::-1][:needed_top_k]\n",
        "gc.collect()\n",
        "actual_sorted_ids_20m_q3 = np.argsort(vectors.dot(query3.T).T / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(query3)), axis= 1).squeeze().tolist()[::-1][:needed_top_k]\n",
        "gc.collect()\n",
        "\n",
        "queries = [query1, query2, query3]\n",
        "actual_sorted_ids_20m = [actual_sorted_ids_20m_q1, actual_sorted_ids_20m_q2, actual_sorted_ids_20m_q3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bW7eI-hIvIfb",
        "outputId": "9d3aefae-8657-4457-9f4e-0706516c7357"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# No more need to the actual vectors so delete it\n",
        "del vectors\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrOlipAOmy9K"
      },
      "source": [
        "This code to actually run the class you have been implemented. The `VecDB` class should take the database path, and index path that you provided.<br>\n",
        "Note at the submission I'll not run the insert records. <br>\n",
        "The query istelf will be changed at submissions day but not the DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "g-hCaQNqlBP0"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "to_print_arr = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-UFbhBPlQtz",
        "outputId": "0d17be44-a751-4949-ea33-ca207957bf68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Team Number 3\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'PATH_DB_10M' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTeam Number\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m      2\u001b[0m database_info \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# \"1M\": {\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m#     \"database_file_path\": db_filename_size_1M,\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m#     \"index_file_path\": PATH_DB_1M,\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m#     \"size\": 10**6\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# },\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m10M\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatabase_file_path\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved_db.dat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m---> 10\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex_file_path\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mPATH_DB_10M\u001b[49m,\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m6\u001b[39m\n\u001b[0;32m     12\u001b[0m     },\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# \"15M\": {\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m#     \"database_file_path\": db_filename_size_15M,\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m#     \"index_file_path\": PATH_DB_15M,\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m#     \"size\": 15 * 10**6\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# },\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# \"20M\": {\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m#     \"database_file_path\": db_filename_size_20M,\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m#     \"index_file_path\": PATH_DB_20M,\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m#     \"size\": 20 * 10**6\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# }\u001b[39;00m\n\u001b[0;32m     23\u001b[0m }\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m db_name, info \u001b[38;5;129;01min\u001b[39;00m database_info\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     26\u001b[0m     db \u001b[38;5;241m=\u001b[39m VecDB(database_file_path \u001b[38;5;241m=\u001b[39m info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatabase_file_path\u001b[39m\u001b[38;5;124m\"\u001b[39m], index_file_path \u001b[38;5;241m=\u001b[39m info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex_file_path\u001b[39m\u001b[38;5;124m\"\u001b[39m], new_db \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'PATH_DB_10M' is not defined"
          ]
        }
      ],
      "source": [
        "print(\"Team Number\", 3)\n",
        "database_info = {\n",
        "    # \"1M\": {\n",
        "    #     \"database_file_path\": db_filename_size_1M,\n",
        "    #     \"index_file_path\": PATH_DB_1M,\n",
        "    #     \"size\": 10**6\n",
        "    # },\n",
        "    \"10M\": {\n",
        "        \"database_file_path\": db_filename_size_10M,\n",
        "        \"index_file_path\": PATH_DB_10M,\n",
        "        \"size\": 10 * 10**6\n",
        "    },\n",
        "    # \"15M\": {\n",
        "    #     \"database_file_path\": db_filename_size_15M,\n",
        "    #     \"index_file_path\": PATH_DB_15M,\n",
        "    #     \"size\": 15 * 10**6\n",
        "    # },\n",
        "    # \"20M\": {\n",
        "    #     \"database_file_path\": db_filename_size_20M,\n",
        "    #     \"index_file_path\": PATH_DB_20M,\n",
        "    #     \"size\": 20 * 10**6\n",
        "    # }\n",
        "}\n",
        "\n",
        "for db_name, info in database_info.items():\n",
        "    db = VecDB(database_file_path = info[\"database_file_path\"], index_file_path = info[\"index_file_path\"], new_db = False)\n",
        "    actual_ids = get_actual_ids_first_k(actual_sorted_ids_20m, info[\"size\"])\n",
        "    # Make a dummy run query to make everything fresh and loaded (wrap up)\n",
        "    res = run_queries(db, query_dummy, 5, actual_ids, 1)\n",
        "    # actual runs to evaluate\n",
        "    res, mem = memory_usage_run_queries((db, queries, 5, actual_ids, 3))\n",
        "    eval = evaluate_result(res)\n",
        "    to_print = f\"{db_name}\\tscore\\t{eval[0]}\\ttime\\t{eval[1]:.2f}\\tRAM\\t{mem:.2f} MB\"\n",
        "    print(to_print)\n",
        "    to_print_arr.append(to_print)\n",
        "    del db\n",
        "    del actual_ids\n",
        "    del res\n",
        "    del mem\n",
        "    del eval\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dX1owty-k2tm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jt1_7ihfB37Z",
        "outputId": "9ab6c78b-73e3-4a6d-8339-74f6d2a17655"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Team Number 3\n",
            "10M\tscore\t0.0\ttime\t2.90\tRAM\t1.86 MB\n"
          ]
        }
      ],
      "source": [
        "print(\"Team Number\", 3)\n",
        "print(\"\\n\".join(to_print_arr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BlxPAk4hKBQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
