{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "G44iH6jnObEj"
      },
      "outputs": [],
      "source": [
        "QUERY_SEED_NUMBER = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "miaF8OeibbIL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "DB_SEED_NUMBER = 42\n",
        "ELEMENT_SIZE = np.dtype(np.float32).itemsize\n",
        "DIMENSION = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import tqdm\n",
        "import heapq\n",
        "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
        "from typing import Annotated\n",
        "import time\n",
        "\n",
        "DB_SEED_NUMBER = 42\n",
        "ELEMENT_SIZE = np.dtype(np.float32).itemsize\n",
        "DIMENSION = 64\n",
        "\n",
        "class VecDB:\n",
        "    def __init__(self, database_file_path = \"saved_db.dat\", index_file_path = \"index.dat\", new_db = True, db_size = None) -> None:\n",
        "        self.db_path = database_file_path\n",
        "        self.index_path = index_file_path\n",
        "        if new_db:\n",
        "            if db_size is None:\n",
        "                raise ValueError(\"You need to provide the size of the database\")\n",
        "            # delete the old DB file if exists\n",
        "            if os.path.exists(self.db_path):\n",
        "                os.remove(self.db_path)\n",
        "            self.generate_database(db_size)\n",
        "    \n",
        "    def generate_database(self, size: int) -> None:\n",
        "        # rng = np.random.default_rng(DB_SEED_NUMBER)\n",
        "        vectors = np.memmap(\"new_embeddings.dat\", dtype=np.float32, mode='r', shape=(size, DIMENSION))\n",
        "        self._write_vectors_to_file(vectors)\n",
        "        self._build_index()\n",
        "\n",
        "    def _write_vectors_to_file(self, vectors: np.ndarray) -> None:\n",
        "        mmap_vectors = np.memmap(self.db_path, dtype=np.float32, mode='w+', shape=vectors.shape)\n",
        "        mmap_vectors[:] = vectors[:]\n",
        "        mmap_vectors.flush()\n",
        "\n",
        "    def _get_num_records(self) -> int:\n",
        "        return os.path.getsize(self.db_path) // (DIMENSION * ELEMENT_SIZE)\n",
        "\n",
        "    def insert_records(self, rows: Annotated[np.ndarray, (int, 64)]):\n",
        "        num_old_records = self._get_num_records()\n",
        "        num_new_records = len(rows)\n",
        "        full_shape = (num_old_records + num_new_records, DIMENSION)\n",
        "        mmap_vectors = np.memmap(self.db_path, dtype=np.float32, mode='r+', shape=full_shape)\n",
        "        mmap_vectors[num_old_records:] = rows\n",
        "        mmap_vectors.flush()\n",
        "        #TODO: might change to call insert in the index, if you need\n",
        "        self._build_index()\n",
        "\n",
        "    def get_one_row(self, row_num: int) -> np.ndarray:\n",
        "        # This function is only load one row in memory\n",
        "        try:\n",
        "            offset = row_num * DIMENSION * ELEMENT_SIZE\n",
        "            mmap_vector = np.memmap(self.db_path, dtype=np.float32, mode='r', shape=(1, DIMENSION), offset=offset)\n",
        "            return np.array(mmap_vector[0])\n",
        "        except Exception as e:\n",
        "            return f\"An error occurred: {e}\"\n",
        "\n",
        "    def get_all_rows(self) -> np.ndarray:\n",
        "        # Take care this load all the data in memory\n",
        "        num_records = self._get_num_records()\n",
        "        vectors = np.memmap(self.db_path, dtype=np.float32, mode='r', shape=(num_records, DIMENSION))\n",
        "        return np.array(vectors)\n",
        "    \n",
        "    def get_all_ids_rows_optimized(self, ids):\n",
        "        ids = np.array(ids)\n",
        "        num_records = self._get_num_records()\n",
        "\n",
        "        sorted_idx = np.argsort(ids)\n",
        "        sorted_ids = ids[sorted_idx]\n",
        "\n",
        "        base = sorted_ids[0]\n",
        "        row_size_bytes = DIMENSION * np.dtype(np.float32).itemsize\n",
        "        offset = base * row_size_bytes\n",
        "\n",
        "        # memmap starting from the base\n",
        "        vectors = np.memmap(\n",
        "            self.db_path, dtype=np.float32, mode='r',\n",
        "            offset=offset,\n",
        "            shape=(num_records - base, DIMENSION)\n",
        "        )\n",
        "\n",
        "        local_ids = sorted_ids - base\n",
        "        \n",
        "        result = np.empty((len(ids), DIMENSION), dtype=np.float32)\n",
        "        result[sorted_idx] = vectors[local_ids]\n",
        "\n",
        "        del vectors\n",
        "        return result\n",
        "    \n",
        "    \n",
        "    \n",
        "    def _cal_score(self, vec1, vec2):\n",
        "        dot_product = np.dot(vec1, vec2)\n",
        "        norm_vec1 = np.linalg.norm(vec1)\n",
        "        norm_vec2 = np.linalg.norm(vec2)\n",
        "        cosine_similarity = dot_product / (norm_vec1 * norm_vec2)\n",
        "        return cosine_similarity\n",
        "\n",
        "####################################################################################\n",
        "####################################################################################\n",
        "####################################################################################\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    def retrieve(self, query, top_k=5, n_probe_level3= 4, n_probe_level2=6, n_probe_level1=8, chunk_size=512):\n",
        "        L3 = os.path.join(self.index_path, \"centroids_L3.npy\")\n",
        "        L2 = os.path.join(self.index_path, \"centroids_L2.npy\")\n",
        "        L1 = os.path.join(self.index_path, \"centroids_L1.npy\")\n",
        "        if not os.path.exists(L3) or not os.path.exists(L2) or not os.path.exists(L1):\n",
        "            return []\n",
        "        \n",
        "        query = np.asarray(query, dtype=np.float32).squeeze()\n",
        "        query_norm = np.linalg.norm(query)\n",
        "        if query_norm == 0:\n",
        "            query_norm = 1.0\n",
        "        q = query / query_norm\n",
        "\n",
        "        # Load headers\n",
        "        level3_header = np.fromfile(\n",
        "            os.path.join(self.index_path, \"level3_header.bin\"), dtype=np.uint32\n",
        "        ).reshape(-1, 2)\n",
        "        level2_header = np.fromfile(\n",
        "            os.path.join(self.index_path, \"level2_header.bin\"), dtype=np.uint32\n",
        "        ).reshape(-1, 2)\n",
        "        index_header = np.fromfile(\n",
        "            os.path.join(self.index_path, \"index_header.bin\"), dtype=np.uint32\n",
        "        ).reshape(-1, 2)\n",
        "        flat_index_path = os.path.join(self.index_path, \"all_indices.bin\")\n",
        "\n",
        "        # Load centroids\n",
        "        centroids_L3 = np.load(L3, mmap_mode=\"r\")\n",
        "        sims_L3 = centroids_L3.dot(q)\n",
        "\n",
        "        # take top L3 clusters\n",
        "        lvl3_candidates = np.argpartition(sims_L3, -n_probe_level3)[-n_probe_level3:]\n",
        "        del sims_L3, centroids_L3\n",
        "\n",
        "        top_heap = []\n",
        "\n",
        "        # traverse level 3 -> level 2\n",
        "        for l3_idx in lvl3_candidates:\n",
        "            l3_offset, l3_len = level3_header[l3_idx]\n",
        "            if l3_len == 0:\n",
        "                continue\n",
        "\n",
        "            # slice level2 centroids for this level3 cluster\n",
        "            l2_start = l3_offset\n",
        "            l2_end = l2_start + l3_len\n",
        "            centroids_l2 = np.load(L2, mmap_mode=\"r\")[l2_start:l2_end]\n",
        "            sims_l2 = centroids_l2.dot(q)\n",
        "            num_l2 = sims_l2.shape[0]\n",
        "            k2 = min(n_probe_level2, num_l2)\n",
        "            lvl2_candidates = np.argpartition(sims_l2, -k2)[-k2:]\n",
        "            del sims_l2, centroids_l2\n",
        "\n",
        "            for local_l2 in lvl2_candidates:\n",
        "                l2_idx = l2_start + local_l2\n",
        "                l2_offset, l2_len = level2_header[l2_idx]\n",
        "\n",
        "                if l2_len == 0:\n",
        "                    continue\n",
        "\n",
        "                # slice level1 centroids for this level2 cluster\n",
        "                l1_start = l2_offset\n",
        "                l1_end = l1_start + l2_len\n",
        "\n",
        "                centroids_l1 = np.load(L1, mmap_mode=\"r\")[l1_start:l1_end]\n",
        "                sims_l1 = centroids_l1.dot(q)\n",
        "                num_l1 = sims_l1.shape[0]\n",
        "                k1 = min(n_probe_level1, num_l1)\n",
        "                lvl1_candidates = np.argpartition(sims_l1, -k1)[-k1:]\n",
        "                del sims_l1, centroids_l1\n",
        "\n",
        "                for local_l1 in lvl1_candidates:\n",
        "                    l1_idx = l1_start + local_l1\n",
        "                    data_offset, data_len = index_header[l1_idx]\n",
        "                    if data_len == 0:\n",
        "                        continue\n",
        "\n",
        "                    for start in range(0, data_len, chunk_size):\n",
        "                        cur = min(chunk_size, data_len - start)\n",
        "\n",
        "                        ids = np.memmap(\n",
        "                            flat_index_path,\n",
        "                            dtype=np.uint32,\n",
        "                            mode=\"r\",\n",
        "                            offset=data_offset + start * 4,\n",
        "                            shape=(cur,)\n",
        "                        )[:]\n",
        "\n",
        "                        vecs = self.get_all_ids_rows_optimized(ids)\n",
        "                        scores = vecs.dot(q)\n",
        "\n",
        "                        for score, id in zip(scores, ids):\n",
        "                            if len(top_heap) < top_k:\n",
        "                                heapq.heappush(top_heap, (score, id))\n",
        "                            else:\n",
        "                                heapq.heappushpop(top_heap, (score, id))\n",
        "\n",
        "                        del scores, ids, vecs\n",
        "\n",
        "        # extract top-k sorted\n",
        "        return [idx for score, idx in heapq.nlargest(top_k, top_heap)]\n",
        "\n",
        "    def _build_index(self):\n",
        "        self.no_centroids = 7000\n",
        "        self.no_level2_centroids = 80\n",
        "        self.no_level3_centroids = 8\n",
        "\n",
        "        data = self.get_all_rows()\n",
        "\n",
        "        # level 1 clustering\n",
        "        kmeans1 = MiniBatchKMeans(\n",
        "            n_clusters=self.no_centroids,\n",
        "            init=\"k-means++\",\n",
        "            batch_size=10_000,\n",
        "            random_state=42\n",
        "        )\n",
        "        kmeans1.fit(data)\n",
        "        labels1 = kmeans1.labels_\n",
        "        centers1 = kmeans1.cluster_centers_.astype(np.float32)\n",
        "        del data, kmeans1\n",
        "\n",
        "        if os.path.exists(self.index_path):\n",
        "            shutil.rmtree(self.index_path)\n",
        "        os.makedirs(self.index_path, exist_ok=True)\n",
        "\n",
        "        cluster_infos_L1 = [(cid, np.where(labels1 == cid)[0].astype(np.uint32))\n",
        "                        for cid in range(self.no_centroids)]\n",
        "        del labels1\n",
        "\n",
        "        # level 2 clustering\n",
        "        kmeans2 = MiniBatchKMeans(\n",
        "            n_clusters=self.no_level2_centroids,\n",
        "            init=\"k-means++\",\n",
        "            batch_size=1000,\n",
        "            random_state=42,\n",
        "        )\n",
        "        kmeans2.fit(centers1)\n",
        "        centers2 = kmeans2.cluster_centers_.astype(np.float32)\n",
        "        labels2 = kmeans2.labels_\n",
        "\n",
        "\n",
        "        cluster_infos_L2 = [(cid, np.where(labels2 == cid)[0].astype(np.uint32))\n",
        "                            for cid in range(self.no_level2_centroids)]\n",
        "        del labels2\n",
        "\n",
        "        # level 3 clustering\n",
        "        kmeans3 = MiniBatchKMeans(\n",
        "            n_clusters=self.no_level3_centroids,\n",
        "            init=\"k-means++\",\n",
        "            batch_size=200,\n",
        "            random_state=42,\n",
        "        )\n",
        "        kmeans3.fit(centers2)\n",
        "        centers3 = kmeans3.cluster_centers_.astype(np.float32)\n",
        "        labels3 = kmeans3.labels_\n",
        "\n",
        "        cluster_infos_L3 = [(cid, np.where(labels3 == cid)[0].astype(np.uint32))\n",
        "                            for cid in range(self.no_level3_centroids)]\n",
        "        del labels3\n",
        "\n",
        "        # reorder centers and cluster infos L3 -> L2 -> L1\n",
        "        reordered_centers_L1 = []\n",
        "        reordered_cluster_infos_L1 = []\n",
        "\n",
        "        for _, inds_L3 in cluster_infos_L3:\n",
        "            for idx_L2 in inds_L3:\n",
        "                _, inds_L2 = cluster_infos_L2[idx_L2]\n",
        "                for ind_L1 in inds_L2:\n",
        "                    reordered_centers_L1.append(centers1[ind_L1])\n",
        "                    reordered_cluster_infos_L1.append(cluster_infos_L1[ind_L1])\n",
        "        centers1 = np.array(reordered_centers_L1, dtype=np.float32)\n",
        "        cluster_infos_L1 = reordered_cluster_infos_L1\n",
        "        del reordered_centers_L1, reordered_cluster_infos_L1\n",
        "\n",
        "        # reordering unnecessary for other levels\n",
        "        # for _, inds in cluster_infos_L2:\n",
        "        #     for ind in inds:\n",
        "        #         reordered_centers_L1.append(centers1[ind])\n",
        "        #         reordered_cluster_infos_L1.append(cluster_infos_L1[ind])\n",
        "        # centers1 = np.array(reordered_centers_L1, dtype=np.float32)\n",
        "        # cluster_infos_L1 = reordered_cluster_infos_L1\n",
        "        # del labels1, labels2\n",
        "        # del reordered_centers_L1, reordered_cluster_infos_L1\n",
        "\n",
        "        header = []\n",
        "        flat_path = os.path.join(self.index_path, \"all_indices.bin\")\n",
        "        with open(flat_path, \"wb\") as f:\n",
        "            offset = 0\n",
        "            for _, inds in cluster_infos_L1:\n",
        "                length = inds.size\n",
        "                f.write(inds.tobytes())\n",
        "                header.append([offset, length])\n",
        "                offset += length * inds.dtype.itemsize\n",
        "\n",
        "        np.array(header, dtype=np.uint32).tofile(os.path.join(self.index_path, \"index_header.bin\"))\n",
        "\n",
        "        # save level2 header (offset, length) for easy slicing later\n",
        "        level2_header = []\n",
        "        offset = 0\n",
        "        for _, inds in cluster_infos_L2:\n",
        "            length = len(inds)  \n",
        "            level2_header.append([offset, length])\n",
        "            offset += length\n",
        "\n",
        "        np.array(level2_header, dtype=np.uint32).tofile(os.path.join(self.index_path, \"level2_header.bin\"))\n",
        "\n",
        "        # save level 3 header (offset, length) for easy slicing later\n",
        "        level3_header = []\n",
        "        offset = 0\n",
        "        for _, inds in cluster_infos_L3:\n",
        "            length = len(inds)  \n",
        "            level3_header.append([offset, length])\n",
        "            offset += length\n",
        "\n",
        "        np.array(level3_header, dtype=np.uint32).tofile(os.path.join(self.index_path, \"level3_header.bin\"))\n",
        "\n",
        "        # normalize centers\n",
        "        centers1 /= (np.linalg.norm(centers1, axis=1, keepdims=True) + 1e-12)\n",
        "        centers2 /= (np.linalg.norm(centers2, axis=1, keepdims=True) + 1e-12)\n",
        "        centers3 /= (np.linalg.norm(centers3, axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "        np.save(os.path.join(self.index_path, \"centroids_L1.npy\"), centers1)\n",
        "        np.save(os.path.join(self.index_path, \"centroids_L2.npy\"), centers2)\n",
        "        np.save(os.path.join(self.index_path, \"centroids_L3.npy\"), centers3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Sg2vfYgeyavn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "# from vec_db import VecDB\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "from memory_profiler import memory_usage\n",
        "import gc\n",
        "\n",
        "@dataclass\n",
        "class Result:\n",
        "    run_time: float\n",
        "    top_k: int\n",
        "    db_ids: List[int]\n",
        "    actual_ids: List[int]\n",
        "\n",
        "def run_queries(db, queries, top_k, actual_ids, num_runs):\n",
        "    \"\"\"\n",
        "    Run queries on the database and record results for each query.\n",
        "\n",
        "    Parameters:\n",
        "    - db: Database instance to run queries on.\n",
        "    - queries: List of query vectors.\n",
        "    - top_k: Number of top results to retrieve.\n",
        "    - actual_ids: List of actual results to evaluate accuracy.\n",
        "    - num_runs: Number of query executions to perform for testing.\n",
        "\n",
        "    Returns:\n",
        "    - List of Result\n",
        "    \"\"\"\n",
        "    global results\n",
        "    results = []\n",
        "    for i in range(num_runs):\n",
        "        tic = time.time()\n",
        "        db_ids = db.retrieve(queries[i], top_k)\n",
        "        toc = time.time()\n",
        "        run_time = toc - tic\n",
        "        results.append(Result(run_time, top_k, db_ids, actual_ids[i]))\n",
        "    return results\n",
        "\n",
        "def memory_usage_run_queries(args):\n",
        "    \"\"\"\n",
        "    Run queries and measure memory usage during the execution.\n",
        "\n",
        "    Parameters:\n",
        "    - args: Arguments to be passed to the run_queries function.\n",
        "\n",
        "    Returns:\n",
        "    - results: The results of the run_queries.\n",
        "    - memory_diff: The difference in memory usage before and after running the queries.\n",
        "    \"\"\"\n",
        "    global results\n",
        "    mem_before = max(memory_usage())\n",
        "    mem = memory_usage(proc=(run_queries, args, {}), interval = 1e-3)\n",
        "    return results, max(mem) - mem_before\n",
        "\n",
        "def evaluate_result(results: List[Result]):\n",
        "    \"\"\"\n",
        "    Evaluate the results based on accuracy and runtime.\n",
        "    Scores are negative. So getting 0 is the best score.\n",
        "\n",
        "    Parameters:\n",
        "    - results: A list of Result objects\n",
        "\n",
        "    Returns:\n",
        "    - avg_score: The average score across all queries.\n",
        "    - avg_runtime: The average runtime for all queries.\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "    run_time = []\n",
        "    for res in results:\n",
        "        run_time.append(res.run_time)\n",
        "        # case for retireving number not equal to top_k, socre will be the lowest\n",
        "        if len(set(res.db_ids)) != res.top_k or len(res.db_ids) != res.top_k:\n",
        "            scores.append( -1 * len(res.actual_ids) * res.top_k)\n",
        "            continue\n",
        "        score = 0\n",
        "        for id in res.db_ids:\n",
        "            try:\n",
        "                ind = res.actual_ids.index(id)\n",
        "                if ind > res.top_k * 3:\n",
        "                    score -= ind\n",
        "            except:\n",
        "                score -= len(res.actual_ids)\n",
        "        scores.append(score)\n",
        "\n",
        "    return sum(scores) / len(scores), sum(run_time) / len(run_time)\n",
        "\n",
        "def get_actual_ids_first_k(actual_sorted_ids, k):\n",
        "    \"\"\"\n",
        "    Retrieve the IDs from the sorted list of actual IDs.\n",
        "    actual IDs has the top_k for the 20 M database but for other databases we have to remove the numbers higher than the max size of the DB.\n",
        "\n",
        "    Parameters:\n",
        "    - actual_sorted_ids: A list of lists containing the sorted actual IDs for each query.\n",
        "    - k: The DB size.\n",
        "\n",
        "    Returns:\n",
        "    - List of lists containing the actual IDs for each query for this DB.\n",
        "    \"\"\"\n",
        "    return [[id for id in actual_sorted_ids_one_q if id < k] for actual_sorted_ids_one_q in actual_sorted_ids]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3bQQzzWlce4"
      },
      "source": [
        "This code to generate all the files for databases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zZPsvyMqX17g"
      },
      "outputs": [],
      "source": [
        "def _write_vectors_to_file(vectors: np.ndarray, db_path) -> None:\n",
        "    mmap_vectors = np.memmap(db_path, dtype=np.float32, mode='w+', shape=vectors.shape)\n",
        "    mmap_vectors[:] = vectors[:]\n",
        "    mmap_vectors.flush()\n",
        "\n",
        "# def generate_database(size: int) -> None:\n",
        "#     rng = np.random.default_rng(DB_SEED_NUMBER)\n",
        "#     vectors = rng.random((size, DIMENSION), dtype=np.float32)\n",
        "#     return vectors\n",
        "def generated_database():\n",
        "    # Load memmap\n",
        "    mmap_vectors = np.memmap(\"new_embeddings.dat\", dtype=np.float32, mode='r', shape=(20_000_000, 64))\n",
        "    return mmap_vectors\n",
        "\n",
        "\n",
        "vectors = generated_database()\n",
        "\n",
        "db_filename_size_20M = 'saved_db_20M.dat'\n",
        "if not os.path.exists(db_filename_size_20M): _write_vectors_to_file(vectors, db_filename_size_20M)\n",
        "db_filename_size_15M = 'saved_db_15M.dat'\n",
        "if not os.path.exists(db_filename_size_15M): _write_vectors_to_file(vectors[:15*10**6], db_filename_size_15M)\n",
        "db_filename_size_10M = 'saved_db_10M.dat'\n",
        "if not os.path.exists(db_filename_size_10M): _write_vectors_to_file(vectors[:10*10**6], db_filename_size_10M)\n",
        "db_filename_size_1M = 'saved_db_1M.dat'\n",
        "if not os.path.exists(db_filename_size_1M): _write_vectors_to_file(vectors[:1*10**6], db_filename_size_1M)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c83ybYSKK85G",
        "outputId": "91dd0f3a-be2c-4dac-a42f-0a1aabe492f4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\hankerz\\AppData\\Local\\Temp\\ipykernel_25132\\1558840725.py:8: RuntimeWarning: invalid value encountered in divide\n",
            "  actual_sorted_ids_20m_q1 = np.argsort(vectors.dot(query1.T).T / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(query1)), axis= 1).squeeze().tolist()[::-1][:needed_top_k]\n",
            "C:\\Users\\hankerz\\AppData\\Local\\Temp\\ipykernel_25132\\1558840725.py:10: RuntimeWarning: invalid value encountered in divide\n",
            "  actual_sorted_ids_20m_q2 = np.argsort(vectors.dot(query2.T).T / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(query2)), axis= 1).squeeze().tolist()[::-1][:needed_top_k]\n",
            "C:\\Users\\hankerz\\AppData\\Local\\Temp\\ipykernel_25132\\1558840725.py:12: RuntimeWarning: invalid value encountered in divide\n",
            "  actual_sorted_ids_20m_q3 = np.argsort(vectors.dot(query3.T).T / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(query3)), axis= 1).squeeze().tolist()[::-1][:needed_top_k]\n"
          ]
        }
      ],
      "source": [
        "needed_top_k = 15\n",
        "rng = np.random.default_rng(QUERY_SEED_NUMBER)\n",
        "query1 = rng.random((1, 64), dtype=np.float32)\n",
        "query2 = rng.random((1, 64), dtype=np.float32)\n",
        "query3 = rng.random((1, 64), dtype=np.float32)\n",
        "query_dummy = rng.random((1, 64), dtype=np.float32)\n",
        "\n",
        "actual_sorted_ids_20m_q1 = np.argsort(vectors.dot(query1.T).T / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(query1)), axis= 1).squeeze().tolist()[::-1][:needed_top_k]\n",
        "gc.collect()\n",
        "actual_sorted_ids_20m_q2 = np.argsort(vectors.dot(query2.T).T / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(query2)), axis= 1).squeeze().tolist()[::-1][:needed_top_k]\n",
        "gc.collect()\n",
        "actual_sorted_ids_20m_q3 = np.argsort(vectors.dot(query3.T).T / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(query3)), axis= 1).squeeze().tolist()[::-1][:needed_top_k]\n",
        "gc.collect()\n",
        "\n",
        "queries = [query1, query2, query3]\n",
        "actual_sorted_ids_20m = [actual_sorted_ids_20m_q1, actual_sorted_ids_20m_q2, actual_sorted_ids_20m_q3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bW7eI-hIvIfb",
        "outputId": "6dc4b563-e084-4bcf-f6a4-d10148efc0fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# No more need to the actual vectors so delete it\n",
        "del vectors\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "g-hCaQNqlBP0"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "to_print_arr = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s-envHmfeBr"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qackwdJN8Tor",
        "outputId": "1a0ae8eb-dccf-431c-a3a3-b5be6d8a32f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Team Number 3\n"
          ]
        }
      ],
      "source": [
        "print(\"Team Number\", 3)\n",
        "database_info = {\n",
        "    # \"1M\": {\n",
        "    #     \"database_file_path\": db_filename_size_1M,\n",
        "    #     \"index_file_path\": \"index_10M_8_80_7000_centroids\",\n",
        "    #     \"size\": 10**6\n",
        "    # },\n",
        "    # \"10M\": {\n",
        "    #     \"database_file_path\": db_filename_size_10M,\n",
        "    #     \"index_file_path\": \"index_10M_8_80_7000_centroids\",\n",
        "    #     \"size\": 10 * 10**6\n",
        "    # },\n",
        "    \"20M\": {\n",
        "        \"database_file_path\": db_filename_size_20M,\n",
        "        \"index_file_path\": \"index_10M_8_80_7000_centroids\",\n",
        "        \"size\": 20 * 10**6\n",
        "    }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "EXM0xTij8IhU"
      },
      "outputs": [],
      "source": [
        "\n",
        "for db_name, info in database_info.items():\n",
        "    db = VecDB(database_file_path = info[\"database_file_path\"], index_file_path = info[\"index_file_path\"], new_db = False)\n",
        "    db._build_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "h-UFbhBPlQtz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20M\tscore\t-75.0\ttime\t3.08\tRAM\t7.13 MB\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for db_name, info in database_info.items():\n",
        "    db = VecDB(database_file_path = info[\"database_file_path\"], index_file_path = info[\"index_file_path\"], new_db = False)\n",
        "    actual_ids = get_actual_ids_first_k(actual_sorted_ids_20m, info[\"size\"])\n",
        "    # Make a dummy run query to make everything fresh and loaded (wrap up)\n",
        "    res = run_queries(db, query_dummy, 5, actual_ids, 1)\n",
        "    # actual runs to evaluate\n",
        "    res, mem = memory_usage_run_queries((db, queries, 5, actual_ids, 3))\n",
        "    eval = evaluate_result(res)\n",
        "    to_print = f\"{db_name}\\tscore\\t{eval[0]}\\ttime\\t{eval[1]:.2f}\\tRAM\\t{mem:.2f} MB\"\n",
        "    print(to_print)\n",
        "    to_print_arr.append(to_print)\n",
        "    del db\n",
        "    del actual_ids\n",
        "    del res\n",
        "    del mem\n",
        "    del eval\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def get_one_row(row_num: int) -> np.ndarray:\n",
        "    # This function is only load one row in memory\n",
        "    try:\n",
        "        offset = row_num * 64 * np.dtype(np.float32).itemsize\n",
        "        mmap_vector = np.memmap(\"saved_db_1M.dat\", dtype=np.float32, mode='r', shape=(1, 64), offset=offset)\n",
        "        return np.array(mmap_vector[0])\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {e}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_all_ids_rows( ids) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Load only the requested rows from the memmap, without loading all data in RAM.\n",
        "    Updated: Instead of loading all vectors into memory, we load only the requested batch.\n",
        "    \"\"\"\n",
        "    num_records = 1_000_000\n",
        "    vectors = np.memmap(\"saved_db_1M.dat\", dtype=np.float32, mode='r', shape=(num_records, 64))\n",
        "    \n",
        "    # Sort IDs to access memmap sequentially (faster disk read)\n",
        "    sorted_idx = np.argsort(ids)\n",
        "    sorted_ids = np.array(ids)[sorted_idx]\n",
        "    \n",
        "    # Load only selected rows\n",
        "    result = np.empty((len(ids), 64), dtype=np.float32)\n",
        "    result[sorted_idx] = vectors[sorted_ids]\n",
        "    \n",
        "    del vectors\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_all_ids_rows_optimized(ids):\n",
        "    ids = np.array(ids)\n",
        "    num_records = 1_000_000\n",
        "\n",
        "    sorted_idx = np.argsort(ids)\n",
        "    sorted_ids = ids[sorted_idx]\n",
        "\n",
        "    base = sorted_ids[0]\n",
        "    row_size_bytes = 64 * np.dtype(np.float32).itemsize\n",
        "    offset = base * row_size_bytes\n",
        "\n",
        "    # memmap starting from the base\n",
        "    vectors = np.memmap(\n",
        "        \"saved_db_1M.dat\", dtype=np.float32, mode='r',\n",
        "        offset=offset,\n",
        "        shape=(num_records - base, 64)\n",
        "    )\n",
        "\n",
        "    local_ids = sorted_ids - base\n",
        "    \n",
        "    result = np.empty((len(ids), 64), dtype=np.float32)\n",
        "    result[sorted_idx] = vectors[local_ids]\n",
        "\n",
        "    del vectors\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1, 2, 3], [10, 12], [20], [40]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "ids_sorted=[1,2,3,10,12,20,40]\n",
        "groups = group_ids_by_window_fast(ids_sorted, 5)\n",
        "print(groups)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time taken: 0.01510763168334961\n"
          ]
        }
      ],
      "source": [
        "time1 = time.time()\n",
        "vectors1 = get_all_ids_rows(ids)\n",
        "time2 = time.time()\n",
        "# print(vectors)\n",
        "print(\"Time taken:\", time2 - time1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time taken: 0.0012903213500976562\n"
          ]
        }
      ],
      "source": [
        "time1 = time.time()\n",
        "vectors2 = get_all_ids_rows_optimized(ids)\n",
        "time2 = time.time()\n",
        "# print(vectors)\n",
        "print(\"Time taken:\", time2 - time1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nice\n"
          ]
        }
      ],
      "source": [
        "if np.array_equal(np.array(vectors1[1]), np.array(vectors2[1])):\n",
        "    print(\"nice\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-6.28111288e-02 -1.47320539e-01  1.90533586e-02  4.43077274e-02\n",
            "  1.05074465e-01  2.83172131e-01 -1.63409859e-01  1.95712700e-01\n",
            "  1.07049912e-01 -1.23330027e-01 -3.00835390e-02  2.45135650e-01\n",
            " -5.26915416e-02 -5.52256368e-02  9.29732770e-02 -6.97657317e-02\n",
            " -6.27087284e-05 -1.32255822e-01 -2.69026589e-02  7.89163932e-02\n",
            " -1.89308122e-01  6.02391511e-02  3.74443419e-02 -1.43766150e-01\n",
            " -2.76008368e-01 -6.13625012e-02  5.22740744e-02  1.01051793e-01\n",
            " -5.17447963e-02  3.67304794e-02 -2.33252682e-02  1.31471306e-01\n",
            " -1.30366221e-01  6.15089536e-02 -1.63679924e-02 -1.46132097e-01\n",
            " -8.24083313e-02  1.16798908e-01 -1.00988574e-01  1.16019160e-01\n",
            " -6.51810691e-02  4.59860981e-04  1.06093474e-01  1.89122364e-01\n",
            " -2.01225087e-01  8.32308456e-02 -4.10196595e-02 -1.48688197e-01\n",
            "  2.44087949e-02 -2.67753322e-02  9.42895487e-02 -1.49306238e-01\n",
            " -1.71684042e-01 -3.24866235e-01  3.02920695e-02 -1.64438151e-02\n",
            " -1.86713353e-01 -1.15885682e-01  1.82194505e-02  9.29413810e-02\n",
            "  9.96821746e-02  1.78462937e-01 -1.56787243e-02 -1.76343977e-01]\n"
          ]
        }
      ],
      "source": [
        "print(vectors[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-6.28111288e-02 -1.47320539e-01  1.90533586e-02  4.43077274e-02\n",
            "  1.05074465e-01  2.83172131e-01 -1.63409859e-01  1.95712700e-01\n",
            "  1.07049912e-01 -1.23330027e-01 -3.00835390e-02  2.45135650e-01\n",
            " -5.26915416e-02 -5.52256368e-02  9.29732770e-02 -6.97657317e-02\n",
            " -6.27087284e-05 -1.32255822e-01 -2.69026589e-02  7.89163932e-02\n",
            " -1.89308122e-01  6.02391511e-02  3.74443419e-02 -1.43766150e-01\n",
            " -2.76008368e-01 -6.13625012e-02  5.22740744e-02  1.01051793e-01\n",
            " -5.17447963e-02  3.67304794e-02 -2.33252682e-02  1.31471306e-01\n",
            " -1.30366221e-01  6.15089536e-02 -1.63679924e-02 -1.46132097e-01\n",
            " -8.24083313e-02  1.16798908e-01 -1.00988574e-01  1.16019160e-01\n",
            " -6.51810691e-02  4.59860981e-04  1.06093474e-01  1.89122364e-01\n",
            " -2.01225087e-01  8.32308456e-02 -4.10196595e-02 -1.48688197e-01\n",
            "  2.44087949e-02 -2.67753322e-02  9.42895487e-02 -1.49306238e-01\n",
            " -1.71684042e-01 -3.24866235e-01  3.02920695e-02 -1.64438151e-02\n",
            " -1.86713353e-01 -1.15885682e-01  1.82194505e-02  9.29413810e-02\n",
            "  9.96821746e-02  1.78462937e-01 -1.56787243e-02 -1.76343977e-01]\n"
          ]
        }
      ],
      "source": [
        "vector_tmp=get_one_row(ids[0])\n",
        "print(vector_tmp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "indices=np.memmap('./index_IVFPQ_1M_230_8_16_centroids/codebook_shape.bin',dtype=np.float32, mode='r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original:\n",
            " [[ 1  2  3  4  5  6  7  8]\n",
            " [ 5  6  7  8  9 10 11 12]\n",
            " [15  0 14  1 13  2 12  3]]\n",
            "Packed:\n",
            " [[ 18  52  86 120]\n",
            " [ 86 120 154 188]\n",
            " [240 225 210 195]]\n",
            "Unpacked:\n",
            " [[ 1  2  3  4  5  6  7  8]\n",
            " [ 5  6  7  8  9 10 11 12]\n",
            " [15  0 14  1 13  2 12  3]]\n",
            "Round-trip OK: True\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "M = 8\n",
        "N = 3\n",
        "\n",
        "# Simple deterministic test values (0..15)\n",
        "codebook = np.array([\n",
        "    [1, 2, 3, 4,5, 6, 7, 8],\n",
        "    [5, 6, 7, 8,9, 10, 11, 12],\n",
        "    [15, 0, 14, 1, 13, 2, 12, 3]\n",
        "], dtype=np.uint8)\n",
        "\n",
        "# PACK\n",
        "packed = np.zeros((N, M // 2), dtype=np.uint8)\n",
        "for i in range(M // 2):\n",
        "    packed[:, i] = (codebook[:, 2*i] << 4) | codebook[:, 2*i + 1]\n",
        "\n",
        "# UNPACK\n",
        "def unpack_packed_codes(packed_block, M):\n",
        "    if packed_block.ndim == 1:\n",
        "        packed_block = packed_block.reshape(1, -1)\n",
        "    n = packed_block.shape[0]\n",
        "    codes = np.empty((n, M), dtype=np.uint8)\n",
        "    high = (packed_block >> 4).astype(np.uint8)\n",
        "    low  = (packed_block & 0x0F).astype(np.uint8)\n",
        "    codes[:, 0::2] = high\n",
        "    codes[:, 1::2] = low\n",
        "    return codes\n",
        "\n",
        "unpacked = unpack_packed_codes(packed, M)\n",
        "\n",
        "print(\"Original:\\n\", codebook)\n",
        "print(\"Packed:\\n\", packed)\n",
        "print(\"Unpacked:\\n\", unpacked)\n",
        "print(\"Round-trip OK:\", np.array_equal(unpacked, codebook))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
